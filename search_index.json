[
["index.html", "Aprendizaje de máquina Temario y referencias", " Aprendizaje de máquina Felipe González 2018-10-01 Temario y referencias Todas las notas y material del curso estarán en este repositorio. Introducción al aprendizaje máquina Regresión lineal múltiple y descenso en gradiente Problemas de clasificación y regresión logística Validación cruzada y métodos de remuestreo Regularización y selección de modelos Redes neuronales Diagnóstico y mejora en problemas de aprendizaje supervisado. Árboles y bosques aleatorios Máquinas de soporte vectorial Componentes principales y análisis de conglomerados Evaluación Tareas semanales (25%) Examen parcial (30% práctico, 20% teórico) Un examen final (25% práctico) Software: R y Rstudio R Sitio de R (CRAN) Rstudio Interfaz gráfica para trabajar en R. Recursos para aprender R Referencias principales An Introduction to Statistical Learning, James et al. (2014) Curso de Machine Learning de Andrew Ng, Ng (2017) Deep Learning, Goodfellow, Bengio, and Courville (2016) Otras referencias Pattern Recognition and Machine Learning, Bishop (2006) The Elements of Statistical Learning, Hastie, Tibshirani y Friedman, Hastie, Tibshirani, and Friedman (2017) References "],
["introduccion.html", "Clase 1 Introducción 1.1 ¿Qué es aprendizaje de máquina (machine learning)? 1.2 Aprendizaje Supervisado 1.3 Predicciones 1.4 Tarea de aprendizaje supervisado 1.5 Balance de complejidad y rigidez 1.6 ¿Cómo estimar f? 1.7 Resumen 1.8 Tarea", " Clase 1 Introducción 1.1 ¿Qué es aprendizaje de máquina (machine learning)? Métodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión. En este curso nos enfocamos en las tareas de aprendizaje supervisado (predecir o estimar una variable respuesta a partir de datos de entrada) y aprendizaje no supervisado (describir estructuras interesantes en datos, donde no necesariamente hay una respuesta que predecir). Ejemplos de tareas de aprendizaje: Predecir si un cliente de tarjeta de crédito va a caer en impago en los próximos tres meses. Reconocer palabras escritas a mano (OCR). Detectar llamados de ballenas en grabaciones de boyas. Estimar el ingreso mensual de un hogar a partir de las características de la vivienda, posesiones y equipamiento y localización geográfica. Dividir a los clientes de Netflix según sus gustos. Recomendar artículos a clientes de un programa de lealtad o servicio online. Las razones usuales para intentar resolver estos problemas computacionalmente son diversas: Quisiéramos obtener una respuesta barata, rápida, automatizada, y con suficiente precisión. Por ejemplo, reconocer caracteres en una placa de coche de una fotografía se puede hacer por personas, pero eso es lento y costoso. Igual oír cada segundo de grabación de las boyas para saber si hay ballenas o no. Hacer mediciones directas del ingreso de un hogar requiere mucho tiempo y esfuerzo. Quisiéramos superar el desempeño actual de los expertos o de reglas simples utilizando datos: por ejemplo, en la decisión de dar o no un préstamo a un solicitante, puede ser posible tomar mejores decisiones con algoritmos que con evaluaciones personales o con reglas simples que toman en cuenta el ingreso mensual, por ejemplo. Queremos entender de manera más completa y sistemática el comportamiento de un fenómeno, identificando variables o patrones importantes. Es posible aproximarse a todos estos problemas usando reglas (por ejemplo, si los pixeles del centro de la imagen están vacíos, entonces es un cero, si el crédito total es mayor al 50% del ingreso anual, declinar el préstamo, etc) Las razones para intentar usar aprendizaje para producir reglas en lugar de intentar construir estas reglas directamente son, por ejemplo: Cuando conjuntos de reglas creadas a mano se desempeñan mal (por ejemplo, para otorgar créditos, reconocer caracteres, etc.) Reglas creadas a mano pueden ser difíciles de mantener (por ejemplo, un corrector ortográfico.) Ejemplo: reconocimiento de dígitos escritos a mano ¿Cómo reconocer los siguientes dígitos de manera automática? En los datos tenemos los valores de cada pixel (los caracteres son imagenes de 16x16 pixeles), y una etiqueta asociada, que es el número que la imagen representa. Podemos ver las imágenes y las etiquetas: library(tidyverse) zip_train &lt;- read_csv(file = &#39;datos/zip-train.csv&#39;) muestra_1 &lt;- sample_n(zip_train, 10) graficar_digitos(muestra_1) muestra_2 &lt;- sample_n(zip_train, 10) graficar_digitos(muestra_2) Los 16x16=256 están escritos acomodando las filas de la imagen en vector de 256 valores (cada renglón de zip_train). Un dígito entonces se representa como sigue: dim(zip_train) ## [1] 7291 257 as.numeric(zip_train[1,]) ## [1] 6.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.631 0.862 ## [11] -0.167 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 ## [21] -1.000 -1.000 -1.000 -0.992 0.297 1.000 0.307 -1.000 -1.000 -1.000 ## [31] -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.410 ## [41] 1.000 0.986 -0.565 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 ## [51] -1.000 -1.000 -1.000 -1.000 -0.683 0.825 1.000 0.562 -1.000 -1.000 ## [61] -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.938 ## [71] 0.540 1.000 0.778 -0.715 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 ## [81] -1.000 -1.000 -1.000 -1.000 -1.000 0.100 1.000 0.922 -0.439 -1.000 ## [91] -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 ## [101] -0.257 0.950 1.000 -0.162 -1.000 -1.000 -1.000 -0.987 -0.714 -0.832 ## [111] -1.000 -1.000 -1.000 -1.000 -1.000 -0.797 0.909 1.000 0.300 -0.961 ## [121] -1.000 -1.000 -0.550 0.485 0.996 0.867 0.092 -1.000 -1.000 -1.000 ## [131] -1.000 0.278 1.000 0.877 -0.824 -1.000 -0.905 0.145 0.977 1.000 ## [141] 1.000 1.000 0.990 -0.745 -1.000 -1.000 -0.950 0.847 1.000 0.327 ## [151] -1.000 -1.000 0.355 1.000 0.655 -0.109 -0.185 1.000 0.988 -0.723 ## [161] -1.000 -1.000 -0.630 1.000 1.000 0.068 -0.925 0.113 0.960 0.308 ## [171] -0.884 -1.000 -0.075 1.000 0.641 -0.995 -1.000 -1.000 -0.677 1.000 ## [181] 1.000 0.753 0.341 1.000 0.707 -0.942 -1.000 -1.000 0.545 1.000 ## [191] 0.027 -1.000 -1.000 -1.000 -0.903 0.792 1.000 1.000 1.000 1.000 ## [201] 0.536 0.184 0.812 0.837 0.978 0.864 -0.630 -1.000 -1.000 -1.000 ## [211] -1.000 -0.452 0.828 1.000 1.000 1.000 1.000 1.000 1.000 1.000 ## [221] 1.000 0.135 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.483 0.813 ## [231] 1.000 1.000 1.000 1.000 1.000 1.000 0.219 -0.943 -1.000 -1.000 ## [241] -1.000 -1.000 -1.000 -1.000 -1.000 -0.974 -0.429 0.304 0.823 1.000 ## [251] 0.482 -0.474 -0.991 -1.000 -1.000 -1.000 -1.000 Un enfoque más utilizado anteriormente para resolver este tipo de problemas consistía en procesar estas imágenes con filtros hechos a mano (por ejemplo, calcular cuántos pixeles están prendidos, si existen ciertas curvas o trazos) para después construir reglas para determinar cada dígito. Actualmente, el enfoque más exitoso es utilizar métodos de aprendizaje que aprendan automáticamente esos filtros y esas reglas basadas en filtros (redes convolucionales). Ejemplo: predecir ingreso trimestral Consideramos la medición de ingreso total trimestral para una muestra de hogares de la encuesta de ENIGH. Cada una de estas mediciones es muy costosa en tiempo y dinero. dat_ingreso &lt;- read_csv(file = &#39;datos/enigh-ejemplo.csv&#39;) head(dat_ingreso) %&gt;% select(TAM_HOG, INGCOR, NOM_ENT_1, FOCOS, PISOS, marginación, tamaño_localidad) %&gt;% knitr::kable() TAM_HOG INGCOR NOM_ENT_1 FOCOS PISOS marginación tamaño_localidad 4 30238.13 Jalisco 11 3 Muy bajo De 15 mil a 100 mil 3 61147.41 México 10 2 Bajo De 15 mil a 100 mil 2 6170.21 Puebla 1 1 Alto De 2500 a 15 mil 2 14639.79 Distrito Federal 5 2 Muy bajo 100 mil o más 1 40638.35 Chihuahua 8 3 Muy bajo De 15 mil a 100 mil 2 21172.35 Baja California 4 2 Muy bajo 100 mil o más ggplot(dat_ingreso, aes(x=INGTOT/1000)) + geom_histogram(bins = 100) + scale_x_log10(breaks = c(2.5, 5, 10, 20, 40, 80, 160, 320, 640, 1280)) + xlab(&quot;Ingreso trimestral (miles de pesos)&quot;) Pero quizá podemos usar otras variables más fácilmente medibles para predecir el ingreso de un hogar. Por ejemplo, si consideramos el número de focos en la vivienda: ggplot(dat_ingreso, aes(x = FOCOS, y = INGTOT/1000)) + geom_point() + scale_y_log10(breaks = c(2.5, 5, 10, 20, 40, 80, 160, 320, 640, 1280)) + ylab(&quot;Ingreso trimestral (miles de pesos)&quot;) + xlim(c(0,50)) O el tamaño de la localidad: ggplot(dat_ingreso, aes(x = tamaño_localidad, y = INGTOT/1000)) + geom_boxplot() + scale_y_log10(breaks = c(2.5, 5, 10, 20, 40, 80, 160, 320, 640, 1280)) + ylab(&quot;Ingreso trimestral (miles de pesos)&quot;) En algunas encuestas se pregunta directamente el ingreso mensual del hogar. La respuesta es generalmente una mala estimación del verdadero ingreso, por lo que actualmente se prefiere utilizar aprendizaje para estimar a partir de otras variables que son más fielmente reportadas por encuestados (años de estudio, ocupación, número de focos en el hogar, etc.) Aprendizaje supervisado Las tareas de aprendizaje se divide en dos grandes partes: aprendizaje supervisado y aprendizaje no supervisado. Aprendizaje supervisado Construir un modelo o algoritmo para predecir o estimar un target o una variable de salida a partir de ciertas variables de entrada. Predecir y estimar, en este contexto, se refieren a cosas similares. Generalmente se usa predecir cuando se trata de variables que no son observables ahora, sino en el futuro, y estimar cuando nos interesan variables actuales que no podemos observar ahora por costos o por la naturaleza del fenómeno. Por ejemplo, para identificar a los clientes con alto riesgo de impago de tarjeta de crédito, utilizamos datos históricos de clientes que han pagado y no han pagado. Con estos datos entrenamos un algoritmo para detectar anticipadamente los clientes con alto riesgo de impago. Usualmente dividimos los problemas de aprendizaje supervisado en dos tipos, dependiendo de la variables salida: Problemas de regresión: cuando la salida es una variable numérica. El ejemplo de estimación de ingreso es un problema de regresión Problemas de clasificación: cuando la salida es una variable categórica. El ejemplo de detección de dígitos escritos a manos es un problema de clasificación. Ejemplo: predecir el rendimiento de un coche. Estimar directamente el rendimiento (km por litro de combustible) de un coche es costoso: hay que hacer varias pruebas en diversas condiciones, etc. ¿Podríamos estimar el rendimiento de un coche usando variables más accesibles, peso del coche, año de producción, etc.? library(ISLR) datos &lt;- Auto[, c(&#39;name&#39;, &#39;weight&#39;,&#39;year&#39;, &#39;mpg&#39;)] datos$peso_kg &lt;- datos$weight*0.45359237 datos$rendimiento_kpl &lt;- datos$mpg*(1.609344/3.78541178) set.seed(213) datos_muestra &lt;- sample_n(datos, 50) datos_muestra %&gt;% select(name, peso_kg, rendimiento_kpl) ## name peso_kg rendimiento_kpl ## 9 pontiac catalina 2007.1462 5.952012 ## 139 dodge coronet custom (sw) 2021.6612 5.952012 ## 248 datsun b210 gx 938.9362 16.750662 ## 229 ford granada 1598.9131 7.865159 ## 166 chevrolet monza 2+2 1461.0210 8.502874 ## 321 datsun 510 hatchback 1104.0438 15.730317 ## 5 ford torino 1564.4401 7.227443 ## 145 toyota corona 747.9738 13.179455 ## 282 mercury zephyr 6 1356.2412 8.417845 ## 297 amc spirit dl 1211.0916 11.648938 ## 19 datsun pl510 966.1517 11.478880 ## 320 mazda 626 1153.0318 13.306998 ## 218 buick opel isuzu deluxe 977.4916 12.754311 ## 1 chevrolet chevelle malibu 1589.3877 7.652587 ## 195 amc hornet 1399.3325 9.565733 ## 317 dodge aspen 1533.5958 8.120245 ## 35 plymouth satellite custom 1559.9042 6.802299 ## 356 honda prelude 1002.4391 14.327343 ## 250 oldsmobile cutlass salon brougham 1526.3383 8.460360 ## 373 pontiac phoenix 1240.5751 11.478880 ## 80 renault 12 (sw) 992.9137 11.053736 ## 201 ford granada ghia 1621.1391 7.652587 ## 202 pontiac ventura sj 1653.3442 7.865159 ## 59 dodge colt hardtop 964.3374 10.628593 ## 277 saab 99gle 1267.7907 9.183104 ## 108 amc gremlin 1265.0691 7.652587 ## 329 mercedes-benz 240d 1474.1752 12.754311 ## 220 plymouth arrow gs 1043.2625 10.841165 ## 209 plymouth volare premier v8 1787.1539 5.526868 ## 263 chevrolet monte carlo landau 1553.5539 8.162759 ## 178 audi 100ls 1221.9778 9.778305 ## 182 honda civic cvcc 814.1983 14.029742 ## 16 plymouth duster 1285.0272 9.353162 ## 191 ford gran torino 1911.8918 6.164584 ## 113 ford pinto 1047.7984 8.077730 ## 285 dodge aspen 6 1524.0704 8.757960 ## 49 ford mustang 1423.8264 7.652587 ## 243 bmw 320i 1179.3402 9.140590 ## 271 toyota celica gt liftback 1140.7848 8.970532 ## 349 toyota tercel 929.8644 16.027918 ## 339 plymouth reliant 1129.4450 11.563909 ## 309 pontiac phoenix 1159.3821 14.242314 ## 345 plymouth champ 850.4857 16.580605 ## 91 mercury marquis brougham 2246.1894 5.101724 ## 275 audi 5000 1283.6664 8.630417 ## 46 amc hornet sportabout (sw) 1343.5406 7.652587 ## 255 ford fairmont (auto) 1344.9014 8.587903 ## 7 chevrolet impala 1974.9412 5.952012 ## 378 plymouth horizon miser 963.8838 16.155461 ## 6 ford galaxie 500 1969.0445 6.377156 Y podríamos comenzar graficando rendimiento contra peso. Cada punto representa un coche distinto. En esta gráfica vemos que los valores de rendimiento varían según según peso de una manera sistemática: cuanto más grande es el peso, más bajo es el rendimiento: library(ggplot2) ggplot(datos_muestra, aes(x=peso_kg, y=rendimiento_kpl)) + geom_point() Podemos entonces ajustar una curva, que para cada nivel de peso da un valor de rendimiento que se ‘aleja lo menos posible’ de los valores de rendimiento cercanos. Por ejemplo: según la curva roja, ¿cómo haríamos la predicción para un peso de 1500 kg? ggplot(datos_muestra, aes(x=peso_kg, y=rendimiento_kpl)) + geom_point() + geom_smooth(se =FALSE, colour=&#39;red&#39;, size=1.1, span=0.4, method=&#39;loess&#39;) + geom_smooth(se =FALSE, colour=&#39;gray&#39;, size=1.1, span=2, method=&#39;loess&#39;) Aprendizaje no supervisado Aprendizaje no supervisado En este caso no hay target o variable salida. Buscamos modelar y entender las relaciones entre variables y entre observaciones, o patrones importantes o interesantes en los datos. Los problemas supervisados tienen un objetivo claro: hacer las mejores predicciones posibles bajo ciertas restricciones. Los problemas no supervisados tienden a tener objetivos más vagos, y por lo mismo pueden ser más difíciles. Ejemplo: tipos de coches en el mercado Quisieramos encontrar categorías de coches tales que: las categorías son diferentes entre sí, y los coches en una misma categoría son similares entre sí. Esta agrupación nos permite entender la estructura general de los datos, cómo están organizados en términos de similitud de características. En este ejemplo, encontramos un plano de máxima variabilidad donde proyectamos los coches, y después formamos grupos de coches similares: autos &lt;- Auto %&gt;% select(mpg, displacement, horsepower, acceleration) comps_autos &lt;- princomp(autos, cor = TRUE) clust &lt;- hclust(dist(comps_autos$scores[,1:2]), method = &#39;ward.D&#39;) autos$grupo &lt;- cutree(clust, k = 4) autos$Comp.1 &lt;- comps_autos$scores[,1] autos$Comp.2 &lt;- comps_autos$scores[,2] autos$nombre &lt;- Auto$name ggplot(autos, aes(x=Comp.1, y=Comp.2, colour=factor(grupo), label=nombre)) + geom_point() ¿Cómo interpretamos los grupos? head(filter(autos, grupo==1)) ## mpg displacement horsepower acceleration grupo Comp.1 Comp.2 ## 1 18 307 130 12.0 1 -1.817719 0.5042535 ## 2 15 350 165 11.5 1 -2.800712 0.3938195 ## 3 18 318 150 11.0 1 -2.310357 0.7966085 ## 4 16 304 150 12.0 1 -2.213807 0.3989781 ## 5 17 302 140 10.5 1 -2.225309 0.9183779 ## 6 15 429 198 10.0 1 -3.900596 0.6915313 ## nombre ## 1 chevrolet chevelle malibu ## 2 buick skylark 320 ## 3 plymouth satellite ## 4 amc rebel sst ## 5 ford torino ## 6 ford galaxie 500 head(filter(autos, grupo==3)) ## mpg displacement horsepower acceleration grupo Comp.1 Comp.2 ## 1 22 198 95 15.5 3 0.01913364 -0.090471378 ## 2 18 199 97 15.5 3 -0.26705470 -0.339015545 ## 3 21 200 85 16.0 3 0.16412490 -0.315611651 ## 4 21 199 90 15.0 3 -0.05362631 -0.004579963 ## 5 19 232 100 13.0 3 -0.79359758 0.413938751 ## 6 16 225 105 15.5 3 -0.63973365 -0.517394423 ## nombre ## 1 plymouth duster ## 2 amc hornet ## 3 ford maverick ## 4 amc gremlin ## 5 amc gremlin ## 6 plymouth satellite custom head(filter(autos, grupo==2)) ## mpg displacement horsepower acceleration grupo Comp.1 Comp.2 ## 1 24 113 95 15.0 2 0.50234800 0.3800473 ## 2 27 97 88 14.5 2 0.79722704 0.7509781 ## 3 24 107 90 14.5 2 0.52837050 0.5437610 ## 4 26 121 113 12.5 2 -0.04757934 1.2605758 ## 5 27 97 88 14.5 2 0.79722704 0.7509781 ## 6 28 140 90 15.5 2 0.76454526 0.4100595 ## nombre ## 1 toyota corona mark ii ## 2 datsun pl510 ## 3 audi 100 ls ## 4 bmw 2002 ## 5 datsun pl510 ## 6 chevrolet vega 2300 head(filter(autos, grupo==4)) ## mpg displacement horsepower acceleration grupo Comp.1 Comp.2 ## 1 26 97 46 20.5 4 2.2421696 -1.1703377 ## 2 25 110 87 17.5 4 1.0737328 -0.3205227 ## 3 25 104 95 17.5 4 0.9902507 -0.3021997 ## 4 22 140 72 19.0 4 1.1727317 -1.0419917 ## 5 30 79 70 19.5 4 2.0927389 -0.5620939 ## 6 31 71 65 19.0 4 2.1920905 -0.3319627 ## nombre ## 1 volkswagen 1131 deluxe sedan ## 2 peugeot 504 ## 3 saab 99e ## 4 chevrolet vega (sw) ## 5 peugeot 304 ## 6 toyota corolla 1200 1.2 Aprendizaje Supervisado Por el momento nos concentramos en problemas supervisados de regresión, es decir predicción de variables numéricas. ¿Cómo entendemos el problema de predicción? Proceso generador de datos (modelo teórico) Para entender lo que estamos intentando hacer, pensaremos en términos de modelos probabilísticos que generan los datos. La idea es que estos representan los procesos que generan los datos o las observaciones. Si \\(Y\\) es la respuesta que queremos predecir, y \\(X\\) es una entrada que queremos usar para predecir \\(Y\\), consideramos que las variables aleatorias \\(Y\\) y \\(X\\) están relacionadas como sigue: \\[Y=f(X)+\\epsilon,\\] donde \\(\\epsilon\\) es una término de error aleatorio que no depende de \\(X\\), y que tiene valor esperado \\(\\textrm{E}(\\epsilon)=0\\). \\(f\\) expresa la relación sistemática que hay entre \\(Y\\) y \\(X\\): para cada valor posible de \\(X\\), la contribución de \\(X\\) a \\(Y\\) es \\(f(X)\\). Pero \\(X\\) no determina a \\(Y\\), como en el ejemplo anterior de rendimiento de coches. Entonces agregamos una error aleatorio \\(\\epsilon\\), con media cero (si la media no es cero podemos agregar una constante a \\(f\\)), que no contiene información acerca de \\(X\\) (independiente de \\(X\\)). \\(\\epsilon\\) representa, por ejemplo, el efecto de variables que no hemos medido o procesos aleatorios que determinan la respuesta. Ejemplo Vamos a usar simulación para entender estas ideas: supongamos que \\(X\\) es el número de años de estudio de una persona y \\(Y\\) es su ingreso mensual. En primer lugar, estas son el número de años de estudio de 8 personas: x &lt;- c(1,7,10,0,0,5,9,13,2,4,17,18,1,2) Ahora supondremos que la dependencia de Y de X está dada por \\(Y=f(X)+\\epsilon\\) por una función \\(f\\) que no conocemos (esta función está determinada por el fenómeno) f &lt;- function(x){ ifelse(x &lt; 10, 1000*sqrt(x), 1000*sqrt(10)) } El ingreso no se determina únicamente por número de años de estudio. Suponemos entonces que hay algunas variables adicionales que perturban los niveles de \\(f(X)\\) por una cantidad aleatoria. Los valores que observamos de \\(Y\\) están dados entonces por \\(Y=f(X)+\\epsilon\\). Entonces podríamos obtener, por ejemplo: x_g &lt;- seq(0,20,0.5) y_g &lt;- f(x_g) dat_g &lt;- data.frame(x = x_g, y = y_g) set.seed(281) error &lt;- rnorm(length(x), 0, 500) y &lt;- f(x) + error datos &lt;- data_frame(x = x, y = y) datos$y_media &lt;- f(datos$x) ggplot(datos, aes(x = x, y = y)) + geom_point() + geom_line(data=dat_g, colour = &#39;blue&#39;, size = 1.1) + geom_segment(aes(x = x, xend = x, y = y, yend = y_media), col=&#39;red&#39;) En problemas de aprendizaje nunca conocemos esta \\(f\\) verdadera, aunque quizá sabemos algo acerca de sus propiedades (por ejemplo, continua, de variación suave). Lo que tenemos son los datos, que también podrían haber resultado en (para otra muestra de personas, por ejemplo): set.seed(28015) error &lt;- rnorm(length(x), 0, 500) y &lt;- f(x) + error datos &lt;- data.frame(x = x, y = y) ggplot(datos, aes(x = x, y = y)) + geom_point() La siguiente observación nos da una idea de lo que intentamos hacer, aunque todavía es vaga y requiere refinamiento: Bajo los supuestos del modelo \\(Y=f(X)+\\epsilon\\), aprender de los datos significa intentar recuperar o estimar la forma de la función \\(f\\) que no conocemos. \\(f\\) representa la relación sistemática entre \\(Y\\) y \\(X\\). ¿Qué tan bien podemos estimar esa \\(f\\) que no conocemos, con los datos disponibles? ¿Qué significa estimar bien? Incluso este ejemplo tan simple muestra las dificultades que vamos a enfrentar, y la importancia de determinar con cuidado qué tanta información tenemos, y qué tan buenas pueden ser nuestras predicciones. 1.3 Predicciones La idea es entonces producir una estimación de f que nos permita hacer predicciones. Si denotamos por \\(\\hat{f}\\) a una estimación de \\(f\\) construida a partir de los datos, podemos hacer predicciones aplicando \\(\\hat{f}\\) a valores de \\(X\\). La predicción de Y la denotamos por \\(\\hat{Y}\\), y \\[\\hat{Y}=\\hat{f}(X).\\] El error de predicción (residual) está dado por el valor observado menos la predicción: \\[Y-\\hat{Y}.\\] En nuestro ejemplo anterior, podríamos construir, por ejemplo, una recta ajustada por mínimos cuadrados: curva_1 &lt;- geom_smooth(data=datos, method = &quot;lm&quot;, se=FALSE, color=&quot;red&quot;, formula = y ~ x, size = 1.1) ggplot(datos, aes(x = x, y = y)) + geom_point() + curva_1 En este caso \\(\\hat{f}\\) es una recta, y la podemos usar para hacer predicciones. Por ejemplo, si tenemos una observación con \\(x_0=8\\) años de estudio, nuestra predicción del ingreso \\(\\hat{y}=\\hat{f}(8)\\) sería lineal &lt;- lm(y ~ x,data = datos) pred_1 &lt;- predict(lineal, newdata = data.frame(x=8)) pred_1 ## 1 ## 2193.561 ggplot(datos, aes(x = x, y = y)) + geom_point() + curva_1 + geom_segment(x = 0, xend = 8, y = pred_1, yend = pred_1, colour = &#39;salmon&#39;) + geom_segment(x = 8, xend = 8, y = 0, yend = pred_1, colour = &#39;salmon&#39;) + annotate(&#39;text&#39;, x = 0.5, y = pred_1 + 100, label = round(pred_1, 1)) + geom_point( x= 8, y =3200, col=&#39;green&#39;, size = 4) Si observamos que para esta observación con \\(x_0=8\\), resulta que el correspondiente ingreso es \\(y_0=3200\\), entonces el error sería y_0 &lt;- 3200 y_0 - pred_1 ## 1 ## 1006.439 En aprendizaje buscamos que estos errores sean lo más cercano a cero que sea posible. 1.4 Tarea de aprendizaje supervisado El elemento faltante para definir la tarea de aprendizaje supervisado es cuantificar qué significa aproximar bien a \\(f\\), o tener predicciones precisas. Para esto definimos una función de pérdida: \\[L(Y, \\hat{f}(X)),\\] que nos dice cuánto nos cuesta hacer la predicción \\(\\hat{f}(X)\\) cuando el verdadero valor es \\(Y\\) y las variables de entrada son \\(X\\). Una opción conveniente para problemas de regresión es la pérdida cuadrática: \\[L(Y, \\hat{f}(X)) = (Y - \\hat{f}(X))^2\\] Esta es una cantidad aleatoria, de modo que en algunos casos este error puede ser más grande o más chico. Usualmente buscamos una \\(\\hat{f}\\) de modo que el error promedio sea chico: \\[Err = E (Y - \\hat{f}(X))^2 \\] Notas: Este valor esperado es sobre la población para la que queremos hacer predicciones. Es una cantidad teórica, no podemos calcularla con ningún conjunto de datos Intenta demostrar que bajo error cuadrático medio y suponiendo el modelo aditivo \\(Y=f(X)+\\epsilon\\), el mejor predictor de \\(Y\\) es \\(f(x)= E[Y|X=x]\\). Es decir: lo que nos interesa es aproximar lo mejor que se pueda la esperanza condicional Ahora tenemos los elementos para definir con precisión el problema de aprendizaje supervisado. Consideramos un proceso generador de datos \\((X,Y)\\). En primer lugar, tenemos datos de los que vamos a aprender. Supongamos entonces que tenemos un conjunto de datos etiquetados (generados según \\((X,Y)\\)) \\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}\\] que llamamos conjunto de entrenamiento. Nótese que usamos minúsculas para denotar observaciones particulares de \\((X,Y)\\). Un algoritmo de aprendizaje (aprender de los datos) es una regla que asigna a cada conjunto de entrenamiento \\({\\mathcal L}\\) una función \\(\\hat{f}\\): \\[{\\mathcal L} \\to \\hat{f}.\\] Una vez que construimos la función \\(\\hat{f}\\), podemos hacer predicciones. El desempeño del predictor particular \\(\\hat{f}\\) se mide como sigue: si en el futuro observamos otra muestra \\({\\mathcal T}\\), que llamamos muestra de prueba, \\[{\\mathcal T}=\\{ (x_0^{(1)},y_0^{(1)}),(x_0^{(2)},y_0^{(2)}), \\ldots, (x_0^{(m)}, y_0^{(m)}) \\}\\] entonces decimos que el error de predicción (cuadrático) de \\(\\hat{f}\\) para el ejemplo \\((x_0^{(j)},y_0^{(j)})\\) está dado por \\[(y_0^{(j)} - \\hat{f}(x_0^{(j)}))^2\\] y el error promedio sobre la muestra \\({\\mathcal T}\\) es \\[\\hat{Err} = \\frac{1}{m}\\sum_{j=1}^m (y_0^{(j)} - \\hat{f}(x_0^{(j)}))^2\\] que es una estimación del error de predicción \\[Err = E (Y - \\hat{f}(X))^2 \\] Adicionalmente, definimos otra cantidad de menor interés, el error de entrenamiento, como \\[\\overline{err} = \\frac{1}{N}\\sum_{i=1}^N (y^{(i)} - \\hat{f}(x^{(i)}))^2.\\] El punto más importante que discutiremos ahora es el siguiente: Nótese que el error de entrenamiento se calcula sobre la muestra \\({\\mathcal L}\\) que se usó para construir \\(\\hat{f}\\), mientras que el error de prueba se calcula usando una muestra independiente \\({\\mathcal T}\\). \\(\\hat{Err}\\) es una estimación razonable de el error de predicción \\(Err\\) (por ejemplo, \\(\\hat{Err} \\to Err\\) cuando el tamaño de la muestra de prueba crece), pero \\(\\overline{err}\\) típicamente es una estimación mala del error de predicción. Ejemplo En el ejemplo que hemos estado usando, ¿que curva preferirías para predecir, la gris, la roja o la azul? ¿Cuál tiene menor error de entrenamiento? set.seed(280572) error &lt;- rnorm(length(x), 0, 500) y &lt;- f(x) + error datos_entrena &lt;- data.frame(x=x, y=y) head(datos_entrena) ## x y ## 1 1 86.22033 ## 2 7 2353.75863 ## 3 10 3078.71029 ## 4 0 -397.80229 ## 5 0 424.73363 ## 6 5 3075.92998 curva_1 &lt;- geom_smooth(data=datos_entrena, method = &quot;loess&quot;, se=FALSE, color=&quot;gray&quot;, span=1, size=1.1) curva_2 &lt;- geom_smooth(data=datos_entrena, method = &quot;loess&quot;, se=FALSE, color=&quot;red&quot;, span=0.5, size=1.1) curva_3 &lt;- geom_smooth(data=datos_entrena, method = &quot;lm&quot;, se=FALSE, color=&quot;blue&quot;, size=1.1) ggplot(datos_entrena, aes(x=x, y=y)) + geom_point() + curva_1 + curva_2 + curva_3 Calculamos los errores de entrenamiento de cada curva: mod_rojo &lt;- loess(y ~ x, data = datos_entrena, span=0.3) mod_gris &lt;- loess(y ~ x, data = datos_entrena, span=1) mod_recta &lt;- lm(y ~ x, data = datos_entrena) df_mods &lt;- data_frame(nombre = c(&#39;recta&#39;, &#39;rojo&#39;,&#39;gris&#39;)) df_mods$modelo &lt;- list(mod_recta, mod_rojo, mod_gris) error_f &lt;- function(df, mod){ function(mod){ preds &lt;- predict(mod, newdata = df) round(sqrt(mean((preds - df$y) ^ 2))) } } error_ent &lt;- error_f(datos_entrena) df_mods &lt;- df_mods %&gt;% mutate(error_entrena = map_dbl(modelo, error_ent)) df_mods ## # A tibble: 3 x 3 ## nombre modelo error_entrena ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; ## 1 recta &lt;S3: lm&gt; 782 ## 2 rojo &lt;S3: loess&gt; 189 ## 3 gris &lt;S3: loess&gt; 389 El error de entrenamiento es considerablemente menor para la curva roja, y es más grande para la recta. Sin embargo, consideremos que tenemos una nueva muestra (de prueba). set.seed(218052272) x_0 &lt;- sample(0:13, 100, replace = T) error &lt;- rnorm(length(x_0), 0, 500) y_0 &lt;- f(x_0) + error datos_prueba &lt;- data_frame(x = x_0, y = y_0) datos_prueba ## # A tibble: 100 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 9 2156. ## 2 11 3227. ## 3 3 2382. ## 4 10 3482. ## 5 7 2733. ## 6 7 2326. ## 7 12 3464. ## 8 0 -564. ## 9 10 3296. ## 10 0 366. ## # ... with 90 more rows error_p &lt;- error_f(datos_prueba) df_mods &lt;- df_mods %&gt;% mutate(error_prueba = map_dbl(modelo, error_p)) df_mods ## # A tibble: 3 x 4 ## nombre modelo error_entrena error_prueba ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 recta &lt;S3: lm&gt; 782 801 ## 2 rojo &lt;S3: loess&gt; 189 628 ## 3 gris &lt;S3: loess&gt; 389 520 Observaciones El “mejor”&quot; modelo en entrenamiento es uno que sobreajusta a los datos, pero es el peor con una muestra de prueba. La curva roja aprende de la componente de ruido del modelo - lo cual realmente no es aprendizaje. El modelo de la recta no es bueno en entrenamiento ni en prueba. Este modelo no tiene la capacidad para aprender de la señal en los datos. El mejor modelo en la muestra de prueba es uno que está entre la recta y la curva roja en términos de flexibilidad. Nuestra intuición para escoger el modelo gris desde el principio se refleja en que generaliza mejor que los otros, y eso a su vez se refleja en un error de prueba más bajo. ¿De dónde provienen los errores en la predicción? ¿Podemos hacer el error igual a cero? Si establemos que el error es una función creciente de \\(Y-\\hat{Y}\\), vemos que \\[ Y-\\hat{Y} = f(X) + \\epsilon - \\hat{f}(X)= (f(X) - \\hat{f}(X)) + \\epsilon,\\] donde vemos que hay dos componentes que pueden hacer grande a \\(Y-\\hat{Y}\\): La diferencia \\(f(X) - \\hat{f}(X)\\) está asociada a error reducible, pues depende de qué tan bien estimemos \\(f(X)\\) con \\(\\hat{f}(X)\\) El error aleatorio \\(\\epsilon\\), asociado a error irreducible. Cualquiera de estas dos cantidades pueden hacer que nuestras predicciones no sean precisas. No podemos hacer mucho acerca del error irreducible (sin cambiar las variables que usamos, la definición del problema, etc.) En nuestro ejemplo anterior, el error reducible: Es grande para el modelo rojo, pues responde demasiado fuerte a ruido en los datos (tiene varianza alta). Es grande para el modelo de la recta, pues no tiene capacidad para acercarse a la verdadera curva (está sesgado). 1.5 Balance de complejidad y rigidez Como vimos en el ejemplo de arriba, el error de entrenamiento no es un buen indicador del desempeño futuro de nuestras predicciones. Para evaluar este desempeño, necesitamos una muestra de prueba independiente de la muestra que usamos para aprender o para entrenar el modelo. Intuitivamente esto tiene sentido: en el proceso de aprendizaje tenemos disponibles las etiquetas (sabemos las respuestas), de modo que puede suceder que el algoritmo memorice la asociación de qué etiquetas \\(y^{(i)}\\) van con cada conjunto de entradas \\(x^{(i)}\\). Esto se dice de varias maneras, por ejemplo: El modelo sobreajusta a los datos: esto quiere decir que por ajustar aspectos de los datos de entrenamiento demasiado fuertemente, el algoritmo parece replicar de cerca los datos de entrenamiento pero se desempeña mal en la predicción. El modelo aprende del ruido: nuestro proceso de aprendizaje captura aspectos irrelevantes de los datos, que nuevos datos no van a compartir. El modelo no tiene capacidad de generalización, porque captura aspectos que solo están presentes en nuestra muestra de entrenamiento. El modelo tiene varianza alta, porque cambia mucho dependiendo de la muestra de entrenamiento. El modelo es demasiado complejo o flexible y fácilmente se adapta a cualquier conjunto de datos, tanto señal como ruido En el ejemplo de arriba, también vimos que algunos modelos pueden tener desempeño malo porque no tienen la capacidad de aprender de patrones reales y generales en los datos (la recta en el ejemplo anterior). Podemos decir esto de varias maneras: El modelo subajusta a los datos: no tienen la capacidad de ajustar aspectos de los datos de entrenamiento que son relaciones reales entre las variables. El modelo ignora señal en los datos: el algoritmo no captura aspectos relevantes de los datos, que comparten con nuevos datos y pueden utilizarse para hacer predicciones. El modelo no tiene capacidad de aprendizaje, pues no puede capturar aspectos que son generales para el fenómeno de interés. El modelo tiene sesgo alto, porque no puede ajustar patrones generalizables en los datos. El modelo es demasiado rígido, y no puede adaptarse ni siquiera a patrones fuertes y claros en los datos. Logramos buenas predicciones cuando refinamos nuestros modelos o algoritmos para lograr aprender de la señal e ignorar el ruido, que no ayuda en la predicción, y lograr reducir el error de predicción lo más posible con los datos disponibles. Esto requiere buscar el nivel adecuado de complejidad en los modelos o algoritmos para los datos que tenemos. Para construir buenos predictores, requerimos que: El algoritmo tenga la flexibilidad necesaria para capturar patrones generales y fuertes en los datos El algoritmo tenga la rigidez necesaria para tener robustez a patrones de ruido o particularidades no repetibles de nuestra muestra de entrenamiento. Saber intuitivamente cuál es el grado adecuado de complejidad para un problema dado es difícil. Para decidirlo, evaluamos el desempeño de nuestros métodos usando una muestra de prueba. El nivel adecuado de complejidad se traduce en menos errores de predicción. 1.5.0.1 Discusión (error de entrenamiento y prueba) En términos teóricos, podemos ver cuál es el problema de intentar evaluar el error de predicción utilizando la muestra de entrenamiento. En primer lugar consideremos evaluar el error de predicción para un ejemplo \\[(y_0- \\hat{f}(x_0))^2\\] donde \\((x_0, y_0)\\) es independiente de la muestra de entrenamiento. En este caso, la \\(\\hat{f}\\) está fija, y el valor esperado (error de predicción) nos da el error de predicción. Sin embargo, si \\((x,y)\\) es un caso de entrenamiento, el valor esperado de \\[(y- \\hat{f}(x))^2\\] requiere un cálculo más complicado, pues ¡ \\(\\hat{f}\\) también depende de \\((x,y)\\), pues se construye con la muestra de entrenamiento ! Esta cantidad podría ser igual a cero para cualquier \\((x,y)\\) (si nuestro algoritmo “interpola” como el en la curva roja del ejemplo anterior), y no necesariamente tiene qué ver con el error de predicción. En general, el error de entrenamiento es una cantidad secundaria, que utilizaremos más como medida de diagnóstico de nuestro proceso de ajuste. La cantidad que realmente queremos hacer chica es el error de predicción, que evaluamos con una muestra de prueba independiente de la muestra de entrenamiento. Para modelos muy simples, el error de entrenamiento puedes ser similar al de prueba. Sin embargo, conforme aumentamos complejidad (necesario para capturar patrones reales en los datos), estos dos errores típicamente divergen. 1.6 ¿Cómo estimar f? Ahora mostramos otro aspecto característico del aprendizaje supervisado. En primer lugar, el método general más usual para encontrar \\(\\hat{f}\\) es hacer lo siguiente: Consideramos una familia de funciones \\(h\\) candidatas para aproximar \\(f\\) Calculamos el error de entrenamiento de cada posible \\(h\\), y encontramos la \\(h\\) que minimiza el error de entrenamiento (la que más se ajusta a los datos de entrenamiento). Tomamos \\(\\hat{f} = h\\). \\[\\hat{f} = \\min_h \\frac{1}{N}\\sum_{i=1}^N (y^{(i)} - h(x^{(i)}))^2.\\] Evaluar el error de predicción del modelo que seleccionamos (queremos que sea bajo): \\[\\hat{Err} = \\frac{1}{m}\\sum_{j=1}^m (y_0^{(j)} - \\hat{f}(x_0^{(j)}))^2\\] De modo que el proceso es un problema de minimización. Lo que hace interesante nuestro caso es que realmente no queremos minimizar el error de entrenamiento. Queremos minimizar el error de prueba. O sea que minimizamos una cantidad que realmente no nos interesa (error de entrenamiento) con la esperanza de minimizar la cantidad que nos interesa (error de predicción). Como es de esperarse, este esquema simple no funciona muy bien en general. Para que la solución anterior sea razonable o buena, entonces: Tenemos que ser cuidadosos y poder regular la elección de la familia inicial de funciones (rectas? curvas muy flexibles? etc.), y/o A veces tenemos que modificar el objetivo del problema de minimización para que nos obligue encontrar un balance adecuado de complejidad y error de predicción bajo. Por ejemplo, penalizar el objetivo de modelos que son poco creíbles o demasiado complicados. Perturbar la muestra de entrenamiento de distintas maneras para evitar que un algoritmo aprenda información irrelevante La mayor parte del curso se concentra en considerar qué familias podemos utilizar, qué modificaciones de la función objetivo pueden hacerse, y qué perturbaciones pueden considerarse mejorar el desempeño predictivo de nuestros modelos. 1.7 Resumen Aprendizaje de máquina: algoritmos que aprenden de los datos para predecir cantidades numéricas, o clasificar (aprendizaje supervisado), o para encontrar estructura en los datos (aprendizaje no supervisado). En aprendizaje supervisado, el esquema general es: Un algoritmo aprende de una muestra de entrenamiento \\({\\mathcal L}\\), que es generada por el proceso generador de datos que nos interesa. Eso quiere decir que produce una función \\(\\hat{f}\\) (a partir de \\({\\mathcal L}\\)) que nos sirve para hacer predicciones \\(x \\to \\hat{f}(x)\\) de \\(y\\) El error de predicción del algoritmo es \\(Err\\), que mide en promedio qué tan lejos están las predicciones de valores reales. Para estimar esta cantidad usamos una muestra de prueba \\({\\mathcal T}\\), que es independiente de \\({\\mathcal L}\\). Esta es porque nos interesa el desempeño futuro de \\(\\hat{f}\\) para nuevos casos que el algoritmo no ha visto (esto es aprender). El error en la muestra de entrenamiento no necesariamente es buen indicador del desempeño futuro de nuestro algoritmo. Para obtener las mejores predicciones posibles, es necesario que el algoritmo sea capaz de capturar patrones en los datos, pero no tanto que tienda a absorber ruido en la estimación - es un balance de complejidad y rigidez. En términos estadísticos, se trata de un balance de varianza y sesgo. 1.8 Tarea En el ejemplo simple que vimos en la sección 1.4, utilizamos una sola muestra de entrenamiento para evaluar el algoritmo. ¿Será posible que escogimos una muestra atípica? Corre el ejemplo con otra muestra y reporta tus resultados de error de entrenamiento y error de prueba para los tres métodos. Opcional (difícil): evalúa los tres métodos comparando estos valores para un número grande de distintas simulaciones de los datos de entrenamiento. "],
["regresion.html", "Clase 2 Regresión lineal 2.1 Introducción 2.2 Aprendizaje de coeficientes (ajuste) 2.3 Descenso en gradiente 2.4 Descenso en gradiente para regresión lineal 2.5 Normalización de entradas 2.6 Interpretación de modelos lineales 2.7 Solución analítica 2.8 ¿Por qué el modelo lineal funciona bien (muchas veces)? Tarea", " Clase 2 Regresión lineal 2.1 Introducción Consideramos un problema de regresión con entradas \\(X=(X_1,X_2,\\ldots, X_p)\\) y respuesta \\(Y\\). Una de las maneras más simples que podemos intentar para predecir \\(Y\\) en función de las \\(X_j\\)´s es mediante una suma ponderada de los valores de las \\(X_j&#39;s\\), usando una función \\[f_\\beta (X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p,\\] Nuestro trabajo será entonces, dada una muestra de entrenamiento \\({\\mathcal L}\\), encontrar valores apropiados de las \\(\\beta\\)’s, para construir un predictor: \\[\\hat{f}(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\hat{\\beta}_2 X_2 \\cdots + \\hat{\\beta} X_p\\] y usaremos esta función \\(\\hat{f}\\) para hacer predicciones \\(\\hat{Y} =\\hat{f}(X)\\). 2.1.0.1 Ejemplos Queremos predecir las ventas futuras anuales \\(Y\\) de un supermercado que se va a construir en un lugar dado. Las variables que describen el lugar son \\(X_1 = trafico\\_peatones\\), \\(X_2=trafico\\_coches\\). En una aproximación simple, podemos suponer que la tienda va a capturar una fracción de esos tráficos que se van a convertir en ventas. Quisieramos predecir con una función de la forma \\[f_\\beta (peatones, coches) = \\beta_0 + \\beta_1\\, peatones + \\beta_2\\, coches.\\] Por ejemplo, después de un análisis estimamos que \\(\\hat{\\beta}_0 = 1000000\\) (ventas base) \\(\\hat{\\beta}_1 = (200)*0.02 = 4\\) \\(\\hat{\\beta}_2 = (300)*0.01 =3\\) Entonces haríamos predicciones con \\[\\hat{f}(peatones, coches) = 1000000 + 4\\,peatones + 3\\, coches\\] El modelo lineal es más flexible de lo que parece en una primera aproximación, porque tenemos libertad para construir las variables de entrada a partir de nuestros datos. Por ejemplo, si tenemos una tercera variable \\(estacionamiento\\) que vale 1 si hay un estacionamiento cerca o 0 si no lo hay, podríamos definir las variables \\(X_1= peatones\\) \\(X_2 = coches\\) \\(X_3 = estacionamiento\\) \\(X_4 = coches*estacionamiento\\) Donde la idea de agregar \\(X_4\\) es que si hay estacionamiento entonces vamos a capturar una fracción adicional del trafico de coches, y la idea de \\(X_3\\) es que la tienda atraerá más nuevas visitas si hay un estacionamiento cerca. Buscamos ahora modelos de la forma \\[f_\\beta(X_1,X_2,X_3,X_4) = \\beta_0 + \\beta_1X_1 + \\beta_2 X_2 + \\beta_3 X_3 +\\beta_4 X_4\\] y podríamos obtener después de nuestra análisis las estimaciones \\(\\hat{\\beta}_0 = 800000\\) (ventas base) \\(\\hat{\\beta}_1 = 4\\) \\(\\hat{\\beta}_2 = (300)*0.005 = 1.5\\) \\(\\hat{\\beta}_3 = 400000\\) \\(\\hat{\\beta}_4 = (300)*0.02 = 6\\) y entonces haríamos predicciones con el modelo \\[\\hat{f} (X_1,X_2,X_3,X_4) = 800000 + 4\\, X_1 + 1.5 \\,X_2 + 400000\\, X_3 +6\\, X_4\\] 2.2 Aprendizaje de coeficientes (ajuste) En el ejemplo anterior, los coeficientes fueron calculados (o estimados) usando experiencia, argumentos teóricos, o quizá otras fuentes de datos (como estudios o encuestas, conteos, etc.) Ahora quisiéramos construir un algoritmo para aprender estos coeficientes del modelo \\[f_\\beta (X_1) = \\beta_0 + \\beta_1 X_1 + \\cdots \\beta_p X_p\\] a partir de una muestra de entrenamiento \\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}\\] El criterio de ajuste (algoritmo de aprendizaje) más usual para regresión lineal es el de mínimos cuadrados. Construimos las predicciones (ajustados) para la muestra de entrenamiento: \\[\\hat{y}^{(i)} = f_\\beta (x^{(i)}) = \\beta_0 + \\beta_1 x_1^{(i)}+ \\cdots + \\beta_p x_p^{(i)}\\] Y consideramos las diferencias de los ajustados con los valores observados: \\[e^{(i)} = y^{(i)} - f_\\beta (x^{(i)})\\] La idea entonces es minimizar la suma de los residuales al cuadrado, para intentar que la función ajustada pase lo más cercana a los puntos de entrenamiento que sea posible. Si \\[RSS(\\beta) = \\sum_{i=1}^N (y^{(i)} - f_\\beta(x^{(i)}))^2\\] Queremos resolver Mínimos cuadrados \\[\\min_{\\beta} RSS(\\beta) = \\min_{\\beta}\\sum_{i=1}^N (y^{(i)} - f_\\beta(x^{(i)}))^2\\] Observación: Como discutimos al final de las sección anterior, minimizar directamente el error de entrenamiento para encontrar los coeficientes puede resultar en en un modelo sobreajustado/con varianza alta/ruidoso. En la sección anterior discutimos tres grandes estrategias para mitigar este problema (restringir la familia de funciones, penalizar la función objetivo, perturbar la muestra de entrenamiento). El método mas común es cambiar la función objetivo, que discutiremos más adelante en la sección de regularización. 2.2.0.1 Ejemplo Consideremos library(readr) library(dplyr) library(knitr) prostata &lt;- read_csv(&#39;datos/prostate.csv&#39;) %&gt;% select(lcavol, lpsa, train) kable(head(prostata), format = &#39;html&#39;) lcavol lpsa train -0.5798185 -0.4307829 TRUE -0.9942523 -0.1625189 TRUE -0.5108256 -0.1625189 TRUE -1.2039728 -0.1625189 TRUE 0.7514161 0.3715636 TRUE -1.0498221 0.7654678 TRUE prostata_entrena &lt;- filter(prostata, train) ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() En este caso, buscamos ajustar el modelo (tenemos una sola entrada) \\(f_{\\beta} (X_1) = \\beta_0 + \\beta_1 X_1\\), que es una recta. Los cálculos serían como sigue: rss_calc &lt;- function(datos){ # esta función recibe los datos (x,y) y devuelve # una función f(betas) que calcula rss y &lt;- datos$lpsa x &lt;- datos$lcavol fun_out &lt;- function(beta){ y_hat &lt;- beta[1] + beta[2]*x e &lt;- (y - y_hat) rss &lt;- sum(e^2) 0.5*rss } fun_out } Nuestra función rss es entonces: rss_prostata &lt;- rss_calc(prostata_entrena) Por ejemplo, si consideramos \\((\\beta_0, \\beta_1) = (0, 1.5)\\), obtenemos beta &lt;- c(0,1.5) rss_prostata(beta) ## [1] 61.63861 Que corresponde a la recta ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() + geom_abline(slope = beta[2], intercept = beta[1], col =&#39;red&#39;) Podemos comparar con \\((\\beta_0, \\beta_1) = (1, 1)\\), obtenemos beta &lt;- c(1,1) rss_prostata(beta) ## [1] 27.11781 ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() + geom_abline(slope = beta[2], intercept = beta[1], col =&#39;red&#39;) Ahora minimizamos. Podríamos hacer res_opt &lt;- optim(c(0,0), rss_prostata, method = &#39;BFGS&#39;) beta_hat &lt;- res_opt$par beta_hat ## [1] 1.5163048 0.7126351 res_opt$convergence ## [1] 0 ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() + geom_abline(slope = 1, intercept = 1, col =&#39;red&#39;) + geom_abline(slope = beta_hat[2], intercept = beta_hat[1], size = 1.2) 2.3 Descenso en gradiente Aunque el problema de mínimos cuadrados se puede resolver analíticamente, proponemos un método numérico básico que es efectivo y puede escalarse a problemas grandes de manera relativamente simple: descenso en gradiente, o descenso máximo. Supongamos que una función \\(h(x)\\) es convexa y tiene un mínimo. La idea de descenso en gradiente es comenzar con un candidato inicial \\(z_0\\) y calcular la derivada en \\(z^{(0)}\\). Si \\(h(z^{(0)})&gt;0\\), la función es creciente en \\(z^{(0)}\\) y nos movemos ligeramente a la izquierda para obtener un nuevo candidato \\(z^{(1)}\\). si \\(h(z^{(0)})&lt;0\\), la función es decreciente en \\(z^{(0)}\\) y nos movemos ligeramente a la derecha para obtener un nuevo candidato \\(z^{(1)}\\). Iteramos este proceso hasta que la derivada es cercana a cero (estamos cerca del óptimo). Si \\(\\eta&gt;0\\) es una cantidad chica, podemos escribir \\[z^{(1)} = z^{(0)} - \\eta \\,h&#39;(z^{(0)}).\\] Nótese que cuando la derivada tiene magnitud alta, el movimiento de \\(z^{(0)}\\) a \\(z^{(1)}\\) es más grande, y siempre nos movemos una fracción de la derivada. En general hacemos \\[z^{(j+1)} = z^{(j)} - \\eta\\,h&#39;(z^{(j)})\\] para obtener una sucesión \\(z^{(0)},z^{(1)},\\ldots\\). Esperamos a que \\(z^{(j)}\\) converja para terminar la iteración. 2.3.0.1 Ejemplo Si tenemos h &lt;- function(x) x^2 + (x - 2)^2 - log(x^2 + 1) Calculamos (a mano): h_deriv &lt;- function(x) 2 * x + 2 * (x - 2) - 2*x/(x^2 + 1) Ahora iteramos con \\(\\eta = 0.4\\) y valor inicial \\(z_0=5\\) z_0 &lt;- 5 eta &lt;- 0.4 descenso &lt;- function(n, z_0, eta, h_deriv){ z &lt;- matrix(0,n, length(z_0)) z[1, ] &lt;- z_0 for(i in 1:(n-1)){ z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ]) } z } z &lt;- descenso(15, 5, eta, h_deriv) z ## [,1] ## [1,] 5.0000000 ## [2,] -1.2461538 ## [3,] 1.9571861 ## [4,] 0.7498212 ## [5,] 1.5340816 ## [6,] 1.0455267 ## [7,] 1.3722879 ## [8,] 1.1573987 ## [9,] 1.3013251 ## [10,] 1.2057209 ## [11,] 1.2696685 ## [12,] 1.2270627 ## [13,] 1.2555319 ## [14,] 1.2365431 ## [15,] 1.2492245 Y vemos que estamos cerca de la convergencia. dat_iteraciones &lt;- data_frame(iteracion = 1:nrow(z), x = z[, 1], y = h(z[, 1])) graf_descenso &lt;- ggplot(dat_iteraciones, aes(x = x, y = y)) + stat_function(fun = h) + geom_point(size = 4, color = &quot;red&quot;) + xlim(c(-2, 5)) if(FALSE){ library(gganimate) graf_descenso + labs(title = &#39;Iteración: {frame_time}&#39;) + transition_time(iteracion) anim_save(filename = &quot;figuras/descenso_1.gif&quot;) } knitr::include_graphics(&quot;figuras/descenso_1.gif&quot;) 2.3.1 Selección de tamaño de paso \\(\\eta\\) Si hacemos \\(\\eta\\) muy chico, el algoritmo puede tardar mucho en converger: z &lt;- descenso(20, 5, 0.01, h_deriv) curve(h, -3, 6) points(z, h(z)) text(z[1:6], h(z[1:6]), pos = 3) Si hacemos \\(\\eta\\) muy grande, el algoritmo puede divergir: z &lt;- descenso(20, 5, 1.5, h_deriv) z ## [,1] ## [1,] 5.000000e+00 ## [2,] -1.842308e+01 ## [3,] 9.795302e+01 ## [4,] -4.837345e+02 ## [5,] 2.424666e+03 ## [6,] -1.211733e+04 ## [7,] 6.059265e+04 ## [8,] -3.029573e+05 ## [9,] 1.514792e+06 ## [10,] -7.573955e+06 ## [11,] 3.786978e+07 ## [12,] -1.893489e+08 ## [13,] 9.467445e+08 ## [14,] -4.733723e+09 ## [15,] 2.366861e+10 ## [16,] -1.183431e+11 ## [17,] 5.917153e+11 ## [18,] -2.958577e+12 ## [19,] 1.479288e+13 ## [20,] -7.396442e+13 Es necesario ajustar el tamaño de paso para cada problema particular. Si la convergencia es muy lenta, podemos incrementarlo. Si las iteraciones divergen, podemos disminuirlo 2.3.2 Funciones de varias variables Si ahora \\(h(z)\\) es una función de \\(p\\) variables, podemos intentar la misma idea usando el gradiente. Por cálculo sabemos que el gradiente apunta en la dirección de máximo crecimiento local. El gradiente es el vector columna con las derivadas parciales de \\(h\\): \\[\\nabla h(z) = \\left( \\frac{\\partial h}{\\partial z_1}, \\frac{\\partial h}{\\partial z_2}, \\ldots, \\frac{\\partial h}{\\partial z_p} \\right)^t\\] Y el paso de iteración, dado un valor inicial \\(z_0\\) y un tamaño de paso \\(\\eta &gt;0\\) es \\[z^{(i+1)} = z^{(i)} - \\eta \\nabla h(z^{(i)})\\] Las mismas consideraciones acerca del tamaño de paso \\(\\eta\\) aplican en el problema multivariado. h &lt;- function(z) { z[1]^2 + z[2]^2 - z[1] * z[2] } h_gr &lt;- function(z_1,z_2) apply(cbind(z_1, z_2), 1, h) grid_graf &lt;- expand.grid(z_1 = seq(-3, 3, 0.1), z_2 = seq(-3, 3, 0.1)) grid_graf &lt;- grid_graf %&gt;% mutate( val = apply(cbind(z_1,z_2), 1, h)) gr_contour &lt;- ggplot(grid_graf, aes(x = z_1, y = z_2, z = val)) + geom_contour(binwidth = 1.5, aes(colour = ..level..)) gr_contour El gradiente está dado por h_grad &lt;- function(z){ c(2*z[1] - z[2], 2*z[2] - z[1]) } Podemos graficar la dirección de máximo descenso para diversos puntos. Estas direcciones son ortogonales a la curva de nivel que pasa por cada uno de los puntos: grad_1 &lt;- h_grad(c(0,-2)) grad_2 &lt;- h_grad(c(1,1)) eta &lt;- 0.2 gr_contour + geom_segment(aes(x=0.0, xend=0.0-eta*grad_1[1], y=-2, yend=-2-eta*grad_1[2]), arrow = arrow(length = unit(0.2,&quot;cm&quot;)))+ geom_segment(aes(x=1, xend=1-eta*grad_2[1], y=1, yend=1-eta*grad_2[2]), arrow = arrow(length = unit(0.2,&quot;cm&quot;)))+ coord_fixed(ratio = 1) Y aplicamos descenso en gradiente: inicial &lt;- c(3, 1) iteraciones &lt;- descenso(20, inicial , 0.1, h_grad) df_iteraciones &lt;- data.frame(iteraciones) %&gt;% mutate(iteracion = 1:nrow(iteraciones)) graf_descenso_2 &lt;- ggplot(data = df_iteraciones) + geom_contour(data= grid_graf, binwidth = 1.5, aes(x = z_1, y = z_2, z = val, colour = ..level..)) + geom_point(aes(x=X1, y=X2), colour = &#39;red&#39;) if(FALSE){ library(gganimate) graf_descenso_2 + labs(title = &#39;Iteración: {frame_time}&#39;) + transition_time(iteracion) anim_save(filename = &quot;figuras/descenso_2.gif&quot;) } knitr::include_graphics(&quot;figuras/descenso_2.gif&quot;) 2.4 Descenso en gradiente para regresión lineal Vamos a escribir ahora el algoritmo de descenso en gradiente para regresión lineal. Igual que en los ejemplos anteriores, tenemos que precalcular el gradiente. Una vez que esto esté terminado, escribir la iteración es fácil. Recordamos que queremos minimizar (dividiendo entre dos para simplificar más adelante) \\[RSS(\\beta) = \\frac{1}{2}\\sum_{i=1}^N (y^{(i)} - f_\\beta(x^{(i)}))^2\\] La derivada de la suma es la suma de las derivadas, así nos concentramos en derivar uno de los términos \\[ u^{(i)}=\\frac{1}{2}(y^{(i)} - f_\\beta(x^{(i)}))^2 \\] Usamos la regla de la cadena para obtener \\[ \\frac{1}{2}\\frac{\\partial}{\\partial \\beta_j} (y^{(i)} - f_\\beta(x^{(i)}))^2 = -(y^{(i)} - f_\\beta(x^{(i)})) \\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_j}\\] Ahora recordamos que \\[f_{\\beta} (x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p\\] Y vemos que tenemos dos casos. Si \\(j=0\\), \\[\\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_0} = 1\\] y si \\(j=1,2,\\ldots, p\\) entonces \\[\\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_j} = x_j^{(i)}\\] Entonces, si ponemos $ u{(i)}=(y{(i)} - f_(x{(i)}))2 $: \\[\\frac{\\partial u^{(i)}}{\\partial \\beta_0} = -(y^{(i)} - f_\\beta(x^{(i)}))\\] y \\[\\frac{\\partial u^{(i)}}{\\partial \\beta_j} = - x_j^{(i)}(y^{(i)} - f_\\beta(x^{(i)}))\\] Y sumando todos los términos (uno para cada caso de entrenamiento): Gradiente para regresión lineal Sea \\(e^{(i)} = y_{(i)} - f_{\\beta} (x^{(i)})\\). Entonces \\[\\begin{equation} \\frac{\\partial RSS(\\beta)}{\\partial \\beta_0} = - \\sum_{i=1}^N e^{(i)} \\tag{2.1} \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial RSS(\\beta)}{\\partial \\beta_j} = - \\sum_{i=1}^N x_j^{(i)}e^{(i)} \\tag{2.2} \\end{equation}\\] para \\(j=1,2,\\ldots, p\\). Nótese que cada punto de entrenamiento contribuye al cálculo del gradiente - la contribución es la dirección de descenso de error para ese punto particular de entrenamiento. Nos movemos entonces en una dirección promedio, para intentar hacer el error total lo más chico posible. Podemos implementar ahora estos cálculos. Aunque podríamos escribir ciclos para hacer estos cálculos, es mejor hacer los cálculos en forma matricial, de manera que aprovechamos rutinas de álgebra lineal eficiente. El cálculo del gradiente es como sigue: grad_calc &lt;- function(x_ent, y_ent){ salida_grad &lt;- function(beta){ f_beta &lt;- as.matrix(cbind(1, x_ent)) %*% beta e &lt;- y_ent - f_beta grad_out &lt;- -as.numeric(t(cbind(1, x_ent)) %*% e) names(grad_out) &lt;- c(&#39;Intercept&#39;, colnames(x_ent)) grad_out } salida_grad } grad_prostata &lt;- grad_calc(prostata_entrena[, 1, drop = FALSE], prostata_entrena$lpsa) grad_prostata(c(0,1)) ## Intercept lcavol ## -76.30319 -70.93938 grad_prostata(c(1,1)) ## Intercept lcavol ## -9.303187 17.064556 Podemos checar nuestro cálculo del gradiente, por ejemplo: delta &lt;- 0.001 (rss_prostata(c(1 + delta, 1)) - rss_prostata(c(1, 1))) / delta ## [1] -9.269687 (rss_prostata(c(1, 1 + delta)) - rss_prostata(c(1, 1))) / delta ## [1] 17.17331 Y ahora iteramos para obtener iteraciones &lt;- descenso(100, c(0,0), 0.005, grad_prostata) iteraciones ## [,1] [,2] ## [1,] 0.0000000 0.0000000 ## [2,] 0.8215356 1.4421892 ## [3,] 0.7332652 0.9545169 ## [4,] 0.8891507 1.0360252 ## [5,] 0.9569494 0.9603012 ## [6,] 1.0353555 0.9370937 ## [7,] 1.0977074 0.9046239 ## [8,] 1.1534587 0.8800287 ## [9,] 1.2013557 0.8576489 ## [10,] 1.2430547 0.8385314 ## [11,] 1.2791967 0.8218556 ## [12,] 1.3105688 0.8074114 ## [13,] 1.3377869 0.7948709 ## [14,] 1.3614051 0.7839915 ## [15,] 1.3818983 0.7745509 ## [16,] 1.3996803 0.7663595 ## [17,] 1.4151098 0.7592518 ## [18,] 1.4284979 0.7530844 ## [19,] 1.4401148 0.7477329 ## [20,] 1.4501947 0.7430895 ## [21,] 1.4589411 0.7390604 ## [22,] 1.4665303 0.7355643 ## [23,] 1.4731155 0.7325308 ## [24,] 1.4788295 0.7298986 ## [25,] 1.4837875 0.7276146 ## [26,] 1.4880895 0.7256328 ## [27,] 1.4918224 0.7239132 ## [28,] 1.4950614 0.7224211 ## [29,] 1.4978719 0.7211265 ## [30,] 1.5003106 0.7200031 ## [31,] 1.5024267 0.7190283 ## [32,] 1.5042627 0.7181825 ## [33,] 1.5058559 0.7174486 ## [34,] 1.5072383 0.7168117 ## [35,] 1.5084378 0.7162592 ## [36,] 1.5094786 0.7157797 ## [37,] 1.5103817 0.7153637 ## [38,] 1.5111654 0.7150027 ## [39,] 1.5118453 0.7146895 ## [40,] 1.5124353 0.7144177 ## [41,] 1.5129473 0.7141819 ## [42,] 1.5133915 0.7139772 ## [43,] 1.5137769 0.7137997 ## [44,] 1.5141114 0.7136456 ## [45,] 1.5144016 0.7135119 ## [46,] 1.5146534 0.7133959 ## [47,] 1.5148718 0.7132953 ## [48,] 1.5150614 0.7132079 ## [49,] 1.5152259 0.7131322 ## [50,] 1.5153687 0.7130664 ## [51,] 1.5154925 0.7130093 ## [52,] 1.5156000 0.7129598 ## [53,] 1.5156933 0.7129169 ## [54,] 1.5157742 0.7128796 ## [55,] 1.5158444 0.7128473 ## [56,] 1.5159053 0.7128192 ## [57,] 1.5159582 0.7127948 ## [58,] 1.5160040 0.7127737 ## [59,] 1.5160438 0.7127554 ## [60,] 1.5160784 0.7127395 ## [61,] 1.5161083 0.7127257 ## [62,] 1.5161343 0.7127137 ## [63,] 1.5161569 0.7127033 ## [64,] 1.5161765 0.7126943 ## [65,] 1.5161934 0.7126865 ## [66,] 1.5162082 0.7126797 ## [67,] 1.5162210 0.7126738 ## [68,] 1.5162321 0.7126687 ## [69,] 1.5162417 0.7126642 ## [70,] 1.5162501 0.7126604 ## [71,] 1.5162573 0.7126570 ## [72,] 1.5162636 0.7126541 ## [73,] 1.5162690 0.7126516 ## [74,] 1.5162738 0.7126495 ## [75,] 1.5162779 0.7126476 ## [76,] 1.5162815 0.7126459 ## [77,] 1.5162846 0.7126445 ## [78,] 1.5162872 0.7126433 ## [79,] 1.5162896 0.7126422 ## [80,] 1.5162916 0.7126412 ## [81,] 1.5162933 0.7126404 ## [82,] 1.5162949 0.7126397 ## [83,] 1.5162962 0.7126391 ## [84,] 1.5162973 0.7126386 ## [85,] 1.5162983 0.7126381 ## [86,] 1.5162992 0.7126377 ## [87,] 1.5162999 0.7126374 ## [88,] 1.5163006 0.7126371 ## [89,] 1.5163012 0.7126368 ## [90,] 1.5163016 0.7126366 ## [91,] 1.5163021 0.7126364 ## [92,] 1.5163024 0.7126363 ## [93,] 1.5163028 0.7126361 ## [94,] 1.5163030 0.7126360 ## [95,] 1.5163033 0.7126359 ## [96,] 1.5163035 0.7126358 ## [97,] 1.5163037 0.7126357 ## [98,] 1.5163038 0.7126356 ## [99,] 1.5163040 0.7126356 ## [100,] 1.5163041 0.7126355 Y checamos que efectivamente el error total de entrenamiento decrece apply(iteraciones, 1, rss_prostata) ## [1] 249.60960 51.70986 32.49921 28.96515 27.22475 25.99191 25.07023 ## [8] 24.37684 23.85483 23.46181 23.16591 22.94312 22.77538 22.64910 ## [15] 22.55401 22.48242 22.42852 22.38794 22.35739 22.33438 22.31706 ## [22] 22.30402 22.29421 22.28681 22.28125 22.27706 22.27390 22.27153 ## [29] 22.26974 22.26839 22.26738 22.26662 22.26604 22.26561 22.26528 ## [36] 22.26504 22.26485 22.26471 22.26461 22.26453 22.26447 22.26443 ## [43] 22.26439 22.26437 22.26435 22.26434 22.26432 22.26432 22.26431 ## [50] 22.26431 22.26430 22.26430 22.26430 22.26430 22.26429 22.26429 ## [57] 22.26429 22.26429 22.26429 22.26429 22.26429 22.26429 22.26429 ## [64] 22.26429 22.26429 22.26429 22.26429 22.26429 22.26429 22.26429 ## [71] 22.26429 22.26429 22.26429 22.26429 22.26429 22.26429 22.26429 ## [78] 22.26429 22.26429 22.26429 22.26429 22.26429 22.26429 22.26429 ## [85] 22.26429 22.26429 22.26429 22.26429 22.26429 22.26429 22.26429 ## [92] 22.26429 22.26429 22.26429 22.26429 22.26429 22.26429 22.26429 ## [99] 22.26429 22.26429 Verificamos el gradiente, que si convergió al mínimo debe ser muy cercano a 0: grad_prostata(iteraciones[100, ]) ## Intercept lcavol ## -2.053148e-05 9.458051e-06 Notación y forma matricial Usando la notación de la clase anterior (agregando una columna de unos al principio): \\[\\underline{X} = \\left ( \\begin{array}{ccccc} 1 &amp; x_1^{(1)} &amp; x_2^{(1)} &amp; \\ldots &amp; x_p^{(1)} \\\\ 1 &amp; x_1^{(2)} &amp; x_2^{(2)} &amp; \\ldots &amp; x_p^{(2)}\\\\ 1&amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1 &amp; x_1^{(N)} &amp; x_2^{(N)} &amp; \\ldots &amp; x_p^{(N)} \\\\ \\end{array} \\right)\\] y \\[\\underline{y} =(y^{(1)},y^{(2)}, \\ldots, y^{(N)})^t.\\] Como \\[\\underline{e} = \\underline{y} - \\underline{X}\\beta\\] tenemos entonces (de las fórmulas (2.1) y (2.2)): \\[\\begin{equation} \\nabla RSS(\\beta) = \\underline{X}^t(\\underline{X}\\beta - \\underline{y}) = -\\underline{X}^t \\underline{e} \\tag{2.3} \\end{equation}\\] 2.5 Normalización de entradas La convergencia de descenso en gradiente (y también el desempeño numérico para otros algoritmos) puede dificultarse cuando las variables tienen escalas muy diferentes. Esto produce curvaturas altas en la función que queremos minimizar. En este ejemplo simple, una variable tiene desviación estándar 10 y otra 1: x1 &lt;- rnorm(100, 0, 5) x2 &lt;- rnorm(100, 0, 1) + 0.1*x1 y &lt;- 0*x1 + 0*x2 + rnorm(100, 0, 0.1) dat &lt;- data_frame(x1, x2, y) rss &lt;- function(beta) mean((as.matrix(dat[, 1:2]) %*% beta - y)^2) grid_beta &lt;- expand.grid(beta1 = seq(-1, 1, length.out = 50), beta2 = seq(-1, 1, length.out = 50)) rss_1 &lt;- apply(grid_beta, 1, rss) dat_x &lt;- data.frame(grid_beta, rss_1) ggplot(dat_x, aes(x = beta1, y = beta2, z = rss_1)) + geom_contour(binwidth = 0.5) + coord_equal() En algunas direcciones el gradiente es muy grande, y en otras chico. Esto implica que la convergencia puede ser muy lenta en algunas direcciones, puede diverger en otras, y que hay que ajustar el paso \\(\\eta &gt; 0\\) con cuidado, dependiendo de dónde comiencen las iteraciones. Por ejemplo, con un tamaño de paso relativamente chico, damos unos saltos grandes al principio y luego avanzamos muy lentamente: grad_sin_norm &lt;- grad_calc(dat[, 1:2, drop = FALSE], dat$y) iteraciones &lt;- descenso(10, c(0, -0.25, -0.75), 0.0001, grad_sin_norm) ggplot(dat_x) + geom_contour(aes(x = beta1, y = beta2, z = rss_1), binwidth = 0.5) + coord_equal() + geom_path(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = &#39;red&#39;) + geom_point(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = &#39;red&#39;) Si incrementamos el tamaño de paso observamos también convergencia lenta. En este caso particular, subir más el tamaño de paso produce divergencia: iteraciones &lt;- descenso(10, c(0, -0.25, -0.75), 0.0007, grad_sin_norm) ggplot(dat_x) + geom_contour(aes(x = beta1, y = beta2, z = rss_1), binwidth = 0.5) + coord_equal() + geom_path(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = &#39;red&#39;) + geom_point(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = &#39;red&#39;) Una normalización usual es con la media y desviación estándar, donde hacemos, para cada variable de entrada \\(j=1,2,\\ldots, p\\) \\[ x_j^{(i)} = \\frac{ x_j^{(i)} - \\bar{x}_j}{s_j}\\] donde \\[\\bar{x}_j = \\frac{1}{N} \\sum_{i=1}^N x_j^{(i)}\\] \\[s_j = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^N (x_j^{(i)}- \\bar{x}_j )^2}\\] es decir, centramos y normalizamos por columna. Otra opción común es restar el mínimo y dividir entre la diferencia del máximo y el mínimo, de modo que las variables resultantes toman valores en \\([0,1]\\). Entonces escalamos antes de ajustar: x1_s = (x1 - mean(x1))/sd(x1) x2_s = (x2 - mean(x2))/sd(x2) dat &lt;- data_frame(x1_s, x2_s, y) rss &lt;- function(beta) mean((as.matrix(dat[, 1:2]) %*% beta - y)^2) grid_beta &lt;- expand.grid(beta1 = seq(-1, 1, length.out = 50), beta2 = seq(-1, 1, length.out = 50)) rss_1 &lt;- apply(grid_beta, 1, rss) dat_x &lt;- data.frame(grid_beta, rss_1) ggplot(dat_x, aes(x = beta1, y = beta2, z = rss_1)) + geom_contour(binwidth = 0.5) + coord_equal() Nótese que los coeficientes ajustados serán diferentes a los del caso no normalizado. Si normalizamos, obtenemos convergencia más rápida grad_sin_norm &lt;- grad_calc(dat[, 1:2, drop = FALSE], dat$y) iteraciones &lt;- descenso(10, c(0, -0.25, -0.75), 0.005, grad_sin_norm) ggplot(dat_x) + geom_contour(aes(x = beta1, y = beta2, z = rss_1), binwidth = 0.5) + coord_equal() + geom_path(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = &#39;red&#39;) + geom_point(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = &#39;red&#39;) Cuando normalizamos antes de ajustar el modelo, las predicciones deben hacerse con entradas normalizadas. La normalización se hace con los mismos valores que se usaron en el entrenamiento (y no recalculando medias y desviaciones estándar con el conjunto de prueba). En cuanto a la forma funcional del predictor \\(f\\), el problema con entradas normalizadas es equivalente al de las entradas no normalizadas. Asegúrate de esto escribiendo cómo correponden los coeficientes de cada modelo normalizado con los coeficientes del modelo no normalizado. Supongamos que el modelo en las variables originales es \\[{f}_\\beta (X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p,\\] Consideramos el modelo con variables estandarizadas \\[{g}_\\beta (X) = \\beta_0^s + \\beta_1^s Z_1 + \\beta_2^s Z_2 + \\cdots + \\beta_p^s Z_p,\\] Sustituyendo \\(Z_j = (X_j - \\mu_j)/s_j,\\) \\[{g}_{\\beta^s} (X) = (\\beta_0^s - \\sum_{j=1}^p \\mu_j/s_j) + \\frac{\\beta_1^s}{s_j} X_1 + \\frac{\\beta_2^s}{s_2} X_2 + \\cdots + \\frac{\\beta_p^s}{s_p} X_p,\\] Y vemos que tiene la misma forma funcional de \\(f_\\beta(X)\\). Si la solución de mínimos cuadrados es única, entonces una vez que ajustemos tenemos que tener \\(\\hat{f}_\\beta(X) = \\hat{g}_{\\beta^s} (X)\\), lo que implica que \\[\\hat{\\beta}_0 = \\hat{\\beta}_0^s - \\sum_{j=1}^p \\mu_j/s_j\\] y \\[\\hat{\\beta}_j = \\hat{\\beta}_j^s/s_j.\\] Nótese que para pasar del problema estandarizado al no estandarizado simplemente se requiere escalar los coeficientes por la \\(s_j\\) correspondiente. 2.6 Interpretación de modelos lineales Muchas veces se considera que la facilidad de interpretación es una fortaleza del modelo lineal. Esto es en parte cierto, pero hay algunas consideraciones importantes que debemos tomar en cuenta. La interpretación más sólida es la de las predicciones: podemos decir por qué una predicción es alta o baja. Consideremos el ejemplo de cáncer de prostata, por ejemplo: library(tidyr) prostate_completo &lt;- read_csv(file = &#39;datos/prostate.csv&#39;) pr_entrena &lt;- filter(prostate_completo, train) pr_entrena &lt;- pr_entrena %&gt;% mutate(id = 1:nrow(pr_entrena)) #normalizamos pr_entrena_s &lt;- pr_entrena %&gt;% select(id, lcavol, age, lpsa) %&gt;% gather(variable, valor, lcavol:age) %&gt;% group_by(variable) %&gt;% mutate(media = mean(valor), desv = sd(valor)) %&gt;% mutate(valor_s = (valor - media)/desv) pr_modelo &lt;- pr_entrena_s %&gt;% select(id, lpsa, variable, valor_s) %&gt;% spread(variable, valor_s) mod_pr &lt;- lm( lpsa ~ lcavol + age , data = pr_modelo ) round(coefficients(mod_pr), 2) ## (Intercept) lcavol age ## 2.45 0.88 0.02 y observamos el rango de \\(lpsa\\): round(summary(pr_modelo$lpsa), 2) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.43 1.67 2.57 2.45 3.37 5.48 Ahora podemos interpretar el predictor: Cuando las variables lcavol y age están en sus media, la predicción de lpsa es 2.5 Si lcavol sube 1 desviación estándar por encima de la media, el predictor de lpsa sube alrededor de 0.9 unidades (de un rango de alrededor de 6 unidades) Si age sube 1 desviación estándar por encima de su media, el predictor de lpsa sube 0.02, lo cual es un movimiento muy chico considerando la variación de lpsa. Así podemos explicar cada predicción - considerando qué variables aportan positiva y cuáles negativamente a la predicción. El camino más seguro es limitarse a hacer este tipo de análisis de las predicciones. Hablamos de entender la estructura predictiva del problema con los datos que tenemos - y no intentamos ir hacia la explicación del fenómeno. Cualquier otra interpretación requiere mucho más cuidados, y requiere una revisión de la especificación correcta del modelo. Parte de estos cuidados se estudian en un curso de regresión desde el punto de vista estadístico, por ejemplo: La interpretación coeficiente a coeficiente no toma en cuenta la estructura de asociación de las \\(x&#39;s\\). Rara vez cambios marginales en una variable de entrada ocurren de manera independiente de las otras variables de entrada. Variación muestral. Es necesario considerar la variación en nuestras estimaciones de los coeficientes para poder concluir acerca de su relación con el fenómeno (tratable desde punto de vista estadístico, pero hay que checar supuestos). Quizá el error de estimación del coeficiente de lcavol es 2 veces su magnitud - difícilmente podemos concluir algo acerca la relación de lcavol. Efectos no lineales: si la estructura del problema es altamente no lineal, los coeficientes de un modelo lineal no tienen una interpretación clara en relación al fenómeno. Esto también es parcialmente tratable con diagnósticos. set.seed(2112) x &lt;- rnorm(20) y &lt;- x^2 summary(lm(y ~x)) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.7462 -0.5022 -0.3313 0.3435 1.6273 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.85344 0.17570 4.857 0.000127 *** ## x 0.04117 0.18890 0.218 0.829929 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7484 on 18 degrees of freedom ## Multiple R-squared: 0.002632, Adjusted R-squared: -0.05278 ## F-statistic: 0.0475 on 1 and 18 DF, p-value: 0.8299 Otros cuidados adicionales se requieren si queremos hacer afirmaciones causales: Variables omitidas: si faltan algunas variables cruciales en el fenómeno que nos interesa, puede ser muy difícil interpretar el resto de los coeficientes en términos del fenómeno Ejemplo: Supongamos que queremos predecir cuánto van a gastar en televisiones samsung ciertas personas que llegan a Amazon. Una variable de entrada es el número de anuncios de televisiones Samsung que recibieron antes de llegar a Amazon. El coeficiente de esta variable es alto (significativo, etc.), así que concluimos que el anuncio causa compras de televisiones Samsung. ¿Qué está mal aquí? El modelo no está mal, sino la interpretación y la conclusión de causalidad. Cuando las personas están investigando acerca de televisiones, reciben anuncios. La razón es que esta variable nos puede indicar más bien quién está en proceso de compra de una televisión samsung (reciben anuncios) y quién no (no hacen búsquedas relevantes, así que no reciben anuncios). El modelo está mal especificado porque no consideramos que hay otra variable importante, que es el interés de la persona en compra de TVs Samsung. En general, la recomendación es que las interpretaciones causales deben considerarse como preliminares (o sugerencias), y se requiere más análisis y consideraciones antes de poder tener interpretaciones causales sólidas. Ejercicio En el siguiente ejercicio intentamos predecir el porcentaje de grasa corporal (una medición relativamente cara) usando mediciones de varias partes del cuerpo, edad, peso y estatura. Ver script ejercicios/bodyfat_ejercicio.R library(tidyr) dat_grasa &lt;- read_csv(file = &#39;datos/bodyfat.csv&#39;) head(dat_grasa) ## # A tibble: 6 x 14 ## grasacorp edad peso estatura cuello pecho abdomen cadera muslo rodilla ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12.3 23 154. 67.8 36.2 93.1 85.2 94.5 59 37.3 ## 2 6.1 22 173. 72.2 38.5 93.6 83 98.7 58.7 37.3 ## 3 25.3 22 154 66.2 34 95.8 87.9 99.2 59.6 38.9 ## 4 10.4 26 185. 72.2 37.4 102. 86.4 101. 60.1 37.3 ## 5 28.7 24 184. 71.2 34.4 97.3 100 102. 63.2 42.2 ## 6 20.9 24 210. 74.8 39 104. 94.4 108. 66 42 ## # ... with 4 more variables: tobillo &lt;dbl&gt;, biceps &lt;dbl&gt;, antebrazo &lt;dbl&gt;, ## # muñeca &lt;dbl&gt; nrow(dat_grasa) ## [1] 252 2.7 Solución analítica El problema de mínimos cuadrados tiene una solución de forma cerrada. A partir del gradiente (2.3), podemos igual a cero y resolver (chécalo) para obtener: \\[\\begin{equation*} \\hat{\\beta} = \\left (\\underline{X}\\underline{X}^t \\right)^{-1} \\underline{X}^t\\underline{y} \\end{equation*}\\] Paquetes como lm de R usan como base esta expresión, pero los cálculos se hacen mediante descomposiciones matriciales para más estabilidad (productos de matrices e inversiones). Aunque es posible escalar y/o paralelizar estos cálculos matriciales para problemas grandes, los procedimientos son más delicados. Nuestro enfoque de descenso máximo tiene la ventaja de que es fácil de entender, usar, aplicar a otros problemas con éxito, y además puede escalarse trivialmente, como veremos más adelante (por ejemplo, descenso estocástico). ¡Aunque siempre que se pueda es buena idea usar lm! 2.8 ¿Por qué el modelo lineal funciona bien (muchas veces)? Regresión lineal es un método muy simple, y parecería que debería haber métodos más avanzados que lo superen fácilmente. Para empezar, es poco creíble que el modelo \\[f(X) = b_0 + b_1X_1 + \\cdots b_p X_p\\] se cumple exactamente para el fenómeno que estamos tratando. Pero regresión lineal muchas veces supera a métodos que intentan construir predictores más complejos. Una de las primeras razones es que podemos ver la aproximación lineal como una aproximación de primer orden a la verdadera \\(f(X)\\), y muchas veces eso es suficiente para producir predicciones razonables. Adicionalmente, otras veces sólo tenemos suficientes datos para hacer una aproximación de primer orden, aún cuando la verdadera \\(f(X)\\) no sea lineal, y resulta que esta aproximación da buenos resultados. Esto es particularmente cierto en problemas de dimensión alta, como veremos a continuación. 2.8.1 k vecinos más cercanos Un método popular, con buen desempeño en varios ejemplos, es el de k-vecinos más cercanos, que consiste en hacer aproximaciones locales directas de \\(f(X)\\). Sea \\({\\mathcal L}\\) un conjunto de entrenamiento. Para \\(k\\) entera fija, y \\(x_0\\) una entrada donde queremos predecir, definimos a \\(N_k(x_0)\\) como el conjunto de los \\(k\\) elementos de \\({\\mathcal L}\\) que tienen \\(x^{(i)}\\) más cercana a \\(x_0\\). Hacemos la predicción \\[\\hat{f}(x_0) = \\frac{1}{k}\\sum_{x^{(i)} \\in N_k(x_0)} y^{(i)}\\] Es decir, promediamos las \\(k\\) \\(y\\)’s con \\(x\\)’s más cercanas a donde queremos predecir. Ejemplo library(ISLR) datos &lt;- Auto[, c(&#39;name&#39;, &#39;weight&#39;,&#39;year&#39;, &#39;mpg&#39;)] datos$peso_kg &lt;- datos$weight*0.45359237 datos$rendimiento_kpl &lt;- datos$mpg*(1.609344/3.78541178) nrow(datos) ## [1] 392 Vamos a separa en muestra de entrenamiento y de prueba estos datos. Podemos hacerlo como sigue (2/3 para entrenamiento aproximadamente en este caso, así obtenemos alrededor de 100 casos para prueba): set.seed(213) datos$muestra_unif &lt;- runif(nrow(datos), 0, 1) datos_entrena &lt;- filter(datos, muestra_unif &gt; 1/3) datos_prueba &lt;- filter(datos, muestra_unif &lt;= 1/3) nrow(datos_entrena) ## [1] 274 nrow(datos_prueba) ## [1] 118 ggplot(datos_entrena, aes(x = peso_kg, y = rendimiento_kpl)) + geom_point() Consideremos un modelo de \\(k=15\\) vecinos más cercanos. La función de predicción ajustada es entonces: library(kknn) # nótese que no normalizamos entradas - esto también es importante # hacer cuando hacemos vecinos más cercanos, pues en otro caso # las variables con escalas más grandes dominan el cálculo mod_15vmc &lt;- kknn(rendimiento_kpl ~ peso_kg, train = datos_entrena, test = data_frame(peso_kg=seq(700,2200, by = 10)), k=15) dat_graf &lt;- data_frame(peso_kg = seq(700,2200, by = 10), rendimiento_kpl = predict(mod_15vmc)) ggplot(datos_entrena, aes(x = peso_kg, y = rendimiento_kpl)) + geom_point(alpha=0.6) + geom_line(data=dat_graf, col=&#39;red&#39;, size = 1.2) Y para \\(k=5\\) vecinos más cercanos: mod_5vmc &lt;- kknn(rendimiento_kpl ~ peso_kg, train = datos_entrena, test = data_frame(peso_kg=seq(700,2200, by = 10)), k = 5) dat_graf &lt;- data_frame(peso_kg = seq(700,2200, by = 10), rendimiento_kpl = predict(mod_5vmc)) ggplot(datos_entrena, aes(x = peso_kg, y = rendimiento_kpl)) + geom_point(alpha=0.6) + geom_line(data=dat_graf, col=&#39;red&#39;, size = 1.2) En nuestro caso, los errores de prueba son mod_3vmc &lt;- kknn(rendimiento_kpl ~ peso_kg, train = datos_entrena, test = datos_prueba, k = 3) mod_15vmc &lt;- kknn(rendimiento_kpl ~ peso_kg, train = datos_entrena, test = datos_prueba, k = 15) (mean((datos_prueba$rendimiento_kpl-predict(mod_3vmc))^2)) ## [1] 3.346934 (mean((datos_prueba$rendimiento_kpl-predict(mod_15vmc))^2)) ## [1] 2.697658 Pregunta: ¿Cómo escogerías una \\(k\\) adecuada para este problema? Recuerda que adecuada significa que se reduzca a mínimo posible el error de predicción. Como ejercicio, compara los modelos con \\(k = 2, 25, 200\\) utilizando una muestra de prueba. ¿Cuál se desempeña mejor? Da las razones de el mejor o peor desempeño: recuerda que el desempeño en predicción puede sufrir porque la función estimada no es suficiente flexible para capturar patrones importantes, pero también porque parte del ruido se incorpora en la predicción. Por los ejemplos anteriores, vemos que k-vecinos más cercanos puede considerarse como un aproximador universal, que puede adaptarse a cualquier patrón importante que haya en los datos. Entonces, ¿cuál es la razón de utilizar otros métodos como regresión? ¿Por qué el desempeño de regresión sería superior? La maldición de la dimensionalidad El método de k-vecinos más cercanos funciona mejor cuando hay muchas \\(x\\) cercanas a \\(x0\\), de forma que el promedio sea estable (muchas \\(x\\)), y extrapolemos poco (\\(x\\) cercanas). Cuando \\(k\\) es muy chica, nuestras estimaciones son ruidosas, y cuando \\(k\\) es grande y los vecinos están lejos, entonces estamos sesgando la estimación local con datos lejanos a nuestra región de interés. El problema es que en dimensión alta, casi cualquier conjunto de entrenamiento (independientemente del tamaño) sufre fuertemente por uno o ambas dificultades del problema. Ejemplo Consideremos que la salida Y es determinística \\(Y = e^{-8\\sum_{j=1}^p x_j^2}\\). Vamos a usar 1-vecino más cercano para hacer predicciones, c on una muestra de entrenamiento de 1000 casos. Generamos $x^{i}‘s uniformes en \\([ 1,1]\\), para \\(p = 2\\), y calculamos la respuesta \\(Y\\) para cada caso: fun_exp &lt;- function(x) exp(-8*sum(x^2)) x_1 &lt;- runif(1000, -1, 1) x_2 &lt;- runif(1000, -1, 1) dat &lt;- data_frame(x_1 = x_1, x_2 = x_2) dat$y &lt;- apply(dat, 1, fun_exp) ggplot(dat, aes(x = x_1, y = x_2, colour = y)) + geom_point() La mejor predicción en \\(x_0 = (0,0)\\) es \\(f((0,0)) = 1\\). Eñ vecino más cercano al origen es dist_origen &lt;- apply(dat, 1, function(x) sqrt(sum(head(x, -1)^2))) mas_cercano_indice &lt;- which.min(dist_origen) mas_cercano &lt;- dat[mas_cercano_indice, ] mas_cercano ## # A tibble: 1 x 3 ## x_1 x_2 y ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0327 0.0101 0.991 Nuestra predicción es entonces \\(\\hat{f}(0)=\\) 0.9906871, que es bastante cercano al valor verdadero (1). Ahora intentamos hacer lo mismo para dimensión \\(p=8\\). dat_lista &lt;- lapply(1:8, function(i) runif(1000, -1, 1)) dat &lt;- Reduce(cbind, dat_lista) %&gt;% data.frame dat$y &lt;- apply(dat, 1, fun_exp) dist_origen &lt;- apply(dat, 1, function(x) sqrt(sum(head(x, -1)^2))) mas_cercano_indice &lt;- which.min(dist_origen) mas_cercano &lt;- dat[mas_cercano_indice, ] mas_cercano ## init V2 V3 V4 V5 V6 ## 239 0.1612183 0.4117209 0.2546389 -0.226929 0.0774977 0.03897632 ## V7 V8 y ## 239 -0.4959736 0.0382697 0.01073141 Y el resultado es un desastre. Nuestra predicción es mas_cercano$y ## [1] 0.01073141 Necesitariamos una muestra de alrededor de un millón de casos para obtener resultados no tan malos (pruébalo). ¿Qué es lo que está pasando? La razón es que en dimensiones altas, los puntos de la muestra de entrenamiento están muy lejos unos de otros, y están cerca de la frontera, incluso para tamaños de muestra relativamente grandes como n = 1000. Cuando la dimensión crece, la situación empeora exponencialmente. En dimensiones altas, todos los conjuntos de entrenamiento factibles se distribuyen de manera rala en el espacio de entradas. Ahora intentamos algo similar con una función que es razonable aproximar con una función lineal: fun_cubica &lt;- function(x) 0.5 * (1 + x[1])^3 Y queremos predecir para \\(x=(0,0,\\ldots,0)\\), cuyo valor exacto es fun_cubica(0) ## [1] 0.5 Repetimos el proceso: simulamos las entradas, y aplicamos un vecino más cercano: set.seed(821) sims_1 &lt;- lapply(1:40, function(i) runif(1000, -0.5, 0.5) ) dat &lt;- data.frame(Reduce(cbind, sims_1)) dat$y &lt;- apply(dat, 1, fun_cubica) dist_origen &lt;- apply(dat[, 1:40], 1, function(x) sqrt(sum(x^2))) mas_cercano_indice &lt;- which.min(dist_origen) dat$y[mas_cercano_indice] ## [1] 0.09842398 Este no es un resultado muy bueno. Sin embargo, regresión se desempeña considerablemente mejor: mod_lineal &lt;- lm(y ~ ., data = dat) origen &lt;- data.frame(matrix(rep(0,40), 1, 40)) names(origen) &lt;- names(dat)[1:40] predict(mod_lineal, newdata = origen) ## 1 ## 0.6251876 Donde podemos ver que típicamente la predicción de regresión es mucho mejor que la de 1 vecino más cercano. Esto es porque el modelo explota la estructura aproximadamente lineal del problema (¿cuál estructura lineal? haz algunas gráficas). Nota: corre este ejemplo varias veces con semilla diferente. Lo que sucede más específicamente es que en regresión lineal utilizamos todos los datos para hacer nuestra estimación en cada predicción. Si la estructura del problema es aproximadamente lineal, entonces regresión lineal explota la estructura para hacer pooling de toda la infromación para construir predicción con sesgo y varianza bajas. Tarea Para este ejemplo usaremos los datos de https://archive.ics.uci.edu/ml/machine-learning-databases/housing/. El objetivo es predecir el valor mediano de las viviendas en áreas del censo de Estados Unidos, utilizando variables relacionadas con criminalidad, ambiente, tipo de viviendas, etc. Separa la muestra en dos partes: unos 400 para entrenamiento y el resto para prueba. Describe las variables en la muestra de prueba (rango, media, mediana, por ejemplo). Construye un modelo lineal para predecir MEDV en términos de las otras variables. Utiliza descenso en gradiente para estimar los coeficientes con los predictores estandarizados. Verifica tus resultados con la función lm. Evalúa el error de entrenamiento \\(\\overline{err}\\) de tu modelo, y evalúa después la estimación del error de predicción \\(\\hat{Err}\\) con la muestra de prueba. Utiliza la raíz del la media de los errores al cuadrado. (Adicional) Construye un modelo de 1,5,20 y 50 vecinos más cercanos, y evalúa su desempeño. ¿Cuál es la mejor \\(k\\) para reducir el error de prueba? "],
["logistica.html", "Clase 3 Regresión logística 3.1 El problema de clasificación 3.2 Estimación de probabilidades de clase 3.3 Error para modelos de clasificación 3.4 Regresión logística 3.5 Aprendizaje de coeficientes para regresión logística (binomial). 3.6 Ejercicio: datos de diabetes", " Clase 3 Regresión logística 3.1 El problema de clasificación Una variabla \\(G\\) categórica o cualitativa toma valores que no son numéricos. Por ejemplo, si \\(G\\) denota el estado del contrato de celular de un cliente dentro de un año, podríamos tener \\(G\\in \\{ activo, cancelado\\}\\). En un problema de clasificación buscamos predecir una variable respuesta categórica \\(G\\) en función de otras variables de entrada \\(X=(X_1,X_2,\\ldots, X_p)\\). Ejemplos Predecir si un cliente cae en impago de una tarjeta de crédito, de forma que podemos tener \\(G=corriente\\) o \\(G=impago\\). Variables de entrada podrían ser \\(X_1=\\) porcentaje de saldo usado, \\(X_2=\\) atrasos en los úlltimos 3 meses, \\(X_3=\\) edad, etc En nuestro ejemplo de reconocimiento de dígitos tenemos \\(G\\in\\{ 0,1,\\ldots, 9\\}\\). Nótese que los` dígitos no se pueden considerar como valores numéricos (son etiquetas). Tenemos que las entradas \\(X_j\\) para \\(j=1,2,\\ldots, 256\\) son valores de cada pixel (imágenes blanco y negro). En reconocimiento de imágenes quiza tenemos que \\(G\\) pertenece a un conjunto que típicamente contiene miles de valores (manzana, árbol, pluma, perro, coche, persona, cara, etc.). Las \\(X_j\\) son valores de pixeles de la imagen para tres canales (rojo, verde y azul). Si las imágenes son de 100x100, tendríamos 30,000 variables de entrada. ¿Qué estimar en problemas de clasificación? En problemas de regresión, consideramos modelos de la forma \\(Y= f(X) + \\epsilon\\), y vimos que podíamos plantear el problema de aprendizaje supervisado como uno donde el objetivo es estimar lo mejor que podamos la función \\(f\\) mediante un estimador \\(\\hat{f}\\). Usamos entonces \\(\\hat{f}\\) para hacer predicciónes. En el caso de regresión: \\(f(X)\\) es la relación sistemática de \\(Y\\) en función de \\(X\\) Dada \\(X\\), la variable observada \\(Y\\) es una variable aleatoria (\\(\\epsilon\\) depende de otras variables que no conocemos) No podemos usar un modelo así en clasificación pues \\(G\\) no es numérica. Sin embargo, podemos pensar que \\(X\\) nos da cierta información probabilística acerca de las clases que pueden ocurrir: \\(P(G|X)\\) es la probabilidad condicional de observar \\(G\\) si tenemos \\(X\\). Esto es la información sistemática de \\(G\\) en función de \\(X\\) Dada \\(X\\), la clase observada \\(G\\) es una variable aleatoria (depende de otras variables que no conocemos). En analogía con el problema de regresión, quisiéramos estimar las probabilidades condicionales \\(P(G|X)\\), que es la parte sistemática de la relación de \\(G\\) en función de \\(X\\). Normalmente codificamos las clases \\(g\\) con una etiqueta numérica, de modo que \\(G\\in\\{1,2,\\ldots, K\\}\\): Ejemplo (Impago de tarjetas de crédito) Supongamos que \\(X=\\) porcentaje del crédito máximo usado, y \\(G\\in\\{1, 2\\}\\), donde \\(1\\) corresponde al corriente y \\(2\\) representa impago. Podríamos tener, por ejemplo: \\[\\begin{align*} p_1(10\\%) &amp;= P(G=1|X=10\\%) = 0.95 \\\\ p_2(10\\%) &amp;= P(G=2|X=10\\%) = 0.05 \\end{align*}\\] y \\[\\begin{align*} p_1(95\\%) &amp;= P(G=1|X=95\\%) = 0.70 \\\\ p_2(95\\%) &amp;= P(G=2|X=95\\%) = 0.30 \\end{align*}\\] En resumen: En problemas de clasificación queremos estimar la parte sistemática de la relación de \\(G\\) en función \\(X\\), que en este caso quiere decir que buscamos estimar las probabilidades condicionales: \\[\\begin{align*} p_1(x) &amp;= P(G=1|X=x), \\\\ p_2(x) &amp;= P(G=2|X=x), \\\\ \\vdots &amp; \\\\ p_K(x) &amp;= P(G=K|X=x) \\end{align*}\\] para cada valor \\(x\\) de las entradas. A partir de estas probabilidades de clase podemos producir un clasificador de varias maneras (las discutiremos más adelante). La forma más simple es usando el clasificador de Bayes: Dadas las probabilidades condicionales \\(p_1(x),p_2(x),\\ldots, p_K(x)\\), el clasificador de Bayes asociado está dado por \\[G (x) = \\arg\\max_{g} p_g(x)\\] Es decir, clasificamos en la clase que tiene máxima probabilidad de ocurrir. Ejemplo (Impago de tarjetas de crédito) Supongamos que \\(X=\\) porcentaje del crédito máximo usado, y \\(G\\in\\{1, 2\\}\\), donde \\(1\\) corresponde al corriente y \\(2\\) representa impago. Las probabilidades condicionales de clase para la clase al corriente podrían ser, por ejemplo: \\(p_1(x) = P(G=1|X = x) =0.95\\) si \\(x &lt; 15\\%\\) \\(p_1(x) = P(G=1|X = x) = 0.95 - 0.007(x-15)\\) si \\(x&gt;=15\\%\\) Estas son probabilidades, pues hay otras variables que influyen en que un cliente permanezca al corriente o no en sus pagos más allá de información contenida en el porcentaje de crédito usado. Nótese que estas probabilidades son diferentes a las no condicionadas, por ejempo, podríamos tener que a total \\(P(G=1)=0.83\\). p_1 &lt;- function(x){ ifelse(x &lt; 15, 0.95, 0.95 - 0.007 * (x - 15)) } ggplot(data_frame(x = 0:100), aes(x = x)) + stat_function(fun = p_1) ¿Por qué en este ejemplo ya no mostramos la función \\(p_2(x)\\)? Si usamos el clasificador de Bayes, tendríamos por ejemplo que si \\(X=10\\%\\), como \\(p_1(10\\%) = 0.95\\) y \\(p_2(10\\%)=0.05\\), nuestra predicción de clase sería \\(G(10\\%) = 1\\) (al corriente), pero si \\(X=70\\%\\), \\(G(70\\%) = 1\\) (impago), pues \\(p_1(70\\%) = 0.57\\) y \\(p_2(70\\%) = 0.43\\). 3.2 Estimación de probabilidades de clase ¿Cómo estimamos ahora las probabilidades de clase a partir de una muestra de entrenamiento? Veremos por ahora dos métodos: k-vecinos más cercanos y regresión logística. Ejemplo Vamos a generar unos datos con el modelo simple del ejemplo anterior: library(tidyverse) library(kknn) # para hacer vecinos más cercanos simular_impago &lt;- function(n = 500){ # suponemos que los valores de x están concentrados en valores bajos, # quizá la manera en que los créditos son otorgados x &lt;- pmin(rexp(n, 1 / 40), 100) # las probabilidades de estar al corriente: probs &lt;- p_1(x) # finalmente, simulamos cuáles clientes siguen en al corriente y cuales no: g &lt;- ifelse(rbinom(length(x), 1, probs) == 1 ,1, 2) dat_ent &lt;- data_frame(x = x, p_1 = probs, g = factor(g)) dat_ent } set.seed(1933) dat_ent &lt;- simular_impago() %&gt;% select(x, g) dat_ent %&gt;% sample_n(20) ## # A tibble: 20 x 2 ## x g ## &lt;dbl&gt; &lt;fct&gt; ## 1 88.8 2 ## 2 97.0 1 ## 3 43.1 1 ## 4 43.2 1 ## 5 8.42 1 ## 6 6.19 1 ## 7 10.6 1 ## 8 15.2 1 ## 9 54.7 1 ## 10 93.6 2 ## 11 20.4 1 ## 12 100 1 ## 13 23.8 1 ## 14 49.6 2 ## 15 0.709 1 ## 16 77.1 1 ## 17 29.7 1 ## 18 6.50 1 ## 19 6.52 1 ## 20 28.7 2 Como este problema es de dos clases, podemos graficar como sigue: graf_1 &lt;- ggplot(dat_ent, aes(x = x)) + geom_point(aes(colour = g, y = as.numeric(g==&#39;1&#39;))) graf_1 Esta gráfica tiene el problema de que hay mucho trasplape entre los puntos. Podemos agregar variación artificial alrededor de 1 y 0, y también alrededor de los valores de \\(x\\) para evitar traslape en los extremos: graf_1 &lt;- ggplot(dat_ent, aes(x = x)) + geom_jitter(aes(colour = factor(g), y = as.numeric(g==&#39;1&#39;)), width=0.5, height=0.05) graf_1 3.2.1 k-vecinos más cercanos Podemos extender fácilmente k vecinos más cercanos para ver un ejemplo de cómo estimar las probabilidades de clase \\(p_g(x)\\). La idea general es igual que en regresión, y es simple: nos fijamos en las tasas locales de impago alrededor de la \\(x\\) para la que queremos predecir. Supongamos entonces que tenemos un conjunto de entrenamiento \\[{\\mathcal L}=\\{ (x^{(1)},g^{(1)}),(x^{(2)},g^{(2)}), \\ldots, (x^{(N)}, g^{(N)}) \\}\\] La idea es que si queremos predecir en \\(x_0\\), busquemos varios \\(k\\) vecinos más cercanos a \\(x_0\\), y estimamos entonces \\(p_g(x)\\) como la proporción de casos tipo \\(g\\) que hay entre los \\(k\\) vecinos de \\(x_0\\). Vemos entonces que este método es un intento de hacer una aproximación directa de las probabilidades condicionales de clase. Podemos escribir esto como: k vecinos más cercanos para clasificación Estimamos contando los elementos de cada clase entre los \\(k\\) vecinos más cercanos: \\[\\hat{p}_g (x_0) = \\frac{1}{k}\\sum_{x^{(i)} \\in N_k(x_0)} I( g^{(i)} = g),\\] para \\(g=1,2,\\ldots, K\\), donde \\(N_k(x_0)\\) es el conjunto de \\(k\\) vecinos más cercanos en \\({\\mathcal L}\\) de \\(x_0\\), y \\(I(g^{(i)}=g)=1\\) cuando \\(g^{(i)}=g\\), y cero en otro caso (indicadora). Ejemplo Regresamos a nuestro problema de impago. Vamos a intentar estimar la probabilidad condicional de estar al corriente usando k vecinos más cercanos (curva roja): graf_data &lt;- data_frame(x = seq(0,100, 1)) vmc &lt;- kknn(g ~ x, train = dat_ent, k = 60, test = graf_data) graf_data$p_1 &lt;- vmc$prob[ ,1] graf_verdadero &lt;- data_frame(x = 0:100, p_1 = p_1(x)) graf_1 + geom_line(data = graf_data, aes(y = p_1), colour = &#39;red&#39;, size=1.2) + geom_line(data = graf_verdadero, aes(y = p_1)) + ylab(&#39;Probabilidad al corriente&#39;) + xlab(&#39;% crédito usado&#39;) Igual que en el caso de regresión, ahora tenemos qué pensar cómo validar nuestra estimación, pues no vamos a tener la curva negra real para comparar. Arriba denotamos las probabilidades teóricas como \\(p_1 (x), p_2 (x), \\ldots, p_K (x)\\). Denotamos probabilidades estimadas como \\(\\hat{p}_1 (x), \\hat{p}_2 (x), \\ldots, \\hat{p}_K (x)\\) Ejemplo Consideremos datos de diabetes en mujeres Pima: A population of women who were at least 21 years old, of Pima Indian heritage and living near Phoenix, Arizona, was tested for diabetes according to World Health Organization criteria. The data were collected by the US National Institute of Diabetes and Digestive and Kidney Diseases. We used the 532 complete records after dropping the (mainly missing) data on serum insulin. npreg number of pregnancies. glu plasma glucose concentration in an oral glucose tolerance test. bp diastolic blood pressure (mm Hg). skin triceps skin fold thickness (mm). bmi body mass index (weight in kg/(height in m)^2). ped diabetes pedigree function. age age in years. type Yes or No, for diabetic according to WHO criteria. diabetes_ent &lt;- as_data_frame(MASS::Pima.tr) diabetes_pr &lt;- as_data_frame(MASS::Pima.te) diabetes_ent ## # A tibble: 200 x 8 ## npreg glu bp skin bmi ped age type ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; ## 1 5 86 68 28 30.2 0.364 24 No ## 2 7 195 70 33 25.1 0.163 55 Yes ## 3 5 77 82 41 35.8 0.156 35 No ## 4 0 165 76 43 47.9 0.259 26 No ## 5 0 107 60 25 26.4 0.133 23 No ## 6 5 97 76 27 35.6 0.378 52 Yes ## 7 3 83 58 31 34.3 0.336 25 No ## 8 1 193 50 16 25.9 0.655 24 No ## 9 3 142 80 15 32.4 0.2 63 No ## 10 2 128 78 37 43.3 1.22 31 Yes ## # ... with 190 more rows Intentaremos predecir diabetes dependiendo del BMI: library(ggplot2) ggplot(diabetes_ent, aes(x = bmi, y= as.numeric(type==&#39;Yes&#39;), colour = type)) + geom_point() Usamos \\(20\\) vecinos más cercanos para estimar \\(p_g(x)\\): graf_data &lt;- data_frame(bmi = seq(20,45, 1)) vmc_5 &lt;- kknn(type ~ bmi, train = diabetes_ent, k = 20, test = graf_data, kernel = &#39;rectangular&#39;) graf_data$Yes &lt;- vmc_5$prob[ ,&quot;Yes&quot;] graf_data$No &lt;- vmc_5$prob[ ,&quot;No&quot;] graf_data &lt;- graf_data %&gt;% gather(type, prob, Yes:No) ggplot(diabetes_ent, aes(x = bmi, y= as.numeric(type==&#39;Yes&#39;), colour = type)) + geom_point() + geom_line(data = filter(graf_data, type ==&#39;Yes&#39;) , aes(x=bmi, y = prob, colour=type, group = type)) + ylab(&#39;Probabilidad diabetes&#39;) 3.3 Error para modelos de clasificación En regresión, vimos que la pérdida cuadrática era una buena opción para ajustar modelos (descenso en gradiente, por ejemplo), y también para evaluar su desempeño. Ahora necesitamos una pérdida apropiada para trabajar con modelos de clasificación. Consideremos entonces que tenemos una estimación \\(\\hat{p}_g(x)\\) de las probabilidad de clase \\(P(G=g|X=x)\\). Supongamos que observamos ahora \\((x, g)\\). Si \\(\\hat{p}_{g}(x)\\) es muy cercana a uno, deberíamos penalizar poco, pues dimos probabilidad alta a \\(G=g\\). Si \\(\\hat{p}_{g}(x)\\) es chica, deberíamos penalizar más, pues dimos probabilidad baja a \\(G=g\\). Si \\(\\hat{p}_{g}(x)\\) es muy cercana a cero, y observamos \\(G=g\\), deberíamos hacer una penalización muy alta (convergiendo a \\(\\infty\\), pues no es aceptable que sucedan eventos con probabilidad estimada extremadamente baja). Quisiéramos encontrar una función \\(h\\) apropiada, de forma que la pérdida al observar \\((x, g)\\) sea \\[s(\\hat{p}_{g}(x)),\\] y que cumpla con los puntos arriba señalados. Entonces tenemos que \\(s\\) debe ser una función continua y decreciente en \\([0,1]\\) Podemos poner \\(s(1)=0\\) (no hay pérdida si ocurre algo con probabilidad 1) \\(s(p)\\) debe ser muy grande is \\(p\\) es muy chica. Una opción analíticamente conveniente es \\[s(p) = - 2log(p)\\] s &lt;- function(z){ -2*log(z)} ggplot(data_frame(p = (0:100)/100), aes(x = p)) + stat_function(fun = s) + ylab(&quot;Devianza&quot;) Y entonces la pérdida (que llamamos devianza) que construimos está dada, para \\((x,g)\\) observado y probabilidades estimadas \\(\\hat{p}_g(x)\\) por \\[ - 2\\log(\\hat{p}_g(x)) \\] Su valor esperado (según el proceso que genera los datos) es nuestra medición del desempeño del modelo \\(\\hat{p}_g (x)\\), es decir, el error de predicción es: \\[-2E\\left [ \\log(\\hat{p}_G(X)) \\right ]\\] que podemos estimar con una muestra de prueba. Observaciones: Ojo: el nombre de devianza se utiliza de manera diferente en distintos lugares (pero para cosas similares). Usamos el factor 2 por razones históricas (la medida de devianza definida en estadística tiene un 2, para usar más fácilmente en pruebas de hipótesis relacionadas con comparaciones de modelos). Para nuestros propósitos, podemos usar o no el 2. No es fácil interpretar la devianza, pero es útil para comparar modelos. Veremos otras medidas más fáciles de intrepretar más adelante. Compara la siguiente definición con la que vimos para modelos de regresión: Sea \\[{\\mathcal L}=\\{ (x^{(1)},g^{(1)}),(x^{(2)},g^{(2)}), \\ldots, (x^{(N)}, g^{(N)}) \\}\\] una muestra de entrenamiento, a partir de las cuales construimos mediante un algoritmo funciones estimadas \\(\\hat{p}_{g} (x)\\) para \\(g=1,2,\\ldots, K\\). La devianza promedio de entrenamiento está dada por \\[\\begin{equation} \\overline{err} = - \\frac{2}{N}\\sum_{i=1}^N log(\\hat{p}_{g^{(i)}} (x^{(i)})) \\tag{3.1} \\end {equation}\\] Sea \\[{\\mathcal T}=\\{ (x_0^{(1)},g_0^{(1)}),(x_0^{(2)},g_0^{(2)}), \\ldots, (x_0^{(m)}, g_0^{(m)}) \\}\\] una muestra de prueba. La devianza promedio de prueba es \\[\\begin{equation} \\hat{Err} = - \\frac{2}{m}\\sum_{i=1}^m log(\\hat{p}_{g_0^{(i)}} (x_0^{(i)})) \\end {equation}\\] que es una estimación de la devianza de predicción \\[-2E\\left [ \\log(\\hat{p}_G(X)) \\right ]\\] Ejemplo Regresamos a nuestros ejemplo simulado de impago de tarjetas de crédito. Primero calculamos la devianza de entrenamiento s &lt;- function(x) -2*log(x) vmc_entrena &lt;- kknn(g ~ x, train = dat_ent, k = 60, test = dat_ent, kernel = &#39;rectangular&#39;) dat_dev &lt;- dat_ent %&gt;% select(x,g) dat_dev$hat_p_1 &lt;- predict(vmc_entrena, type =&#39;prob&#39;)[,1] dat_dev$hat_p_2 &lt;- predict(vmc_entrena, type =&#39;prob&#39;)[,2] dat_dev &lt;- dat_dev %&gt;% mutate(hat_p_g = ifelse(g==1, hat_p_1, hat_p_2)) Nótese que dependiendo de qué clase observamos (columna \\(g\\)), extraemos la probabilidad correspondiente a la columna hat_p_g: set.seed(125) dat_dev %&gt;% sample_n(20) ## # A tibble: 20 x 5 ## x g hat_p_1 hat_p_2 hat_p_g ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9.47 1 0.967 0.0333 0.967 ## 2 100 2 0.417 0.583 0.583 ## 3 6.70 1 0.983 0.0167 0.983 ## 4 57.6 2 0.683 0.317 0.317 ## 5 54.1 1 0.683 0.317 0.683 ## 6 37.4 1 0.817 0.183 0.817 ## 7 69.5 1 0.667 0.333 0.667 ## 8 6.82 1 0.983 0.0167 0.983 ## 9 50.8 2 0.767 0.233 0.233 ## 10 59.3 1 0.7 0.3 0.7 ## 11 65.9 1 0.733 0.267 0.733 ## 12 9.64 1 0.967 0.0333 0.967 ## 13 67.3 1 0.717 0.283 0.717 ## 14 10.3 1 0.967 0.0333 0.967 ## 15 25.8 1 0.867 0.133 0.867 ## 16 30.4 1 0.867 0.133 0.867 ## 17 12.0 1 0.967 0.0333 0.967 ## 18 2.42 1 0.967 0.0333 0.967 ## 19 36.4 1 0.833 0.167 0.833 ## 20 84.8 2 0.483 0.517 0.517 Ahora aplicamos la función \\(s\\) que describimos arriba, y promediamos sobre el conjunto de entrenamiento: dat_dev &lt;- dat_dev %&gt;% mutate(dev = s(hat_p_g)) dat_dev %&gt;% ungroup %&gt;% summarise(dev_entrena = mean(dev)) ## # A tibble: 1 x 1 ## dev_entrena ## &lt;dbl&gt; ## 1 0.794 Recordemos que la devianza de entrenamiento no es la cantidad que evalúa el desempeño del modelo. Hagamos el cálculo entonces para una muestra de prueba: set.seed(1213) dat_prueba &lt;- simular_impago(n = 1000) %&gt;% select(x, g) vmc_prueba &lt;- kknn(g ~ x, train = dat_ent, k = 60, test = dat_prueba, kernel = &#39;rectangular&#39;) dat_dev_prueba &lt;- dat_prueba %&gt;% select(x,g) dat_dev_prueba$hat_p_1 &lt;- predict(vmc_prueba, type =&#39;prob&#39;)[,1] dat_dev_prueba$hat_p_2 &lt;- predict(vmc_prueba, type =&#39;prob&#39;)[,2] dat_dev_prueba &lt;- dat_dev_prueba %&gt;% mutate(hat_p_g = ifelse(g==1, hat_p_1, hat_p_2)) dat_dev_prueba &lt;- dat_dev_prueba %&gt;% mutate(dev = s(hat_p_g)) dat_dev_prueba %&gt;% ungroup %&gt;% summarise(dev_prueba = mean(dev)) ## # A tibble: 1 x 1 ## dev_prueba ## &lt;dbl&gt; ## 1 0.851 3.3.1 Ejercicio Utiliza 5, 20, 60, 200 y 400 vecinos más cercanos para nuestro ejemplo de tarjetas de crédito. ¿Cuál tiene menor devianza de prueba? ¿Cuál tiene menor devianza de entrenamiento? Grafica el mejor que obtengas y otros dos modelos malos. ¿Por qué crees que la devianza es muy grande para los modelos malos? Nota: ten cuidado con probabilidades iguales a 0 o 1, pues en en estos casos la devianza puede dar \\(\\infty\\). Puedes por ejemplo hacer que las probabilidades siempre estén en \\([\\epsilon, 1-\\epsilon]\\) para \\(\\epsilon&gt;0\\) chica. 3.3.2 Error de clasificación y función de pérdida 0-1 Otra medida común para medir el error de un clasificador es el error de clasificación, que también llamamos probabilidad de clasificación incorrecta, o error bajo pérdida 0-1. Si \\(\\hat{G}\\) es un clasificador (que puede ser construido a partir de probabilidades de clase), decimos que su error de clasificación es \\[P(\\hat{G}\\neq G)\\] Aunque esta definición aplica para cualquier clasificador, podemos usarlo para clasificadores construidos con probabilidades de clase de la siguiente forma: Sean \\(\\hat{p}_g(x)\\) probabilidades de clase estimadas. El clasificador asociado está dado por \\[\\hat{G} (x) = \\arg\\max_g \\hat{p}_g(x)\\] Podemos estimar su error de clasificación \\(P(\\hat{G} \\neq G)\\) con una muestra de prueba \\[{\\mathcal T}=\\{ (x_0^{(1)},g_0^{(1)}),(x_0^{(2)},g_0^{(2)}), \\ldots, (x_0^{(m)}, g_0^{(m)})\\] mediante \\[\\hat{Err} = \\frac{1}{m} \\sum_{j=i}^m I(\\hat{G}(x_0^{(i)}) \\neq g_0^{(i)}),\\] es decir, la proporción de casos de prueba que son clasificados incorrectamente. Ejemplo Veamos cómo se comporta en términos de error de clasificación nuestro último modelo: dat_dev$hat_G &lt;- predict(vmc_entrena) dat_dev %&gt;% mutate(correcto = hat_G == g) %&gt;% ungroup %&gt;% summarise(p_correctos = mean(correcto)) %&gt;% mutate(error_clasif = 1 - p_correctos) ## # A tibble: 1 x 2 ## p_correctos error_clasif ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.828 0.172 Y calculamos el error de clasificación de prueba: dat_dev_prueba$hat_G &lt;- predict(vmc_prueba) dat_dev_prueba %&gt;% mutate(correcto = hat_G == g) %&gt;% ungroup %&gt;% summarise(p_correctos = mean(correcto)) %&gt;% mutate(error_clasif = 1 - p_correctos) ## # A tibble: 1 x 2 ## p_correctos error_clasif ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.799 0.201 3.3.3 Discusión: relación entre devianza y error de clasificación Cuando utilizamos devianza, el mejor desempeño se alcanza cuando las probabilidades \\(\\hat{p}_g (x)\\) están bien calibradas, es decir, están cercanas a las probabilidades verdaderas \\(p_g (x)\\). Esto se puede ver demostrando que las probabilidades \\(\\hat{p}_g (x)\\) que minimizan la devianza \\[-2E(\\log (\\hat{p}_G (X))) = -2E_X \\left[ \\sum_{g=1}^K p_g(X)\\log\\hat{p}_g(X) \\right]\\] son precisamente \\(\\hat{p}_g (x)=p_g (x)\\). Por otro lado, si consideramos el error de clasificación \\(P(\\hat{G}\\neq G)\\), es posible demostrar que se minimiza cuando \\(\\hat{G} = G_{bayes}\\), donde \\[{G}_{bayes} (x) = \\arg\\max_g {p}_g(x).\\] En consecuencia, cuando las \\(\\hat{p}_g(x)\\) estimadas están cercanas a las verdaderas \\(p_g (x)\\) (que es lo que intentamos hacer cuando usamos devianza), el clasificador \\(\\hat{G}(x)\\) producido a partir de las \\(\\hat{p}_g(x)\\) deberá estar cercano a \\(G_{bayes}(x)\\), que es el clasificador que minimiza el error de clasificación. Este argumento explica que buscar modelos con devianza baja está alineado con buscar modelos con error de clasificación bajo. Cuando sea posible, es mejor trabajar con probabilidades de clase y devianza que solamente con clasificadores y error de clasificación. Hay varias razones para esto: Tenemos una medida de qué tan seguros estamos en la clasificación (por ejemplo, \\(p_1 = 0.55\\) en vez de \\(p_1 = 0.995\\)). La salida de probabilides es un insumo más útil para tareas posteriores (por ejemplo, si quisiéramos ofrecer las 3 clases más probables en clasificación de imágenes). Permite hacer selección de modelos de manera más atinada: por ejemplo, dada una misma tasa de correctos, preferimos aquellos modelos que lo hacen con probabilidades que discriminan más (más altas cuando está en lo correcto y más bajas cuando se equivoca). 3.4 Regresión logística En \\(k\\) vecinos más cercanos, intentamos estimar directamente con promedios las probabilidades de clase, sin considerar ninguna estructura. Regresión logística (y otros métodos, como redes neuronales), son ajustados intentando minimizar la devianza de entrenamiento. Esto es necesario si queremos aprovechar la estructura adicional que estos modelos aportan. En el caso de regresion logística, establecemos una estructura lineal de cierto tipo. Recordemos el caso de regresión lineal: intentamos minimizar el error de entrenamiento para estimar nuestro predictor, y así podíamos explotar apropiadamente la estructura lineal del problema. Regresión logística es un método lineal de clasificación, en el sentido de que produce fronteras lineales de decisión para el clasificador asociado. Ejemplo Mostramos aquí una frontera de decisión de regresión logística y una de k vecinos más cercanos: knitr::include_graphics(path = c(&quot;figuras/clas_lineal.png&quot;, &quot;figuras/clas_nolineal.png&quot;)) 3.4.1 Regresión logística simple Vamos a construir el modelo de regresión logística (binaria) para una sola entrada. Suponemos que tenemos una sola entrada \\(X_1\\), y que \\(G\\in\\{1,2\\}\\). Nos convendrá crear una nueva variable \\(Y\\) dada por \\(Y=1\\) si \\(G=2\\), \\(Y=0\\) si \\(G=1\\). Nótese que intentar estimar las probabilidades de clase \\(p_1(x)\\) de forma lineal con \\[p_1(x)=\\beta_0+\\beta_1 x_1\\] tiene el defecto de que el lado derecho puede producir valores fuera de \\([0,1]\\). La idea es entonces aplicar una función \\(h\\) simple que transforme la recta real al intervalo \\([0,1]:\\) \\[p_1(x) = h(\\beta_0+\\beta_1 x_1),\\] donde \\(h\\) es una función que toma valores en \\([0,1]\\). ¿Cúal es la función más simple que hace esto? 3.4.2 Función logística Comenzamos con el caso más simple, poniendo \\(\\beta_0=0\\) y \\(\\beta_1=1\\), de modo que \\[p_1(x)=h(x).\\] ¿Cómo debe ser \\(h\\) para garantizar que \\(h(x)\\) está entre 0 y 1 para toda \\(x\\)? No van a funcionar polinomios, por ejemplo, porque para un polinomio cuando \\(x\\) tiende a infinito, el polinomio tiende a \\(\\infty\\) o a \\(-\\infty\\). Hay varias posibilidades, pero una de las más simples es tomar (ver gráfica al margen): La función logística está dada por \\[h(x)=\\frac{e^x}{1+e^x}\\] h &lt;- function(x){exp(x)/(1+exp(x)) } ggplot(data_frame(x = seq(-6, 6, 0.01)), aes(x = x)) + stat_function(fun = h) Esta función comprime adecuadamente (para nuestros propósitos) el rango de todos los reales dentro del intervalo \\([0,1]\\). Si aplicamos al predictor lineal que consideramos, obtenemos: El modelo de regresión logística simple está dado por \\[p_1(x)=p_1(x;\\beta)= h(\\beta_0+\\beta_1x_1)= \\frac{e^{\\beta_0+\\beta_1x_1}}{1+ e^{\\beta_0+\\beta_1x_1}},\\] y \\[p_0(x)=p_0(x;\\beta)=1-p_1(x;\\beta),\\] donde \\(\\beta=(\\beta_0,\\beta_1)\\). Este es un modelo paramétrico con 2 parámetros. Ejercicio Demostrar que, si \\(p_1(x)\\) está dado como en la ecuación anterior, entonces también podemos escribir: \\[p_0(x)=\\frac{1}{1+e^{\\beta_0+\\beta_1x_1}}.\\] Graficar las funciones \\(p_1(x;\\beta)\\) para distintos valores de \\(\\beta_0\\) y \\(\\beta_1\\). Ejemplo En nuestro ejemplo, teníamos el siguiente ajuste con k-vecinos más cercanos: graf_data &lt;- data_frame(x = seq(0,100, 1)) vmc_graf &lt;- kknn(g ~ x, train = dat_ent, k = 60, test = graf_data, kernel = &#39;rectangular&#39;) graf_data$p_1 &lt;- vmc_graf$prob[ ,1] graf_verdadero &lt;- data_frame(x = 0:100, p_1 = p_1(x)) graf_1 + geom_line(data = graf_data, aes(y = p_1), colour = &#39;red&#39;, size=1.2) + geom_line(data = graf_verdadero, aes(y = p_1)) + ylab(&#39;Probabilidad al corriente&#39;) + xlab(&#39;% crédito usado&#39;) Ahora intentaremos ajustar a mano (intenta cambiar las betas para p_mod_1 y p_mod_2 en el ejemplo de abajo) algunos modelos logísticos para las probabilidades de clase: h &lt;- function(z) exp(z)/(1+exp(z)) p_logistico &lt;- function(beta_0, beta_1){ p &lt;- function(x){ z &lt;- beta_0 + beta_1*x h(z) } } p_mod_1 &lt;- p_logistico(-20, 1) p_mod_2 &lt;- p_logistico(3, -0.04) graf_data &lt;- graf_data %&gt;% mutate(p_mod_1 = p_mod_1(x), p_mod_2 = p_mod_2(x)) graf_1 + geom_line(data = graf_data, aes(y = p_mod_2), colour = &#39;red&#39;, size=1.2) + geom_line(data = graf_data, aes(y = p_mod_1), colour = &#39;orange&#39;, size=1.2) + geom_line(data = graf_verdadero, aes(y = p_1)) + ylab(&#39;Probabilidad al corriente&#39;) + xlab(&#39;% crédito usado&#39;) Podemos usar también la función glm de R para ajustar los coeficientes: mod_1 &lt;- glm(g==1 ~ x, data = dat_ent, family = &#39;binomial&#39;) coef(mod_1) ## (Intercept) x ## 3.11902058 -0.03732159 p_mod_final &lt;- p_logistico(coef(mod_1)[1], coef(mod_1)[2]) graf_data &lt;- graf_data %&gt;% mutate(p_mod_f = p_mod_final(x)) graf_1 + geom_line(data = graf_data, aes(y = p_mod_f), colour = &#39;red&#39;, size = 1.2) + geom_line(data = graf_data, aes(y = p_mod_1), colour = &#39;orange&#39;, size = 1.2) + geom_line(data = graf_verdadero, aes(y = p_1)) + ylab(&#39;Probabilidad al corriente&#39;) + xlab(&#39;% crédito usado&#39;) 3.4.3 Regresión logística Ahora escribimos el modelo cuando tenemos más de una entrada. La idea es la misma: primero combinamos las variables linealmente usando pesos \\(\\beta\\), y despúes comprimimos a \\([0,1]\\) usando la función logística: El modelo de regresión logística está dado por \\[p_1(x)=p_1(x;\\beta)= h(\\beta_0+\\beta_1x_1 + \\beta_2x_2 +\\cdots + \\beta_p x_p),\\] y \\[p_0(x)=p_0(x;\\beta)=1-p_1(x;\\beta),\\] donde \\(\\beta=(\\beta_0,\\beta_1, \\ldots, \\beta_p)\\). 3.5 Aprendizaje de coeficientes para regresión logística (binomial). Ahora veremos cómo aprender los coeficientes con una muestra de entrenamiento. La idea general es : Usamos la devianza de entrenamiento como medida de ajuste Usamos descenso en gradiente para minimizar esta devianza y aprender los coeficientes. Sea entonces \\({\\mathcal L}\\) una muestra de entrenamiento: \\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}\\] Donde \\(y=1\\) o \\(y=0\\) son las dos clases. Escribimos también \\[p_1(x)=p_1(x;\\beta)= h(\\beta_0+\\beta_1x_1 + \\beta_2x_2 +\\cdots + \\beta_p x_p),\\] y definimos la devianza sobre el conjunto de entrenamiento \\[D(\\beta) = -2\\sum_{i=1}^N \\log(p_{y^{(i)}} (x^{(i)})).\\] Los coeficientes estimados por regresión logística están dados por \\[\\hat{\\beta} = \\arg\\min_\\beta D(\\beta)\\] Para minimizar utilizaremos descenso en gradiente (aunque hay más opciones). La última expresión para \\(D(\\beta)\\) puede ser difícil de operar, pero podemos reescribir como: \\[D(\\beta) = -2\\sum_{i=1}^N y^{(i)} \\log(p_{1} (x^{(i)})) + (1-y^{(i)}) \\log(p_{0} (x^{(i)})).\\] Para hacer descenso en gradiente, necesitamos encontrar \\(\\frac{\\partial D}{\\beta_j}\\) para \\(j=1,2,\\ldots,p\\). Igual que en regresión lineal, comenzamos por calcular la derivada de un término: \\[D^{(i)} (\\beta) = y^{(i)} \\log(p_{1} (x^{(i)})) + (1-y^{(i)}) \\log(1-p_{1} (x^{(i)}))\\] Calculamos primero las derivadas de \\(p_1 (x^{(i)};\\beta)\\) (demostrar la siguiente ecuación): \\[\\frac{\\partial p_1}{\\partial \\beta_0} = {p_1(x^{(i)})(1-p_1(x^{(i)}))},\\] y \\[\\frac{\\partial p_1}{\\partial \\beta_j} = p_1(x^{(i)})(1-p_1(x^{(i)}))x_j^{(i)},\\] Así que \\[\\begin{align*} \\frac{\\partial D^{(i)}}{\\partial \\beta_j} &amp;= \\frac{y^{(i)}}{(p_1(x^{(i)}))}\\frac{\\partial p_1}{\\partial \\beta_j} - \\frac{1- y^{(i)}}{(1-p_1(x^{(i)}))}\\frac{\\partial p_1}{\\partial \\beta_j} \\\\ &amp;= \\left( \\frac{y^{(i)} - p_1(x^{(i)})}{(p_1(x^{(i)}))(1-p_1(x^{(i)}))} \\right )\\frac{\\partial p_1}{\\partial \\beta_j} \\\\ &amp; = \\left ( y^{(i)} - p_1(x^{(i)}) \\right ) x_j^{(i)} \\\\ \\end{align*}\\] para \\(j=0,1,\\ldots,p\\), usando la convención de \\(x_0^{(i)}=1\\). Podemos sumar ahora sobre la muestra de entrenamiento para obtener \\[ \\frac{\\partial D}{\\partial\\beta_j} = - 2\\sum_{i=1}^N (y^{(i)}-p(x^{(i)}))x_j^{(i)}\\] De modo que, Para un paso \\(\\eta&gt;0\\) fijo, la iteración de descenso para regresión logística para el coeficiente \\(\\beta_j\\) es: \\[\\beta_{j}^{(k+1)} = \\beta_j^{(k)} + {2\\eta} \\sum_{i=1}^N (y^{(i)}-p(x^{(i)}))x_j^{(i)}\\] para \\(j=0,1,\\ldots, p\\), donde fijamos \\(x_0^{(i)}=1\\). Podríamos usar las siguientes implementaciones, que representan cambios menores de lo que hicimos en regresión lineal. En primer lugar, escribimos la función que calcula la devianza. Podríamos poner: devianza_calc_simple &lt;- function(x, y){ dev_fun &lt;- function(beta){ p_beta &lt;- h(as.matrix(cbind(1, x)) %*% beta) -2*sum(y*log(p_beta) + (1-y)*log(1-p_beta)) } dev_fun } *Observación Sin embargo, podemos hacer una simplificación para tener mejor desempeño y estabilidad. Observamos que \\[\\log (p_1(x;\\beta)) = \\log\\frac{ e^{x^t \\beta}}{1+ e^{x^t\\beta}} = x^t\\beta - \\log Z\\] donde \\(Z = 1+ e^{x^t\\beta}\\). Por otra parte \\[\\log(p_0(x;\\beta)) = \\log\\frac{ 1}{1+ e^{x^t\\beta}} = - \\log Z\\] De modo que \\[y\\log(p_1(x;\\beta)) + (1- y)\\log(p_0(x;\\beta)) = yx^t\\beta - \\log Z= yx^t\\beta - \\log (1+e^{x^t\\beta})\\] Así que podemos escribir: devianza_calc &lt;- function(x, y){ dev_fun &lt;- function(beta){ x_beta &lt;- as.matrix(cbind(1, x)) %*% beta -2*sum(y*x_beta - log(1 + exp(x_beta))) } dev_fun } grad_calc &lt;- function(x_ent, y_ent){ salida_grad &lt;- function(beta){ p_beta &lt;- h(as.matrix(cbind(1, x_ent)) %*% beta) e &lt;- y_ent - p_beta grad_out &lt;- -2*as.numeric(t(cbind(1,x_ent)) %*% e) names(grad_out) &lt;- c(&#39;Intercept&#39;, colnames(x_ent)) grad_out } salida_grad } descenso &lt;- function(n, z_0, eta, h_deriv){ z &lt;- matrix(0,n, length(z_0)) z[1, ] &lt;- z_0 for(i in 1:(n-1)){ z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ]) } z } Ejemplo Probemos nuestros cálculos con el ejemplo de 1 entrada de tarjetas de crédito. dat_ent$y &lt;- as.numeric(dat_ent$g==1) dat_ent &lt;- dat_ent %&gt;% ungroup %&gt;% mutate(x_s = (x - mean(x))/sd(x)) devianza &lt;- devianza_calc_simple(dat_ent[, &#39;x_s&#39;, drop = FALSE], dat_ent$y) grad &lt;- grad_calc(dat_ent[, &#39;x_s&#39;, drop = FALSE], dat_ent$y) grad(c(0,1)) ## Intercept x_s ## -319.0719 384.3834 grad(c(0.5,-0.1)) ## Intercept x_s ## -185.8135 151.6872 Verificamos cálculo de gradiente: (devianza(c(0.5+0.0001,-0.1)) - devianza(c(0.5,-0.1)))/0.0001 ## [1] -185.8018 (devianza(c(0.5,-0.1+0.0001)) - devianza(c(0.5,-0.1)))/0.0001 ## [1] 151.6991 Y hacemos descenso: iteraciones &lt;- descenso(200, z_0 = c(0,0), eta = 0.001, h_deriv = grad) tail(iteraciones, 20) ## [,1] [,2] ## [181,] 1.772441 -1.100098 ## [182,] 1.772441 -1.100098 ## [183,] 1.772441 -1.100098 ## [184,] 1.772441 -1.100098 ## [185,] 1.772442 -1.100098 ## [186,] 1.772442 -1.100098 ## [187,] 1.772442 -1.100098 ## [188,] 1.772442 -1.100098 ## [189,] 1.772442 -1.100098 ## [190,] 1.772442 -1.100098 ## [191,] 1.772442 -1.100099 ## [192,] 1.772442 -1.100099 ## [193,] 1.772442 -1.100099 ## [194,] 1.772442 -1.100099 ## [195,] 1.772442 -1.100099 ## [196,] 1.772442 -1.100099 ## [197,] 1.772442 -1.100099 ## [198,] 1.772442 -1.100099 ## [199,] 1.772442 -1.100099 ## [200,] 1.772442 -1.100099 #Checamos devianza plot(apply(iteraciones, 1, devianza)) # Y gradiente de devianza en la iteración final: grad(iteraciones[nrow(iteraciones), ]) ## Intercept x_s ## -1.295382e-05 9.393880e-06 Comparamos con glm: mod_1 &lt;- glm(y~x_s, data=dat_ent, family = &#39;binomial&#39;) coef(mod_1) ## (Intercept) x_s ## 1.772442 -1.100099 mod_1$deviance ## [1] 395.6225 devianza(iteraciones[200,]) ## [1] 395.6225 Nótese que esta devianza está calculada sin dividirentre el número de casos. Podemos calcular la devianza promedio de entrenamiento haciendo: devianza(iteraciones[200,])/nrow(dat_ent) ## [1] 0.7912451 Máxima verosimilitud Es fácil ver que este método de estimación de los coeficientes (minimizando la devianza de entrenamiento) es el método de máxima verosimilitud. La verosimilitud de la muestra de entrenamiento está dada por: \\[L(\\beta) =\\prod_{i=1}^N p_{y^{(i)}} (x^{(i)})\\] Y la log verosimilitud es \\[l(\\beta) =\\sum_{i=1}^N \\log(p_{y^{(i)}} (x^{(i)})).\\] Así que ajustar el modelo minimizando la expresión (3.1) es los mismo que hacer máxima verosimilitud (condicional a los valores de \\(x\\)). Normalización Igual que en regresión lineal, en regresión logística conviene normalizar las entradas antes de ajustar el modelo Desempeño de regresión logística como método de aprendizaje Igual que en regresión lineal, regresión logística supera a métodos más sofisticados o nuevos en numerosos ejemplos. Las razones son similares: la rigidez de regresión logística es una fortaleza cuando la estructura lineal es una buena aproximación. 3.5.0.1 Solución analítica El problema de regresión logística no tiene solución analítica. Paquetes como glm utilizan métodos numéricos (Newton-Raphson para regresión logística, por ejemplo). 3.5.0.2 Interpretación de modelos logísticos Todas las precauciones que mencionamos en modelos lineales aplican para los modelos logísticos (aspectos estadísticos del ajuste, relación con fenómeno de interés, argumentos de causalidad). Igual que en regresión lineal, podemos explicar el comportamiento de las probabilidades de clase ajustadas, pero es un poco más difícil por la no linealidad introducida por la función logística. Ejemplo Consideremos el modelo ajustado: head(dat_ent) ## # A tibble: 6 x 4 ## x g y x_s ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.709 1 1 -1.20 ## 2 33.9 1 1 -0.0755 ## 3 50.0 1 1 0.471 ## 4 27.8 1 1 -0.280 ## 5 94.5 1 1 1.98 ## 6 19.8 1 1 -0.553 coeficientes &lt;- iteraciones[200,] names(coeficientes) &lt;- c(&quot;Intercept&quot;, &quot;x_s&quot;) coeficientes ## Intercept x_s ## 1.772442 -1.100099 Como centramos todas las entradas, la ordenada al origen (Intercept) se interpreta como la probabilidad de clase cuando todas las variables están en su media: options(digits = 2) coeficientes[1] ## Intercept ## 1.8 h(coeficientes[1]) ## Intercept ## 0.85 Esto quiere decir que la probabilidad de estar al corriente es de 87% cuando la variable \\(x\\) está en su media. Si \\(x\\) se incrementa en una desviación estándar, la cantidad \\[z = \\beta_0 + \\beta_1x\\] la probabilidad de estar al corriente cambia a 67%: h(coeficientes[1]+ coeficientes[2]*1) ## Intercept ## 0.66 Nótese que una desviación estándar de \\(x\\) equivale a sd(dat_ent$x) ## [1] 29 Así que en las unidades originales, un incremento de 30 en la variable \\(x\\) implica un cambio de h(coeficientes[1] + coeficientes[2]) - h(coeficientes[1]) ## Intercept ## -0.19 es decir, la probabilidad de manenterse al corriente baja 19 puntos porcentuales, de 85% a 67% Ojo: En regresión lineal, las variables contribuyen independientemente de otras al predictor. Eso no pasa en regresión logística debido a la no linealidad introducida por la función logística \\(h\\). Por ejemplo, imaginemos el modelo: \\[p(z) = h(0.5 + 0.2 x_1 -0.5 x_2 + 0.7x_3),\\] y suponemos las entradas normalizadas. Si todas las variables están en su media, la probabilidad de clase 1 es h(0.5) ## [1] 0.62 Si todas las variables están en su media, y cambiamos en 1 desviación estándar la variable \\(x_1\\), la probabilidad de clase 1 es: h(0.5+0.2) ## [1] 0.67 Y el cambio en puntos de probabilidad es: h(0.5+0.2) - h(0.5) ## [1] 0.046 Pero si la variable \\(x_2 = -1\\), por ejemplo, el cambio en probabilidad es de h(0.5+ 0.2 + 0.5*1) - h(0.5 + 0.5*1) ## [1] 0.037 3.6 Ejercicio: datos de diabetes Ya están divididos los datos en entrenamiento y prueba diabetes_ent &lt;- as_data_frame(MASS::Pima.tr) diabetes_pr &lt;- as_data_frame(MASS::Pima.te) diabetes_ent ## # A tibble: 200 x 8 ## npreg glu bp skin bmi ped age type ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; ## 1 5 86 68 28 30.2 0.364 24 No ## 2 7 195 70 33 25.1 0.163 55 Yes ## 3 5 77 82 41 35.8 0.156 35 No ## 4 0 165 76 43 47.9 0.259 26 No ## 5 0 107 60 25 26.4 0.133 23 No ## 6 5 97 76 27 35.6 0.378 52 Yes ## 7 3 83 58 31 34.3 0.336 25 No ## 8 1 193 50 16 25.9 0.655 24 No ## 9 3 142 80 15 32.4 0.2 63 No ## 10 2 128 78 37 43.3 1.22 31 Yes ## # ... with 190 more rows diabetes_ent$id &lt;- 1:nrow(diabetes_ent) diabetes_pr$id &lt;- 1:nrow(diabetes_pr) Normalizamos library(dplyr) library(tidyr) datos_norm &lt;- diabetes_ent %&gt;% gather(variable, valor, npreg:age) %&gt;% group_by(variable) %&gt;% summarise(media = mean(valor), de = sd(valor)) normalizar &lt;- function(datos, datos_norm){ datos %&gt;% gather(variable, valor, npreg:age) %&gt;% left_join(datos_norm) %&gt;% mutate(valor_s = (valor - media)/de) %&gt;% select(id, type, variable, valor_s) %&gt;% spread(variable, valor_s) } diabetes_ent_s &lt;- normalizar(diabetes_ent, datos_norm) diabetes_pr_s &lt;- normalizar(diabetes_pr, datos_norm) x_ent &lt;- diabetes_ent_s %&gt;% select(age:skin) %&gt;% as.matrix p &lt;- ncol(x_ent) y_ent &lt;- diabetes_ent_s$type == &#39;Yes&#39; grad &lt;- grad_calc(x_ent, y_ent) iteraciones &lt;- descenso(1000, rep(0,p+1), 0.001, h_deriv = grad) matplot(iteraciones) diabetes_coef &lt;- data_frame(variable = c(&#39;Intercept&#39;,colnames(x_ent)), coef = iteraciones[1000,]) diabetes_coef ## # A tibble: 8 x 2 ## variable coef ## &lt;chr&gt; &lt;dbl&gt; ## 1 Intercept -0.956 ## 2 age 0.452 ## 3 bmi 0.513 ## 4 bp -0.0547 ## 5 glu 1.02 ## 6 npreg 0.347 ## 7 ped 0.559 ## 8 skin -0.0225 Ahora calculamos devianza de prueba y error de clasificación: x_prueba &lt;- diabetes_pr_s %&gt;% select(age:skin) %&gt;% as.matrix y_prueba &lt;- diabetes_pr_s$type == &#39;Yes&#39; dev_prueba &lt;- devianza_calc(x_prueba, y_prueba) dev_prueba(iteraciones[1000,])/nrow(x_prueba) ## [1] 0.88 Y para el error clasificación de prueba, necesitamos las probabilidades de clase ajustadas: beta &lt;- iteraciones[1000, ] p_beta &lt;- h(as.matrix(cbind(1, x_prueba)) %*% beta) y_pred &lt;- as.numeric(p_beta &gt; 0.5) mean(y_prueba != y_pred) ## [1] 0.2 Tarea La tarea está en tareas/tarea_3_actualizada.Rmd. (Nota: la versión anterior la dejaremos para más tarde. Si ya resolvieron la versión anterior no hay problema. Si tienen dudas pueden escribirme a felipexgonzalez at gmail dot com). "],
["mas-sobre-problemas-de-clasificacion.html", "Clase 4 Más sobre problemas de clasificación 4.1 Análisis de error para clasificadores binarios 4.2 Perfil de un clasificador binario y curvas ROC 4.3 Regresión logística para problemas de más de 2 clases 4.4 Descenso en gradiente para regresión multinomial logística", " Clase 4 Más sobre problemas de clasificación En esta parte presentamos técnicas adicionales para evaluar el desempeño de un modelo. En la parte anterior vimos que La devianza es una buena medida para ajustar y evaluar el desempeño de un modelo y comparar modelos, y utiliza las probabilidades de clase. Sin embargo, es una medida de dificil de interpretar en cuanto a los errores que podemos esperar del modelo. Por otro lado, la tasa de clasificación incorrecta puede usarse para evaluar el desempeño de un clasificador (incluyendo uno derivado de probabilidades de clase), puede interpretarse con facilidad, pero se queda corta en muchas aplicaciones. Una deficiencia grande de esta medida es que, contrario al problema de regresión, hay errores de clasificación que son cualitativamente diferentes. Ejemplo Por ejemplo, diagnosticar a alguien con una enfermedad cuando no la tiene tiene consecuencias distintas a diagnosticar como libre de enfermedad a alguien que la tiene. Estas consecuencias dependen de cómo son son los tratamientos consecuentes, y de qué tan peligrosa es la enfermedad. Cuando usamos un buscador como Google, es cualitativamente diferente que el buscador omita resultados relevantes a que nos presente resultados irrelevantes. ¿Otros ejemplos? En general, los costos de los distintos errores son distintos, y en muchos problemas quiséramos entenderlos y controlarlos individualmente. Aunque en teoría podríamos asignar costos a los errores y definir una función de pérdida apropiada, en la práctica esto muchas veces no es tan fácil o deseable. Podemos, sin embargo, reportar el tipo de errores que ocurren Matriz de confusión. Sea \\(\\hat{G}\\) un clasificador. La matriz de confusión \\(C\\) de \\(\\hat{G}\\) está dada por \\(C_{i,j} =\\) número de casos de la clase verdadera \\(j\\) que son clasificados como clase \\(i\\) por el clasificador Ejemplo En un ejemplo de tres clases, podríamos obtener la matriz de confusión: A B C A.pred 50 2 0 B.pred 20 105 10 C.pred 20 10 30 Esto quiere decir que de 90 casos de clase \\(A\\), sólo clasificamos a 50 en la clase correcta, de 117 casos de clase \\(B\\), acertamos en 105, etcétera. Podemos ver esta tabla de distintas formas, por ejemplo, usando porcentajes por columna, nos dice cómo se distribuyen los casos de cada clase: knitr::kable(round(prop.table(tabla_1, 2),2)) A B C A.pred 0.56 0.02 0.00 B.pred 0.22 0.90 0.25 C.pred 0.22 0.09 0.75 Mientras que una tabla de porcentajes por renglón nos muestra qué pasa cada vez que hacemos una predicción dada: knitr::kable(round(prop.table(tabla_1, 1),2)) A B C A.pred 0.96 0.04 0.00 B.pred 0.15 0.78 0.07 C.pred 0.33 0.17 0.50 Ahora pensemos cómo podría sernos de utilidad esta tabla. Discute El clasificador fuera uno de severidad de emergencias en un hospital, donde A=requiere atención inmediata B=urgente C=puede posponerse. El clasificador fuera de tipos de cliente de un negocio. Por ejemplo, A = cliente de gasto alto, B=cliente medio, C=abandonador. Imagínate que tiene un costo intentar conservar a un abandonador, y hay una inversión alta para tratar a los clientes A. La tasa de incorrectos es la misma en los dos ejemplos, pero la adecuación del clasificador es muy diferente. 4.1 Análisis de error para clasificadores binarios Cuando la variable a predecir es binaria (dos clases), podemos etiquetar una clase como positiva y otra como negativa. En el fondo no importa cómo catalogemos cada clase, pero para problemas particulares una asignación puede ser más natural. Por ejemplo, en diagnóstico de enfermedades, positivo=tiene la enfermedad, en análisis de crédito, positivo=cae en impago, en sistemas de recomendacion, positivo = le gusta el producto X, en recuperación de textos, positivo=el documento es relevante a la búsqueda, etc. Hay dos tipos de errores en un clasificador binario (positivo - negativo): Falsos positivos (fp): clasificar como positivo a un caso negativo. Falsos negativos (fn): clasificar como negativo a un caso positivo. A los casos clasificados correctamente les llamamos positivos verdaderos (pv) y negativos verdaderos (nv). La matriz de confusion es entonces tabla &lt;- data_frame(&#39; &#39; = c(&#39;positivo.pred&#39;,&#39;negativo.pred&#39;,&#39;total&#39;), &#39;positivo&#39;=c(&#39;vp&#39;,&#39;fn&#39;,&#39;pos&#39;), &#39;negativo&#39;=c(&#39;fp&#39;,&#39;nv&#39;,&#39;neg&#39;), &#39;total&#39; = c(&#39;pred.pos&#39;,&#39;pred.neg&#39;,&#39;&#39;)) knitr::kable(tabla) pos itivo neg ativo tot al positivo.pred vp fp pred.pos negativo.pred fn nv pred.neg total pos neg Nótese que un clasificador bueno, en general, es uno que tiene la mayor parte de los casos en la diagonal de la matriz de confusión. Podemos estudiar a nuestro clasificador en términos de las proporciones de casos que caen en cada celda, que dependen del desempeño del clasificador en cuanto a casos positivos y negativos. La nomenclatura puede ser confusa, pues en distintas áreas se usan distintos nombres para estas proporciones: Tasa de falsos positivos \\[\\frac{fp}{fp+nv}=\\frac{fp}{neg}\\] Tasa de falsos negativos \\[\\frac{fn}{pv+fn}=\\frac{fn}{pos}\\] Especificidad \\[\\frac{vn}{fp+vn}=\\frac{vn}{neg}\\] Sensibilidad o Recall \\[\\frac{vp}{vp+fn}=\\frac{vp}{pos}\\] Y también otras que tienen como base las predicciones: Valor predictivo positivo o Precisión \\[\\frac{vp}{vp+fp}=\\frac{vp}{pred.pos}\\] Valor predictivo negativo \\[\\frac{vn}{fn+vn}=\\frac{vn}{pred.neg}\\] Dependiendo de el tema y el objetivo hay medidas más naturales que otras: En estadística muchas veces se usa sensibilidad (recall) y especificidad (proporción de positivos que detectamos y proporción de negativos que descartamos). Por ejemplo, si se tratara de una prueba para detectar riesgo de una enfermedad, sensibilidad nos dice qué porcentaje de los casos riesgosos estamos capturando (sensibilidad), y especificidad nos dice qué tan bien excluimos a los casos no riesgosos (especificidad). Estas dos medidas muestran directamente como el clasificador discrimina entre positivos y entre negativos. En búsqueda y recuperación de documentos o imagenes, o detección de fraude ( donde positivo = el documento es relevante / la transacción es fraudulenta y negativo = el documento no es relevante / transacción normal), se usa más comunmente precisión y recall. Esto es porque nos interesa saber de todos los resultados con predicción positiva (documentos o imagenes recuperadas), qué porcentaje son relevantes (precisión), y también, de todos los documentos relevantes (positivos), cuáles son recuperados (recall). Pero tiende a considerarse que especificidad o tasas de falsos positivos es menos útil, pues estas son cantidades dependen de una gran cantidad de documentos o transacciones irrelevantes, y tienden a ser cercanas a 1 para cualquier clasificador razonable (¿por qué?). Cada clasificador tiene un balance distinto especificidad-sensibliidad. Muchas veces no escogemos clasificadores por la tasa de incorrectos solamente, sino que intentamos buscar un balance adecuado entre el comportamiento de clasificación para positivos y para negativos. Medidas resumen de desempeño La primera medida resumen que vimos es el error de clasificación, que no toma en cuenta el tipo de errores: Tasa de clasificación incorrecta \\[\\frac{fn+fv}{neg+pos}\\] Y existen otras medidas que intentan resumir los dos tipos de errores de distinta manera, como Medida F (media armónica de precisión y recall) \\[2\\frac{precision \\cdot recall}{precision + recall}\\] Se usa la la media armónica que penaliza más fuertemente desempeño malo en alguna de nuestras dos medidas (precisión y recall) que el promedio armónico. Ejemplo Si precision = 0.01 (muy malo) y recall = 1 (excelente), o recall=0.01 y precisión = 1 (excelente), la media usual considera igual de buenos estos dos clasificadores. A su vez, estos dos se califican similar a un clasificador con precision = 0.5 y recall = 0.5. Sin embargo, la media armónica (F) da un score mucho más bajo a los primeros dos clasificadores: media_armonica &lt;- function(x){ 1/mean(1/x) } media_armonica(c(0.01, 1)) ## [1] 0.01980198 media_armonica(c(0.5, 0.5)) ## [1] 0.5 AUC (area bajo la curva ROC) que veremos más adelante. Interpetación de resúmenes de desempeño y tasas base Cuando consideramos las tasas de desempeño de un clasificador debemos comparar con lo que sucedería si no usáramos el clasificador (si no usáramos datos). Una manera de hacer esto consiste en utilizar el clasificador base: Clasificar todo como negativo (o positivo) Ejemplo Consideramos un problema donde tenemos 20% de positivos y 80% de negativos en una población. ¿Cuál es el desempeño de estos clasificadores base? Podemos usar la ecuación \\[P(acertar) = P(acertar|pos)P(pos) + P(acertar|neg)P(neg)\\] que en este caso es: \\[P(acertar) = 0.2P(acertar|pos) + 0.8 P(acertar|neg)\\] Si clasificamos siempre a positivo, la tasa de correctos será de 20%, pues \\(P(acertar|pos) = 1\\) y \\(P(acertar|neg)= 0\\) Si clasificamos siempre a negativo, la tasa de correctos será de 80%, pues \\(P(acertar|pos) = 0\\) y \\(P(acertar|neg)= 1\\) En este ejemplo, si tuviéramos un clasificador con una tasa de correctos de \\(75\\%\\), querría decir que no logramos mucho en el sentido de la certeza de nuestra predicción. Comparamos la tasa de correctos de clasificadores contra la tasa base \\(\\max\\{p_{pos}, 1- p_{pos}\\}\\), donde \\(p_{pos}\\) es la tasa de positivos en nuestros datos. Típicamente esperamos superar esta tasa de correctos base. Nota: cuando los costos de los distintos errores son muy diferentes, existen otras medidas más apropiadas Ejercicio Calcular la matriz de confusión (sobre la muestra de prueba) para el clasificador logístico de diabetes en términos de glucosa. Calcula adicionalmente con la muestra de prueba sus valores de especificidad y sensibilidad, y precisión y recall. diabetes_ent &lt;- as_data_frame(MASS::Pima.tr) diabetes_pr &lt;- as_data_frame(MASS::Pima.te) mod_1 &lt;- glm(type ~ glu, data = diabetes_ent, family = &#39;binomial&#39;) preds_prueba &lt;- predict(mod_1, newdata = diabetes_pr, type =&#39;response&#39;) # rellena esta linea en términos de preds_prueba # clase_pred &lt;- # table(clase_pred) # ahora calcula la matriz de confusión # table() # Usando esta tabla encuentra # tasa de incorrectos especificidad y sensibilidad, precisión y recall. 4.1.1 Puntos de corte para un clasificador binario ¿Qué sucede cuando el perfil de sensibilidad y especificidad de un clasificador binario no es apropiado para nuestros fines? Recordemos que una vez que hemos estimado con \\(\\hat{p}_1(x)\\), nuestra regla de clasificación es: Predecir positivo si \\(\\hat{p}_1(x) &gt; 0.5\\), Predecir negativo si \\(\\hat{p}_1(x) \\leq 0.5.\\) Esto sugiere una regla alternativa: Para \\(0 &lt; d &lt; 1\\), podemos utilizar nuestras estimaciones \\(\\hat{p}_1(x)\\) para construir un clasificador alternativo poniendo: Predecir positivo si \\(\\hat{p}_1(x) &gt; d\\), Predecir negativo si \\(\\hat{p}_1(x) \\leq d\\). Distintos valores de \\(d\\) dan distintos perfiles de sensibilidad-especificidad para una misma estimación de las probabilidades condicionales de clase: Para minimizar la tasa de incorrectos conviene poner \\(d = 0.5\\). Sin embargo, es común que este no es el único fin de un clasificador bueno (pensar en ejemplo de fraude). Cuando incrementamos d, quiere decir que exigimos estar más seguros de que un caso es positivo para clasificarlo como positivo. Eso quiere decir que la especifidad va a ser más grande (entre los negativos verdaderos va a haber menos falsos positivos). Sin embargo, la sensibilidad va a ser más chica pues captamos menos de los verdaderos positivos. Ejemplo Por ejemplo, si en el caso de diabetes incrementamos el punto de corte a 0.7: table(preds_prueba &gt; 0.7, diabetes_pr$type) ## ## No Yes ## FALSE 220 77 ## TRUE 3 32 tab &lt;- prop.table(table(preds_prueba &gt; 0.7, diabetes_pr$type), 2) tab ## ## No Yes ## FALSE 0.98654709 0.70642202 ## TRUE 0.01345291 0.29357798 La especificidad ahora 0.99 , muy alta (descartamos muy bien casos negativos), pero la sensibilidad se deteriora a 0.29 Cuando hacemos más chico d, entonces exigimos estar más seguros de que un caso es negativo para clasificarlo como negativo. Esto aumenta la sensibilidad, pero la especificidad baja. Por ejemplo, si en el caso de diabetes ponemos el punto de corte en 0.3: table(preds_prueba &gt; 0.3, diabetes_pr$type) ## ## No Yes ## FALSE 170 37 ## TRUE 53 72 tab &lt;- prop.table(table(preds_prueba &gt; 0.3, diabetes_pr$type),2) tab ## ## No Yes ## FALSE 0.7623318 0.3394495 ## TRUE 0.2376682 0.6605505 Ejemplo Podemos tener una intuición de cómo cambian las tasas de error dependiendo de donde cortamos mostrando la tabla ordenada por probabilidades estimadas (incluimos también las covariables para entender qué variables están más correlacionadas con las probabilidades): library(tabplot) mod_1 &lt;- glm(type ~ glu, diabetes_ent, family = &#39;binomial&#39;) diabetes_pr$probs_prueba_1 &lt;- predict(mod_1, newdata = diabetes_pr, type = &quot;response&quot;) head(arrange(diabetes_pr, desc(probs_prueba_1))) ## # A tibble: 6 x 9 ## npreg glu bp skin bmi ped age type probs_prueba_1 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 2 197 70 45 30.5 0.158 53 Yes 0.874 ## 2 4 197 70 39 36.7 2.33 31 No 0.874 ## 3 8 196 76 29 37.5 0.605 57 Yes 0.870 ## 4 1 196 76 36 36.5 0.875 29 Yes 0.870 ## 5 3 193 70 31 34.9 0.241 25 Yes 0.857 ## 6 5 189 64 33 31.2 0.583 29 Yes 0.837 tableplot(diabetes_pr, sortCol = probs_prueba_1) La columna de probabilidad de la derecha nos dice en qué valores podemos cortar para obtener distintos clasificadores. Nótese que si cortamos más arriba, se nos escapan más positivos verdaderos que clasificamos como negativos, pero clasificamos a más negativos verdaderos como negativos. Lo opuesto ocurre cuando cortamos más abajo. 4.1.2 Espacio ROC de clasificadores Podemos visualizar el desempeño de cada uno de estos clasificadores construidos con puntos de corte mapeándolos a las coordenadas de tasa de falsos positivos (1-especificidad) y sensibilidad: clasif_1 &lt;- data.frame( corte = c(&#39;0.3&#39;,&#39;0.5&#39;,&#39;0.7&#39;,&#39;perfecto&#39;,&#39;azar&#39;), tasa_falsos_pos=c(0.24,0.08,0.02,0,0.7), sensibilidad =c(0.66, 0.46,0.19,1,0.7)) ggplot(clasif_1, aes(x=tasa_falsos_pos, y=sensibilidad, label=corte)) + geom_point() + geom_abline(intercept=0, slope=1) + xlim(c(0,1)) +ylim(c(0,1)) + geom_text(hjust=-0.3, col=&#39;red&#39;)+ xlab(&#39;1-especificidad (tasa falsos pos)&#39;) Nótese que agregamos otros dos clasificadores, uno perfecto, que tiene tasa de falsos positivos igual a 0 y sensibilidad igual a 1. En esta gráfica, un clasificador \\(G_2\\) que está arriba a la izquierda de \\(G_1\\) domina a \\(G_1\\), pues tiene mejor especificidad y mejor sensibilidad. Entre los clasificadores 0.3, 0.5 y 0.7 de la gráfica, no hay ninguno que domine a otro. Todos los clasificadores en la diagonal son equivalentes a un clasificador al azar. ¿Por qué? La razón es que si cada vez que vemos un nuevo caso lo clasificamos como positivo con probabilidad \\(p\\) fija y arbitraria. Esto implica que cuando veamos un caso positivo, la probabilidad de ’atinarle’ es de p (sensibilidad), y cuando vemos un negativo, la probabilidad de equivocarnos también es de 1-p (tasa de falsos positivos), por lo que la espcificidad es p también. De modo que este clasificador al azar está en la diagonal. ¿Qué podemos decir acerca de clasificadores que caen por debajo de la diagonal? Estos son clasificadores particularmente malos, pues existen clasificadores con mejor especificidad y/o sensibilidad que son clasificadores al azar! Sin embargo, se puede construir un mejor clasificador volteando las predicciones, lo que cambia sensibilidad por tasa de falsos positivos. ¿Cuál de los tres clasificadores es el mejor? En términos de la tasa de incorrectos, el de corte 0.5. Sin embargo, para otros propósitos puede ser razonable escoger alguno de los otros. 4.2 Perfil de un clasificador binario y curvas ROC En lugar de examinar cada punto de corte por separado, podemos hacer el análisis de todos los posibles puntos de corte mediante la curva ROC (receiver operating characteristic, de ingeniería). Para un problema de clasificación binaria, dadas estimaciones \\(\\hat{p}(x)\\), la curva ROC grafica todos los pares de (1-especificidad, sensibilidad) para cada posible punto de corte \\(\\hat{p}(x) &gt; d\\). Vamos a graficar todos los pares (1-especificidad, sensibilidad) para cada punto de corte \\(d\\) de estas probabilidades. library(ROCR) pred_rocr &lt;- prediction(diabetes_pr$probs_prueba_1, diabetes_pr$type) perf &lt;- performance(pred_rocr, measure = &quot;sens&quot;, x.measure = &quot;fpr&quot;) graf_roc_1 &lt;- data_frame(tfp = perf@x.values[[1]], sens = perf@y.values[[1]], d = perf@alpha.values[[1]]) ggplot(graf_roc_1, aes(x = tfp, y = sens, colour=d)) + geom_point() + xlab(&#39;1-especificidad&#39;) + ylab(&#39;Sensibilidad&#39;) En esta gráfica podemos ver todos los clasificadores posibles basados en las probabilidades de clase. Podemos usar estas curvas como evaluación de nuestros clasificadores, dejando para más tarde la selección del punto de corte, si esto es necesario (por ejemplo, dependiendo de los costos de cada tipo de error). También podemos definir una medida resumen del desempeño de un clasificador según esta curva: La medida AUC (area under the curve) para un clasificador es el área bajo la curva generada por los pares sensibilidad-especificidad de la curva ROC. auc_1 &lt;- performance(pred_rocr, measure = &#39;auc&#39;)@y.values auc_1[[1]] ## [1] 0.7970543 También es útil para comparar modelos. Consideremos el modelo de los datos de diabetes que incluyen todas las variables: mod_2 &lt;- glm(type ~ ., diabetes_ent, family = &#39;binomial&#39;) diabetes_pr$probs_prueba_2 &lt;- predict(mod_2, newdata = diabetes_pr, type = &quot;response&quot;) head(arrange(diabetes_pr, desc(probs_prueba_2))) ## # A tibble: 6 x 10 ## npreg glu bp skin bmi ped age type probs_prueba_1 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 0 180 78 63 59.4 2.42 25 Yes 0.785 ## 2 4 197 70 39 36.7 2.33 31 No 0.874 ## 3 5 187 76 27 43.6 1.03 53 Yes 0.827 ## 4 3 173 82 48 38.4 2.14 25 Yes 0.737 ## 5 0 173 78 32 46.5 1.16 58 No 0.737 ## 6 17 163 72 41 40.9 0.817 47 Yes 0.658 ## # ... with 1 more variable: probs_prueba_2 &lt;dbl&gt; tableplot(diabetes_pr, sortCol = probs_prueba_2) Y graficamos juntas: library(ROCR) pred_rocr &lt;- prediction(diabetes_pr$probs_prueba_2, diabetes_pr$type) perf &lt;- performance(pred_rocr, measure = &quot;sens&quot;, x.measure = &quot;fpr&quot;) auc_2 &lt;- performance(pred_rocr, measure = &quot;auc&quot;)@y.values graf_roc_2 &lt;- data_frame(tfp = perf@x.values[[1]], sens = perf@y.values[[1]], d = perf@alpha.values[[1]]) graf_roc_2$modelo &lt;- &#39;Todas las variables&#39; graf_roc_1$modelo &lt;- &#39;Solo glucosa&#39; graf_roc &lt;- bind_rows(graf_roc_1, graf_roc_2) ggplot(graf_roc, aes(x = tfp, y = sens, colour = modelo)) + geom_point() + xlab(&#39;1-especificidad&#39;) + ylab(&#39;Sensibilidad&#39;) Comparación auc: auc_1 ## [[1]] ## [1] 0.7970543 auc_2 ## [[1]] ## [1] 0.8658823 En este ejemplo, vemos que casi no importa que perfil de especificidad y sensibilidad busquemos: el clasificador que usa todas las variables domina casi siempre al clasificador que sólo utiliza las variables de glucosa. La razón es que para cualquier punto de corte (con sensibilidad menor a 0.4) en el clasificador de una variable, existe otro clasificador en la curva roja (todas las variable), que domina al primero. La excepción es para clasificadores de valores de sensibilidad baja, con tasas de falsos positivos muy chicas: en este caso, el modelo de una variable puede ser ligeramente superior. 4.3 Regresión logística para problemas de más de 2 clases Consideramos ahora un problema con más de dos clases, de manera que \\(G ∈ {1,2,...,K}\\) (\\(K\\) clases), y tenemos \\(X = (X1 ...,Xp)\\) entradas. ¿Cómo generalizar el modelo de regresión logística para este problema? Una estrategia es la de uno contra todos: En clasificación uno contra todos, hacemos Para cada clase \\(g\\in\\{1,\\ldots,K\\}\\) entrenamos un modelo de regresión logística (binaria) \\(\\hat{p}^{(g)}(x)\\), tomando como positivos a los casos de 1 clase \\(g\\), y como negativos a todo el resto. Esto lo hacemos como en las secciones anteriores, y de manera independiente para cada clase. Para clasificar un nuevo caso \\(x\\), calculamos \\[\\hat{p}^{(1)}, \\hat{p}^{(2)},\\ldots, \\hat{p}^{(K)}\\] y clasificamos a la clase de máxima probabilidad \\[\\hat{G}(x) = \\arg\\max_g \\hat{p}^{(g)}(x)\\] Nótese que no hay ninguna garantía de que las probabilidades de clase sumen 1, pues se trata de estimaciones independientes de cada clase. En este sentido, produce estimaciones que en realidad no satisfacen las propiedades del modelo de probabilidad establecido - aunque pueden normalizarse. Sin embargo, esta estrategia es simple y en muchos casos funciona bien. 4.3.1 Regresión logística multinomial Si queremos obtener estimaciones de las probabilidades de clase que sumen uno, entonces tenemos que contruir las estimaciones de cada clase de clase de manera conjunta. Como vimos antes, tenemos que estimar, para cada \\(x\\) y \\(g\\in\\{1,\\ldots, K\\}\\), las probabilidades condicionales de clase: \\[p_g(x) = P(G = g|X = x).\\] Consideremos primero cómo funciona el modelo de regresión logística (2 clases) Tenemos que \\[p_1(x) = h(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_p x_p) = \\exp(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_p x_p)/Z \\] y \\[p_2 (x) = 1/Z\\] donde \\(Z = 1 + \\exp(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_p x_p)\\). Podemos generalizar para más de 2 clases usando una idea similar. Cada clase tiene su juego de coeficientes: \\[p_1(x) = \\exp(\\beta_{0,1} + \\beta_{1,1}x_1 + \\ldots + \\beta_{p,1} x_p)/Z\\] \\[p_2(x) = \\exp(\\beta_{0,2} + \\beta_{1,2}x_2 + \\ldots + \\beta_{p.2} x_p)/Z\\] hasta \\[p_{K-1}(x) = \\exp(\\beta_{0,{K-1}} + \\beta_{1,{K-1}}x_2 + \\ldots + \\beta_{p,{K-1}} x_p)/Z\\] y \\[p_K(x) = 1/Z\\] En este caso, para que las probabilidades sumen 1, necesitamos que \\[Z = 1 + \\sum_{j=1}^{K-1}\\exp(\\beta_0^j + \\beta_1^jx_1 + \\ldots + \\beta_p^j x_p)\\] Para ajustar coeficientes, usamos el mismo criterio de devianza de entrenamiento. Buscamos minimizar: \\[D(\\beta)=−2 \\sum_{i=1}^N \\log p_{g^{(i)}}(x^{(i)}),\\] Donde \\(\\beta\\) contiene todos los coeficientes organizados en un vector de tamaño \\((p+1)(K-1)\\): \\[\\beta = ( \\beta_0^1, \\beta_1^1, \\ldots , \\beta_p^1, \\beta_0^2, \\beta_1^2, \\ldots , \\beta_p^2, \\ldots \\beta_0^{K-1}, \\beta_1^{K-1}, \\ldots , \\beta_p^{K-1} )\\] Y ahora podemos usar algún método númerico para minimizar la devianza (por ejemplo, descenso en gradiente). Cuando es muy importante tener probabilidades bien calibradas, el enfoque multinomial es más apropiado, pero muchas veces, especialmente si sólo nos interesa clasificar, los dos métodos dan resultados similares. 4.3.2 Interpretación de coeficientes Los coeficientes mostrados en la parametrización de arriba se intrepretan más fácilmente como comparaciones de la clase \\(g\\) contra la clase \\(K\\), pues \\[\\log\\left (\\frac{p_g(x)}{p_K(x)}\\right ) = \\beta_{0,{g}} + \\beta_{1,{g}}x_1 + \\ldots + \\beta_{p,{g}} x_p\\] Para comparar la clase \\(j\\) con la clase \\(k\\) notamos que \\[\\log\\left (\\frac{p_j(x)}{p_k(x)}\\right ) = (\\beta_{0,{j}}- \\beta_{0,{k}}) + (\\beta_{1,{j}}-\\beta_{1,{k}} )x_1 + \\ldots + (\\beta_{p,{j}} -\\beta_{p,{k}}) x_p\\] Así que sólo hace falta restar los coeficientes. Nótese adicionalmente que en la parametrización, podemos pensar que \\[\\beta_{0,K} = \\beta_{1,K} = \\cdots = \\beta_{p,K} = 0\\] 4.3.3 Ejemplo: Clasificación de dígitos con regresión multinomial digitos_entrena &lt;- read_csv(&#39;datos/zip-train.csv&#39;) ## Parsed with column specification: ## cols( ## .default = col_double() ## ) ## See spec(...) for full column specifications. digitos_prueba &lt;- read_csv(&#39;datos/zip-test.csv&#39;) ## Parsed with column specification: ## cols( ## .default = col_double() ## ) ## See spec(...) for full column specifications. names(digitos_entrena)[1] &lt;- &#39;digito&#39; names(digitos_entrena)[2:257] &lt;- paste0(&#39;pixel_&#39;, 1:256) names(digitos_prueba)[1] &lt;- &#39;digito&#39; names(digitos_prueba)[2:257] &lt;- paste0(&#39;pixel_&#39;, 1:256) En este ejemplo, usamos la función multinom de nnet, que usa BFGS para hacer la optimización: library(nnet) mod_mult &lt;- multinom(digito ~ ., data = digitos_entrena, MaxNWt=100000, maxit = 20) ## # weights: 2580 (2313 variable) ## initial value 16788.147913 ## iter 10 value 2598.959017 ## iter 20 value 1494.978090 ## final value 1494.978090 ## stopped after 20 iterations Checamos para diagnóstico la matriz de confusión de entrenamiento. table(predict(mod_mult), digitos_entrena$digito) ## ## 0 1 2 3 4 5 6 7 8 9 ## 0 1153 0 5 2 3 9 1 1 7 0 ## 1 0 998 0 0 2 0 1 1 2 3 ## 2 2 0 693 1 7 2 8 3 10 2 ## 3 9 0 15 632 2 21 0 2 24 2 ## 4 3 2 9 2 621 4 4 9 10 44 ## 5 24 4 5 19 9 511 43 1 34 6 ## 6 2 0 0 0 1 3 607 0 0 0 ## 7 0 0 1 0 0 1 0 613 1 8 ## 8 1 1 3 2 2 4 0 1 451 2 ## 9 0 0 0 0 5 1 0 14 3 577 Ahora validamos con la muestra de prueba y calculamos error de clasificación: confusion_prueba &lt;- table(predict(mod_mult, newdata = digitos_prueba), digitos_prueba$digito) confusion_prueba ## ## 0 1 2 3 4 5 6 7 8 9 ## 0 335 0 3 0 3 6 4 0 3 0 ## 1 0 252 0 0 1 0 0 0 1 3 ## 2 1 1 171 4 8 0 5 2 3 2 ## 3 3 3 8 145 1 18 0 3 12 0 ## 4 3 6 7 1 176 1 2 6 8 16 ## 5 11 1 5 13 2 130 13 2 14 1 ## 6 4 1 1 0 2 0 143 0 1 0 ## 7 0 0 1 1 2 1 0 130 1 5 ## 8 1 0 2 1 2 1 3 0 118 0 ## 9 1 0 0 1 3 3 0 4 5 150 sum(diag(confusion_prueba))/sum(confusion_prueba) ## [1] 0.8719482 round(prop.table(confusion_prueba, 2),2) ## ## 0 1 2 3 4 5 6 7 8 9 ## 0 0.93 0.00 0.02 0.00 0.02 0.04 0.02 0.00 0.02 0.00 ## 1 0.00 0.95 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.02 ## 2 0.00 0.00 0.86 0.02 0.04 0.00 0.03 0.01 0.02 0.01 ## 3 0.01 0.01 0.04 0.87 0.00 0.11 0.00 0.02 0.07 0.00 ## 4 0.01 0.02 0.04 0.01 0.88 0.01 0.01 0.04 0.05 0.09 ## 5 0.03 0.00 0.03 0.08 0.01 0.81 0.08 0.01 0.08 0.01 ## 6 0.01 0.00 0.01 0.00 0.01 0.00 0.84 0.00 0.01 0.00 ## 7 0.00 0.00 0.01 0.01 0.01 0.01 0.00 0.88 0.01 0.03 ## 8 0.00 0.00 0.01 0.01 0.01 0.01 0.02 0.00 0.71 0.00 ## 9 0.00 0.00 0.00 0.01 0.02 0.02 0.00 0.03 0.03 0.85 El resultado no es muy bueno. Veremos más adelante mejores métodos para este problema. ¿Podemos interpretar el modelo? Una idea es tomar los coeficientes y graficarlos según la estructura de las imágenes: coefs &lt;- coef(mod_mult) coefs_reng &lt;- coefs[1, , drop =FALSE] coefs &lt;- rbind(coefs_reng, coefs) coefs[1 , ] &lt;- 0 dim(coefs) ## [1] 10 257 beta_df &lt;- coefs[,-1] %&gt;% as.data.frame %&gt;% mutate(digito = 0:(nrow(coefs)-1)) %&gt;% gather(pixel, valor, contains(&#39;pixel&#39;)) %&gt;% separate(pixel, into = c(&#39;str&#39;,&#39;pixel_no&#39;), sep=&#39;_&#39;) %&gt;% mutate(x = (as.integer(pixel_no)-1) %% 16, y = -((as.integer(pixel_no)-1) %/% 16)) head(beta_df) ## digito str pixel_no valor x y ## 1 0 pixel 1 0.000000000 0 0 ## 2 1 pixel 1 0.621681333 0 0 ## 3 2 pixel 1 -0.005914605 0 0 ## 4 3 pixel 1 0.044257959 0 0 ## 5 4 pixel 1 0.190966643 0 0 ## 6 5 pixel 1 -0.010655932 0 0 Podemos cruzar la tabla con sí misma para hacer comparaciones de cómo discrimina el modelo entre cada par de dígitos: tab_coef &lt;- beta_df %&gt;% select(digito, x, y, valor) tab_coef_1 &lt;- tab_coef names(tab_coef_1) &lt;- c(&#39;digito_1&#39;,&#39;x&#39;,&#39;y&#39;,&#39;valor_1&#39;) tab_cruzada &lt;- full_join(tab_coef_1, tab_coef) %&gt;% mutate(dif = valor_1 - valor) ## Joining, by = c(&quot;x&quot;, &quot;y&quot;) tab_cruzada &lt;- tab_cruzada %&gt;% group_by(digito, digito_1) %&gt;% mutate(dif_s = (dif - mean(dif))/sd(dif)) %&gt;% mutate(dif_p = pmin(pmax(dif_s, -2), 2)) ggplot(tab_cruzada, aes(x=x, y=y)) + geom_tile(aes(fill = dif_p)) + facet_grid(digito_1~digito) + scale_fill_distiller(palette = &quot;Spectral&quot;) Discusión Nótese que no corrimos el modelo hasta convergencia. Vamos a hacerlo ahora: mod_mult &lt;- multinom(digito ~ ., data = digitos_entrena, MaxNWt=100000, maxit = 500) ## # weights: 2580 (2313 variable) ## initial value 16788.147913 ## iter 10 value 2598.959017 ## iter 20 value 1494.978090 ## iter 30 value 903.291402 ## iter 40 value 443.785686 ## iter 50 value 260.626756 ## iter 60 value 190.835491 ## iter 70 value 160.773160 ## iter 80 value 114.048146 ## iter 90 value 88.746976 ## iter 100 value 76.302570 ## iter 110 value 63.400188 ## iter 120 value 54.375215 ## iter 130 value 46.291174 ## iter 140 value 38.303470 ## iter 150 value 28.822810 ## iter 160 value 17.888648 ## iter 170 value 9.531256 ## iter 180 value 2.985614 ## iter 190 value 0.714996 ## iter 200 value 0.209654 ## iter 210 value 0.066710 ## iter 220 value 0.030412 ## iter 230 value 0.014036 ## iter 240 value 0.006702 ## iter 250 value 0.004146 ## iter 260 value 0.001844 ## iter 270 value 0.001128 ## iter 280 value 0.000744 ## iter 290 value 0.000462 ## iter 300 value 0.000308 ## iter 310 value 0.000265 ## iter 320 value 0.000231 ## final value 0.000076 ## converged confusion_prueba &lt;- table(predict(mod_mult, newdata = digitos_prueba), digitos_prueba$digito) confusion_prueba ## ## 0 1 2 3 4 5 6 7 8 9 ## 0 332 0 6 2 4 2 1 2 7 2 ## 1 0 242 1 3 3 4 1 2 0 2 ## 2 2 2 148 5 5 0 4 3 3 0 ## 3 4 1 9 128 4 10 0 3 2 4 ## 4 3 5 8 0 149 8 6 7 5 2 ## 5 0 1 3 11 5 116 8 0 10 1 ## 6 5 7 4 3 10 4 144 0 4 1 ## 7 2 1 3 1 4 1 1 125 2 4 ## 8 6 3 14 7 6 10 4 0 132 3 ## 9 5 2 2 6 10 5 1 5 1 158 sum(diag(confusion_prueba))/sum(confusion_prueba) ## [1] 0.8340807 round(prop.table(confusion_prueba, 2),2) ## ## 0 1 2 3 4 5 6 7 8 9 ## 0 0.92 0.00 0.03 0.01 0.02 0.01 0.01 0.01 0.04 0.01 ## 1 0.00 0.92 0.01 0.02 0.02 0.02 0.01 0.01 0.00 0.01 ## 2 0.01 0.01 0.75 0.03 0.02 0.00 0.02 0.02 0.02 0.00 ## 3 0.01 0.00 0.05 0.77 0.02 0.06 0.00 0.02 0.01 0.02 ## 4 0.01 0.02 0.04 0.00 0.74 0.05 0.04 0.05 0.03 0.01 ## 5 0.00 0.00 0.02 0.07 0.02 0.72 0.05 0.00 0.06 0.01 ## 6 0.01 0.03 0.02 0.02 0.05 0.02 0.85 0.00 0.02 0.01 ## 7 0.01 0.00 0.02 0.01 0.02 0.01 0.01 0.85 0.01 0.02 ## 8 0.02 0.01 0.07 0.04 0.03 0.06 0.02 0.00 0.80 0.02 ## 9 0.01 0.01 0.01 0.04 0.05 0.03 0.01 0.03 0.01 0.89 Y nota que el error es más grande que cuando nos detuvimos antes. Discute en clase: Grafica los coeficientes para este segundo modelo ¿En cuál de los dos modelos es más fácil interpretar los coeficientes? ¿En cuál es menor el error? ¿Cuál crees que es el problema de este segundo modelo comparado con el primero? ¿Por qué crees que sucede? ¿Cómo podríamos corregir este problema? 4.4 Descenso en gradiente para regresión multinomial logística Supondremos \\(K\\) clases, numeradas de \\(0,1,\\ldots, K-1\\). OJO: al aplicar este código debes ser cuidadoso con las etiquetas de clase. pred_multinom &lt;- function(x, beta){ p &lt;- ncol(x) K &lt;- length(beta)/(p+1) + 1 beta_mat &lt;- matrix(beta, K - 1, p + 1 , byrow = TRUE) u_beta &lt;- exp(as.matrix(cbind(1, x)) %*% t(beta_mat)) Z &lt;- 1 + apply(u_beta, 1, sum) p_beta &lt;- cbind(u_beta, 1)/Z as.matrix(p_beta) } devianza_calc &lt;- function(x, y){ dev_fun &lt;- function(beta){ p_beta &lt;- pred_multinom(x, beta) p &lt;- sapply(1:nrow(x), function(i) p_beta[i, y[i]+1]) -2*sum(log(p)) } dev_fun } grad_calc &lt;- function(x_ent, y_ent){ p &lt;- ncol(x_ent) K &lt;- length(unique(y_ent)) y_fact &lt;- factor(y_ent) # matriz de indicadoras de clase y_dummy &lt;- model.matrix(~-1 + y_fact) salida_grad &lt;- function(beta){ p_beta &lt;- pred_multinom(x_ent, beta) e_mat &lt;- (y_dummy - p_beta)[, -K] grad_out &lt;- -2*(t(cbind(1,x_ent)) %*% e_mat) as.numeric(grad_out) } salida_grad } descenso &lt;- function(n, z_0, eta, h_deriv, dev_fun){ z &lt;- matrix(0,n, length(z_0)) z[1, ] &lt;- z_0 for(i in 1:(n-1)){ z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ]) if(i %% 100 == 0){ print(paste0(i, &#39; Devianza: &#39;, dev_fun(z[i+1, ]))) } } z } x_ent &lt;- digitos_entrena %&gt;% select(contains(&#39;pixel&#39;)) %&gt;% as.matrix y_ent &lt;- digitos_entrena$digito x_ent_s &lt;- scale(x_ent) medias &lt;- attr(x_ent_s, &#39;scaled:center&#39;) sd &lt;- attr(x_ent_s, &#39;scaled:scale&#39;) x_pr &lt;- digitos_prueba %&gt;% select(contains(&#39;pixel&#39;)) %&gt;% as.matrix y_pr &lt;- digitos_prueba$digito # inicializamos coeficientes al azar beta &lt;- runif(257*9) dev_ent &lt;- devianza_calc(x_ent_s, y_ent) grad &lt;- grad_calc(x_ent_s, y_ent) dev_ent(beta) ## [1] 227854.4 Hacemos algunas revisiones del gradiente: beta_2 &lt;- beta epsilon &lt;- 0.00001 beta_2[1000] &lt;- beta[1000] + epsilon (dev_ent(beta_2) - dev_ent(beta))/epsilon ## [1] -725.2213 grad(beta)[1000] ## [1] -725.2234 Ya ahora podemos hacer descenso: iteraciones &lt;- descenso(2000, rep(0, 257*9), eta=0.001, h_deriv = grad, dev_fun = dev_ent) ## [1] &quot;100 Devianza: 817.809554010357&quot; ## [1] &quot;200 Devianza: 408.010947736697&quot; ## [1] &quot;300 Devianza: 289.951542494061&quot; ## [1] &quot;400 Devianza: 227.805737974779&quot; ## [1] &quot;500 Devianza: 190.43408903327&quot; ## [1] &quot;600 Devianza: 165.487702748531&quot; ## [1] &quot;700 Devianza: 147.301091651991&quot; ## [1] &quot;800 Devianza: 133.221066964653&quot; ## [1] &quot;900 Devianza: 121.903186824327&quot; ## [1] &quot;1000 Devianza: 112.560175747607&quot; ## [1] &quot;1100 Devianza: 104.688785448699&quot; ## [1] &quot;1200 Devianza: 97.9483674585563&quot; ## [1] &quot;1300 Devianza: 92.0984398108757&quot; ## [1] &quot;1400 Devianza: 86.9631199039947&quot; ## [1] &quot;1500 Devianza: 82.4103777725155&quot; ## [1] &quot;1600 Devianza: 78.3393471851082&quot; ## [1] &quot;1700 Devianza: 74.6718258066508&quot; ## [1] &quot;1800 Devianza: 71.3463032980959&quot; ## [1] &quot;1900 Devianza: 68.3137316150628&quot; x_pr_s &lt;- scale(x_pr, center = medias, scale = sd) probas &lt;- pred_multinom(x_pr_s, iteraciones[2000,]) clase &lt;- apply(probas, 1, which.max) table(clase - 1, y_pr ) ## y_pr ## 0 1 2 3 4 5 6 7 8 9 ## 0 347 0 4 1 3 3 1 1 7 0 ## 1 0 252 0 0 4 0 0 1 0 1 ## 2 2 1 168 4 7 1 7 2 5 0 ## 3 2 5 5 148 2 6 0 2 1 0 ## 4 4 0 5 1 168 2 2 5 2 3 ## 5 1 0 2 8 3 139 3 0 6 1 ## 6 0 3 2 1 3 2 156 0 2 0 ## 7 1 1 4 1 3 0 0 133 2 3 ## 8 1 1 8 1 3 5 1 0 136 3 ## 9 1 1 0 1 4 2 0 3 5 166 1 - mean(clase-1 != y_pr) ## [1] 0.9033383 Tarea 4 Ver tareas/tarea_4.Rmd. "],
["regularizacion.html", "Clase 5 Regularización 5.1 Sesgo y varianza de predictores 5.2 Regularización ridge 5.3 Entrenamiento, Validación y Prueba 5.4 Regularización lasso", " Clase 5 Regularización Los métodos para ajustar modelos lineales que vimos en secciones anteriores (mínimos cuadrados y minimización de devianza) 5.1 Sesgo y varianza de predictores Consideremos el problema de regresión, donde el proceso que genera los datos está dado por \\[Y = f(X) + \\epsilon\\] Consideremos que queremos hacer predicciones para una \\(X=x_0\\) particular, de modo que el error es \\[Y - \\hat{f}(x_0) = (f(x_0) - \\hat{f}(x_0)) + \\epsilon\\] Como discutimos antes, no podemos hacer nada por la variación de \\(\\epsilon\\) (a menos que incluyamos otros predictores \\(X\\) informativos, por ejemplo). La pregunta es entonces ¿por qué podría pasar que \\(\\hat{f}(x_0)\\) estuviera lejos de \\(f(x_0)\\)? Recordemos que \\(\\hat{f}(x_0)\\) depende de una muestra de entrenamiento \\({\\mathcal L}\\), de modo que: Puede ser que \\(\\hat{f}(x_0)\\) está consistentemente lejos de \\(f(x_0)\\), independientemente de cuál es la muestra de entrenamiento. Puede ser que \\(\\hat{f}(x_0)\\) varía mucho dependiendo de la muestra de entrenamiento, y en consecuencia es poco probable que \\(\\hat{f}(x_0)\\) esté cerca de \\(f(x_0)\\). Es posible demostrar que \\[E\\left ( (f(x_0)-\\hat{f}(x_0))^2 \\right) = (f(x_0) - E(\\hat{f}(x_0)))^2 + Var (\\hat{f}(x_0))\\] donde los valores esperados y varianza son sobre posibles muestras de entrenamiento. Al primer término le llamamos sesgo : Qué tan lejos en promedio están las estimaciones de nuestro modelo del verdadero valor, y al segundo término le llamamos varianza: qué tanto varían las estimaciones del modelo. Ambas pueden ser razones por las que obtengamos predicciones malas. Ejemplo Consideremos dos métodos: regresión lineal y regresión polinomial (pensemos que es un tipo de ajuste de curvas). Para ilustrar los conceptos de sesgo y varianza simularemos varios posibles muestras de entrenamiento: f &lt;- function(x){ sin(4 * x) } sim_data &lt;- function(n = 30){ x &lt;- runif(n, 0, 1) y &lt;- f(x) + rnorm(n, 0, 0.5) data_frame(x = x, y = y) } dat &lt;- sim_data(n = 100) ggplot(dat, aes(x = x, y = y)) + geom_point() set.seed(92142) sims &lt;- data_frame(rep = 1:10) sims &lt;- sims %&gt;% group_by(rep) %&gt;% mutate(data = list(data = sim_data())) %&gt;% unnest Regresión lineal en \\(x\\) nos da diferencias consistentes entre predicciones y observaciones (es un método que sufre de sesgo): ggplot(sims, aes(x=x, y=y)) + geom_point() + facet_wrap(~rep) + geom_smooth(formula = y~x, method =&#39;lm&#39;, colour = &#39;red&#39;, se = FALSE) + ylim(c(-3,3)) Mientras que regresión polinomial nos da diferencias variables y grandes entre predicciones y observaciones (es un método que sufre de varianza): ggplot(sims, aes(x=x, y=y)) + geom_point() + facet_wrap(~rep) + geom_smooth(formula = y~ poly(x, 9, raw = TRUE), method =&#39;lm&#39;, colour = &#39;red&#39;, se = FALSE) + ylim(c(-3,3)) Podemos ver todos los modelos una misma gráfica y apreciar mejor la variación entre ellos: ggplot(sims, aes(x = x, y = y, group = rep)) + geom_smooth(formula = y~ poly(x, 9, raw = TRUE), method =&#39;lm&#39;, colour = &#39;grey40&#39;, se = FALSE) + geom_smooth(formula = y~x, method =&#39;lm&#39;, colour = &#39;red&#39;, se = FALSE) En este ejemplo, ambos métodos se desempeñan mal, pero por razones distintas. El primer método sufre más de sesgo (subajuste): es un método rígido que no aprende de patrones en los datos. El segundo método sufre más de varianza (sobreajuste): es un método flexible que aprende ruido. Cada uno de estos problemas requiere soluciones diferentes. Típicamente, reducciones en sesgo producen incrementos potenciales de varianza, y reducciones en varianza tienden a producir incrementos potenciales de sesgo. En esta parte veremos métodos de regularización, que sirven para reducir la varianza. Esta reducción en varianza será exitosa cuando el costo en sesgo que paguemos sea menor que esta reducción. 5.1.1 Sesgo y varianza en modelos lineales Aunque típicamente pensamos que los modelos lineales son métodos simples, con estructura rígida, y que tienden a sufrir más por sesgo que por varianza (parte de la razón por la que existen métodos más flexibles como bosques aleatorios, redes nueronales, etc.), hay varias razones por las que los métodos lineales pueden sufrir de varianza alta: Cuando la muestra de entrenamiento es relativamente chica (\\(N\\) chica), la varianza puede ser alta. Cuando el número de entradas \\(p\\) es grande, podemos también sufrir de varianza grande (pues tenemos muchos parámetros para estimar). Cuando hay variables correlacionadas en las entradas la varianza también puede ser alta. En estos casos, conviene buscar maneras de reducir varianza - generalmente a costa de un incremento de sesgo. Ejemplo Consideramos regresión logística. En primer lugar, supondremos que tenemos un problema con \\(n=400\\) y \\(p=100\\), y tomamos como modelo para los datos (sin ordenada al origen): \\[p_1(x)=h\\left(\\sum_{j=1}^{100} \\beta_j x_j\\right ),\\] donde \\(h\\) es la función logística. Nótese que este es el verdadero modelo para los datos. Para producir datos de entrenamiento, primero generamos las betas fijas, y después, utilizando estas betas, generamos 400 casos de entrenamiento. Generamos las betas: h &lt;- function(x){ 1 / (1 + exp(-x))} set.seed(2805) beta &lt;- rnorm(100,0,0.1) names(beta) &lt;- paste0(&#39;V&#39;, 1:length(beta)) head(beta) ## V1 V2 V3 V4 V5 ## -0.119875530 0.034627590 -0.081818069 0.014920959 0.040160152 ## V6 ## 0.002043735 Con esta función simulamos datos de entrenamiento (400) y datos de prueba (5000). sim_datos &lt;- function(n, m, beta){ p &lt;- length(beta) #n = casos de entrenamiento, m= casos de prueba, p=num variables mat &lt;- matrix(rnorm((n+m)*p, 0, 0.5), n+m, p) + rnorm(n + m) prob &lt;- h(mat %*% beta) y &lt;- rbinom(n + m, 1, prob) dat &lt;- as.data.frame(mat) dat$y &lt;- y dat$entrena &lt;- FALSE dat$entrena[1:n] &lt;- TRUE dat } set.seed(9921) datos &lt;- sim_datos(n = 400, m = 2000, beta = beta) Y ahora ajustamos el modelo de regresión logística: mod_1 &lt;- glm(y ~ -1 + ., datos %&gt;% filter(entrena) %&gt;% select(-entrena), family = &#39;binomial&#39;) ¿Qué tan buenas fueron nuestras estimaciones? qplot(beta, mod_1$coefficients) + xlab(&#39;Coeficientes&#39;) + ylab(&#39;Coeficientes estimados&#39;) + geom_abline(intercept=0, slope =1) + xlim(c(-1.5,1.5))+ ylim(c(-1.5,1.5)) Y notamos que las estimaciones no son buenas. Podemos hacer otra simulación para confirmar que el problema es que las estimaciones son muy variables. Con otra muestra de entrenamiento, vemos que las estimaciones tienen varianza alta. datos_2 &lt;- sim_datos(n = 400, m = 10, beta = beta) mod_2 &lt;- glm(y ~ -1 + ., datos_2 %&gt;% filter(entrena) %&gt;% select(-entrena), family = &#39;binomial&#39;) qplot(mod_1$coefficients, mod_2$coefficients) + xlab(&#39;Coeficientes mod 1&#39;) + ylab(&#39;Coeficientes mod 2&#39;) + geom_abline(intercept=0, slope =1) + xlim(c(-1.5,1.5))+ ylim(c(-1.5,1.5)) Si repetimos varias veces: dat_sim &lt;- lapply(1:50, function(i){ salida &lt;- sim_datos(n=400, m=10, beta) mod &lt;- glm(y ~ -1 + ., salida %&gt;% filter(entrena) %&gt;% select(-entrena), family = &#39;binomial&#39;) data_frame(rep = i, vars = names(coef(mod)), coefs = coef(mod)) }) %&gt;% bind_rows head(dat_sim) ## # A tibble: 6 x 3 ## rep vars coefs ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 V1 -0.209 ## 2 1 V2 -0.0538 ## 3 1 V3 0.149 ## 4 1 V4 0.768 ## 5 1 V5 0.123 ## 6 1 V6 -0.257 Vemos que hay mucha variabilidad en la estimación de los coeficientes (en rojo están los verdaderos): dat_sim &lt;- dat_sim %&gt;% mutate(vars = reorder(vars, coefs, mean)) ggplot(dat_sim, aes(x=vars, y=coefs)) + geom_boxplot() + geom_line(data=data_frame(coefs=beta, vars=names(beta)), aes(y=beta, group=1), col=&#39;red&#39;,size=1.1) + coord_flip() En la práctica, nosotros tenemos una sola muestra de entrenamiento. Así que, con una muestra de tamaño \\(n=500\\) como en este ejemplo, obtendremos típicamente resultados no muy buenos. Estos coeficientes ruidosos afectan nuestras predicciones de manera negativa. Vemos ahora lo que pasa con nuestra \\(\\hat{p}_1(x)\\) estimadas, comparándolas con \\(p_1(x)\\), para la primera simulación: dat_e &lt;- datos %&gt;% filter(entrena) dat_p &lt;- datos %&gt;% filter(!entrena) x_e &lt;- dat_e %&gt;% select(-entrena, -y) %&gt;% as.matrix x_p &lt;- dat_p %&gt;% select(-entrena, -y) %&gt;% as.matrix p_entrena &lt;- data_frame(prob_hat_1 = (mod_1$fitted.values), prob_1 = as.numeric(h(x_e %*% beta)), clase = dat_e$y) p_prueba &lt;- data_frame(prob_hat_1 = as.numeric(h(x_p %*% (mod_1$coefficients))), prob_1 = as.numeric(h(x_p %*% beta)), clase = dat_p$y) Para los datos de entrenamiento: ggplot(p_entrena, aes(x=prob_hat_1, y=prob_1, colour=factor(clase))) + geom_point() + coord_flip() Notamos en esta gráfica: El ajuste parece discriminar bien entre las dos clases del conjunto de entrenamiento (cuando la probabilidad estimada es chica, observamos casi todos clase 0, y cuando la probabilidad estimada es grande, observamos casi todos clase 1). Sin embargo, vemos que las probabilidades estimadas tienden a ser extremas: muchas veces estimamos probabilidad cercana a 0 o 1, cuando la probabilidad real no es tan extrema (por ejemplo, está entre 0.25 y 0.75). Estos dos aspectos indican sobreajuste. Podemos verificar comparando con los resultados que obtenemos con la muestra de prueba. Si calculamos la matriz de confusión y sensibilidad y especifidad de entrenamiento: tab &lt;- table(p_entrena$prob_hat_1 &gt; 0.5, p_entrena$clase) tab ## ## 0 1 ## FALSE 159 34 ## TRUE 44 163 prop.table(tab, margin=2) ## ## 0 1 ## FALSE 0.7832512 0.1725888 ## TRUE 0.2167488 0.8274112 Pero con la muestra de prueba obtenemos tab &lt;- table(p_prueba$prob_hat_1 &gt; 0.5, p_prueba$clase) tab ## ## 0 1 ## FALSE 608 374 ## TRUE 396 622 prop.table(tab, margin=2) ## ## 0 1 ## FALSE 0.6055777 0.3755020 ## TRUE 0.3944223 0.6244980 Que es un desempeño pobre comparado con lo que la muestra de entrenamiento podría indicar. Finalmente, podemos también repetir la gráfica de arriba con los datos de prueba: ggplot(p_prueba, aes(x=prob_hat_1)) + geom_point(aes(y=prob_1, colour=factor(clase))) + coord_flip() Si la estimación fuera perfecta, esta gráfica sería una diagonal. Vemos entonces que Cometemos errores grandes en la estimación de probabilidades. El desempeño predictivo del modelo es pobre, aún cuando nuestro modelo puede discriminar razonablemente bien las dos clases en el conjunto de entrenamiento. El problema no es que nuestro modelo no sea apropiado (logístico), pues ese es el modelo real. El problema es el sobreajuste asociado a la variabilidad de los coeficientes que notamos arriba. 5.1.2 Reduciendo varianza de los coeficientes Como el problema es la varianza, podemos atacar este problema poniendo restricciones a los coeficientes, de manera que caigan en rangos más aceptables. Una manera de hacer esto es sustituir el problema de minimización de regresión logística, que es minimizar la devianza: \\[\\min_{\\beta} D(\\beta)\\] con un problema penalizado \\[\\min_{\\beta} D(\\beta) + \\lambda\\sum_{i=1}^p \\beta_j^2\\] escogiendo un valor apropiado de \\(\\lambda\\). Si escogemos un valor relativamente grande de \\(\\lambda\\), entonces terminaremos con una solución donde los coeficientes \\(\\beta_j\\) no pueden alejarse mucho de 0, y esto previene parte del sobreajuste que observamos en nuestro primer ajuste. Otra manera de decir esto es: intentamos minimizar la devianza, pero no permitimos que los coeficientes se alejen demasiado de cero. También es posible poner restricciones sobre el tamaño de \\(\\sum_{i=1}^p \\beta_j^2\\), lo cual es equivalente al problema de penalización. En este caso obtenemos (veremos más del paquete glmnet): library(glmnet) mod_restringido &lt;- glmnet(x = x_e, y = dat_e$y, alpha = 0, family=&#39;binomial&#39;, intercept = F, lambda = 0.1) beta_penalizado &lt;- coef(mod_restringido)[-1] # quitar intercept Y podemos ver que el tamaño de los coeficientes se redujo considerablemente: sum(beta_penalizado^2) ## [1] 0.4837593 sum(coef(mod_1)^2) ## [1] 18.2092 Los nuevos coeficientes estimados tienen menor variación: qplot(beta, beta_penalizado) + xlab(&#39;Coeficientes&#39;) + ylab(&#39;Coeficientes estimados&#39;) + geom_abline(intercept=0, slope =1) + xlim(c(-0.5,0.5))+ ylim(c(-0.5,0.5)) Y las probabilidades estimadas son más razonables: p_entrena$prob_hat_pen &lt;- h(x_e %*% as.numeric(beta_penalizado)) p_prueba$prob_hat_pen &lt;- h(x_p %*% as.numeric(beta_penalizado)) ggplot(p_entrena, aes(x=prob_1, y=prob_hat_pen, colour=factor(clase))) + geom_point() El desempeño es considerablemente mejor: tab &lt;- table(p_prueba$prob_hat_pen &gt; 0.5, p_prueba$clase) prop.table(tab, 2) ## ## 0 1 ## FALSE 0.6603586 0.2851406 ## TRUE 0.3396414 0.7148594 Y finalmente, comparamos las curvas ROC de prueba para los dos modelos, el penalizado y el no penalizado. El modelo penalizado es considerablemente mejor: library(ROCR) pred &lt;- prediction(predictions = p_prueba$prob_hat_1, labels = p_prueba$clase) perf &lt;- performance(pred, measure = &quot;sens&quot;, x.measure = &quot;fpr&quot;) plot(perf) pred_r &lt;- prediction(predictions = p_prueba$prob_hat_pen, labels = p_prueba$clase) perf_r &lt;- performance(pred_r, measure = &quot;sens&quot;, x.measure = &quot;fpr&quot;) plot(perf_r, add =T, col =&#39;red&#39;) abline(a=0, b=1, col =&#39;gray&#39;) Observación: Sin embargo, vemos que en la muestra de entrenamiento se desempeña mejor el modelo sin penalización, como es de esperarse (el mínimo irrestricto es más bajo que el mínimo del problema con restricción). library(ROCR) pred &lt;- prediction(predictions = p_entrena$prob_hat_1, labels = p_entrena$clase) perf &lt;- performance(pred, measure = &quot;sens&quot;, x.measure = &quot;fpr&quot;) plot(perf) pred_r &lt;- prediction(predictions = p_entrena$prob_hat_pen, labels = p_entrena$clase) perf_r &lt;- performance(pred_r, measure = &quot;sens&quot;, x.measure = &quot;fpr&quot;) plot(perf_r, add =T, col =&#39;red&#39;) abline(a=0, b=1, col =&#39;gray&#39;) 5.2 Regularización ridge Arriba vimos un ejemplo de regresión penalizada tipo ridge. Recordemos que para regresión lineal, buscábamos minimizar la cantidad \\[D(\\beta)=\\frac{1}{n}\\sum_{i=1}^n (y_i -\\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij})^2\\] y en regresión logística, \\[D(\\beta)=-\\frac{2}{n}\\sum_{i=1}^n y_i \\log(h(\\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij})) + (1-y_i) \\log(1 - h(\\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij})) ,\\] donde los denotamos de la misma forma para unificar notación. En regresión ridge (lineal/logística), para \\(\\lambda&gt;0\\) fija minimizamos \\[D_{\\lambda}^{ridge} (\\beta)=D(\\beta) + \\lambda\\sum_{i=1}^p \\beta_j^2,\\] donde suponemos que las entradas están estandarizadas (centradas y escaladas por la desviación estándar). Observaciones La idea de regresión penalizada consiste en estabilizar la estimación de los coeficientes, especialmente en casos donde tenemos muchas variables en relación a los casos de entrenamiento. La penalización no permite que varíen tan fuertemente los coeficientes. Cuando \\(\\lambda\\) es mas grande, los coeficientes se encogen más fuertemente hacia cero con respecto al problema no regularizado. En este caso, estamos reduciendo la varianza pero potencialmente incrementando el sesgo. Cuando \\(\\lambda\\) es mas chico, los coeficientes se encogen menos fuertemente hacia cero, y quedan más cercanos a los coeficientes de mínimos cuadrados/máxima verosimilitud. En este caso, estamos reduciendo el sesgo pero incrementando la varianza. Nótese que no penalizamos \\(\\beta_0\\). Es posible hacerlo, pero típicamente no lo hacemos. En regresión lineal, de esta forma garantizamos que la predicción \\(\\hat{y}\\), cuando todas las variables \\(x_j\\) toman su valor en la media, es el promedio de las \\(y_i\\)’s de entrenamiento. Igualmente en regresión logística, la probabilidad ajustada cuando las entradas toman su valor en la media es igual a \\(h(\\beta_0)\\). Que las variables estén estandarizadas es importante para que tenga sentido la penalización. Si las variables \\(x_j\\) están en distintas escalas (por ejemplo pesos y dólares), entonces también los coeficientes \\(\\beta_j\\) están en distintas escalas, y una penalización fija no afecta de la misma forma a cada coeficiente. Resolver este problema penalizado por descenso en gradiente no tienen dificultad, pues: \\[\\frac{\\partial D_{\\lambda}^{ridge} (\\beta)}{\\partial\\beta_j} = \\frac{\\partial D(\\beta)}{\\beta_j} + 2\\lambda\\beta_j\\] para \\(j=1,\\ldots, p\\), y \\[\\frac{\\partial D_{\\lambda}^{ridge} (\\beta)}{\\partial\\beta_0} = \\frac{\\partial D(\\beta)}{\\beta_0}.\\] De forma que sólo hay que hacer una modificación mínima al algoritmo de descenso en gradiente para el caso no regularizado. 5.2.1 Selección de coeficiente de regularización Seleccionamos \\(\\lambda\\) para minimizar el error de predicción, es decir, para mejorar nuestro modelo ajustado en cuanto a sus predicciones. No tiene sentido intentar escoger \\(\\lambda&gt;0\\) usando el error de entrenamiento. La razón es que siempre que aumentamos \\(\\lambda\\), obtenemos un valor mayor de la suma de cuadrados / devianza del modelo, pues \\(\\lambda\\) más grande implica que pesa menos la minimización de la suma de cuadrados /devianza en el problema de la minimización. En otras palabras, los coeficientes tienen una penalización más fuerte, de modo que el mínimo que se alcanza es mayor en términos de devianza. Intentamos escoger \\(\\lambda\\) de forma que se minimice el error de predicción, o el error de prueba (que estima el error de predicción). Ejemplo (simulación) Regresamos a nuestro problema original simulado de clasificación. La función glmnet se encarga de estandarizar variables y escoger un rango adecuado de penalizaciones \\(\\lambda\\). La función glmnet ajusta varios modelos (parámetro nlambda) para un rango amplio de penalizaciones \\(\\lambda\\): library(glmnet) mod_ridge &lt;- glmnet(x = x_e, y = dat_e$y, alpha = 0, #ridge family = &#39;binomial&#39;, intercept = F, nlambda=50) #normalmente ponemos intercept = T dim(coef(mod_ridge)) ## [1] 101 50 En primer lugar, observamos cómo se encogen los coeficientes para distintos valores de \\(\\lambda\\): plot(mod_ridge, xvar=&#39;lambda&#39;) Para escoger el valor adecuado de \\(\\lambda\\), calculamos la devianza bajo la muestra de prueba: devianza &lt;- function(p, y){ -2*mean(y * log(p) + (1-y) * log(1 - p)) } # predict en glmnet produce probabilidades para los 50 modelos preds_ridge &lt;- predict(mod_ridge, newx = x_p, type = &#39;response&#39;) %&gt;% data.frame %&gt;% mutate(id = 1:nrow(x_p)) %&gt;% gather(modelo, prob, -id) %&gt;% left_join(dat_p %&gt;% mutate(id=1:nrow(dat_p)) %&gt;% select(id, y)) ## Joining, by = &quot;id&quot; head(preds_ridge) ## id modelo prob y ## 1 1 s0 0.5 1 ## 2 2 s0 0.5 1 ## 3 3 s0 0.5 1 ## 4 4 s0 0.5 1 ## 5 5 s0 0.5 1 ## 6 6 s0 0.5 0 tail(preds_ridge) ## id modelo prob y ## 99995 1995 s49 0.50969336 1 ## 99996 1996 s49 0.46159912 1 ## 99997 1997 s49 0.40584246 1 ## 99998 1998 s49 0.01436745 0 ## 99999 1999 s49 0.45568264 1 ## 100000 2000 s49 0.73158603 1 df_lambdas &lt;- data_frame(modelo = attr(mod_ridge$a0, &#39;names&#39;), lambda = mod_ridge$lambda) devianzas_prueba &lt;- preds_ridge %&gt;% group_by(modelo) %&gt;% summarise( devianza = devianza(prob, y)) %&gt;% left_join(df_lambdas) ## Joining, by = &quot;modelo&quot; ggplot(devianzas_prueba, aes(x = lambda, y= devianza)) + scale_x_log10(breaks = round(2^seq(-5,5,1),2)) + geom_point() Buscamos entonces minimizar la devianza (evaluada en la muestra de prueba), que corresponde a tomar un valor de \\(\\lambda\\) alrededor de 0.5, por ejemplo Discusión: ¿por qué la devianza de prueba tiene esta forma, que es típica para problemas de regularización? El modelo final queda como sigue: df_lambdas ## # A tibble: 50 x 2 ## modelo lambda ## &lt;chr&gt; &lt;dbl&gt; ## 1 s0 226. ## 2 s1 187. ## 3 s2 155. ## 4 s3 129. ## 5 s4 107. ## 6 s5 88.3 ## 7 s6 73.1 ## 8 s7 60.6 ## 9 s8 50.2 ## 10 s9 41.6 ## # ... with 40 more rows coefs_selec &lt;- coef(mod_ridge)[-1, &#39;s35&#39;] pred_prueba_final &lt;- h(x_p %*% coefs_selec) tab_confusion &lt;- table(pred_prueba_final &gt; 0.5, dat_p$y) tab_confusion ## ## 0 1 ## FALSE 657 293 ## TRUE 347 703 prop.table(tab_confusion, margin=2) ## ## 0 1 ## FALSE 0.6543825 0.2941767 ## TRUE 0.3456175 0.7058233 Ejemplo: variables correlacionadas Ridge es efectivo para reducir varianza inducida por variables correlacionadas. Consideramos el siguiente ejemplo donde queremos predecir el porcentaje de grasa corporal a partir de varias medidas del cuerpo (estas medidas están claramente correlacionadas): library(readr) dat_grasa &lt;- read_csv(file = &#39;datos/bodyfat.csv&#39;) head(dat_grasa) ## # A tibble: 6 x 14 ## grasacorp edad peso estatura cuello pecho abdomen cadera muslo rodilla ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12.3 23 154. 67.8 36.2 93.1 85.2 94.5 59 37.3 ## 2 6.1 22 173. 72.2 38.5 93.6 83 98.7 58.7 37.3 ## 3 25.3 22 154 66.2 34 95.8 87.9 99.2 59.6 38.9 ## 4 10.4 26 185. 72.2 37.4 102. 86.4 101. 60.1 37.3 ## 5 28.7 24 184. 71.2 34.4 97.3 100 102. 63.2 42.2 ## 6 20.9 24 210. 74.8 39 104. 94.4 108. 66 42 ## # ... with 4 more variables: tobillo &lt;dbl&gt;, biceps &lt;dbl&gt;, antebrazo &lt;dbl&gt;, ## # muñeca &lt;dbl&gt; nrow(dat_grasa) ## [1] 252 set.seed(127) dat_grasa$unif &lt;- runif(nrow(dat_grasa), 0, 1) dat_grasa &lt;- arrange(dat_grasa, unif) dat_grasa$id &lt;- 1:nrow(dat_grasa) bfat_e &lt;- dat_grasa[1:100,] bfat_p &lt;- dat_grasa[101:252,] xbf_e &lt;- bfat_e %&gt;% select(estatura, peso, abdomen, muslo, biceps) %&gt;% as.matrix cor(xbf_e) ## estatura peso abdomen muslo biceps ## estatura 1.00000000 0.2534694 0.0928379 0.04835578 0.1857616 ## peso 0.25346939 1.0000000 0.9059227 0.86412005 0.8273691 ## abdomen 0.09283790 0.9059227 1.0000000 0.78986726 0.7308348 ## muslo 0.04835578 0.8641200 0.7898673 1.00000000 0.7899550 ## biceps 0.18576161 0.8273691 0.7308348 0.78995504 1.0000000 Ahora ajustamos varios modelos penalizamos, y observamos qué pasa con los coeficientes conforme aumentamos la penalización: ridge_bodyfat &lt;- glmnet(x = scale(xbf_e), y = bfat_e$grasacorp, alpha=0, lambda = exp(seq(-5, 5, 0.25))) plot(ridge_bodyfat, xvar = &#39;lambda&#39;, label=TRUE) Donde notamos que las variables con correlaciones altas se “encogen” juntas hacia valores similares conforme aumentamos la constante de penalización \\(\\lambda\\). Nótese que para regularización muy baja peso y abdomen por ejemplo, tienen signos opuestos y valores altos: esto es posible pues tienen correlación alta, de modo que la función de predicción está pobremente determinada: hay un espacio grande de pares de parámetros que dan predicciones similares, y esto resulta en coeficientes con varianza alta y predicciones inestables y ruidosas. Nótese, adicionalmente, que los coeficientes parecen tener más sentido en relación al problema con regularización. Regularización, en este tipo de problemas, es una de las componentes necesarias (pero no suficiente) para ir hacia interpretación del fenómeno que nos interesa. 5.3 Entrenamiento, Validación y Prueba El enfoque que vimos arriba, en donde dividemos la muestra en dos partes al azar, es la manera más fácil de seleccionar modelos. En general, el proceso es el siguiente: Una parte con los que ajustamos todos los modelos que nos interesa. Esta es la muestra de entrenamiento Una parte como muestra de prueba, con el que evaluamos el desempeño de cada modelo ajustado en la parte anterior. En este contexto, a esta muestra se le llama muestra de validación}. Posiblemente una muestra adicional independiente, que llamamos muestra de prueba, con la que hacemos una evaluación final del modelo seleccionado arriba. Es una buena idea apartar esta muestra si el proceso de validación incluye muchos métodos con varios parámetros afinados (como la \\(\\lambda\\) de regresión ridge). knitr::include_graphics(&quot;./figuras/div_muestra.png&quot;) Cuando tenemos datos abundantes, este enfoque es el usual. Por ejemplo, podemos dividir la muestra en 50-25-25 por ciento. Ajustamos modelos con el primer 50%, evaluamos y seleccionamos con el segundo 25% y finalmente, si es necesario, evaluamos el modelo final seleccionado con la muestra final de 25%. La razón de este proceso es que así podemos ir y venir entre entrenamiento y validación, buscando mejores enfoques y modelos, y no ponemos en riesgo la estimación final del error. (Pregunta: ¿por qué probar agresivamente buscando mejorar el error de validación podría ponder en riesgo la estimación final del error del modelo seleccionado? ) 5.3.1 Validación cruzada En muchos casos, no queremos apartar una muestra de validación para seleccionar modelos, pues no tenemos muchos datos (al dividir la muestra obtendríamos un modelo relativamente malo en relación al que resulta de todos los datos). Un criterio para seleccionar la regularización adecuada es el de **validación cruzada*, que es un método computacional para producir una estimación interna (usando sólo muestra de entrenamiento) del error de predicción. En validación cruzada (con \\(k\\) vueltas), construimos al azar una partición, con tamaños similares, de la muestra de entrenamiento \\({\\mathcal L}=\\{ (x_i,y_i)\\}_{i=1}^n\\): \\[ {\\mathcal L}={\\mathcal L}_1\\cup {\\mathcal L}_2\\cup\\cdots\\cup {\\mathcal L}_k.\\] knitr::include_graphics(&quot;./figuras/div_muestra_cv.png&quot;) Construimos \\(k\\) modelos distintos, digamos \\(\\hat{f}_j\\), usando solamente la muestra \\({\\mathcal L}-{\\mathcal L}_j\\), para \\(j=1,2,\\ldots, k\\). Cada uno de estos modelos lo evaluamos usando la parte que no usamos para entrenarlo, \\({\\mathcal L}_j\\), para obtener una estimación honesta del error del modelo \\(\\hat{f}_k\\), a la que denotamos por \\(\\hat{e}_j\\). Notemos entonces que tenemos \\(k\\) estimaciones del error \\(\\hat{e}_1,\\ldots, \\hat{e}_k\\), una para cada uno de los modelos que construimos. La idea ahora es que Cada uno de los modelos \\(\\hat{f}_j\\) es similar al modelo ajustado con toda la muestra \\(\\hat{f}\\), de forma que podemos pensar que cada una de las estimaciones \\(\\hat{e}_j\\) es un estimador del error de \\(\\hat{f}\\). Dado el punto anterior, podemos construir una mejor estimación promediando las \\(k\\) estimaciones anteriores, para obtener: \\[\\widehat{cv} = \\frac{1}{k} \\sum_{j=1}^k \\hat{e}_j.\\] ¿Cómo escoger \\(k\\)? Usualmente se usan \\(k=5,10,20\\), y \\(k=10\\) es el más popular. La razón es que cuando \\(k\\) es muy chico, tendemos a evaluar modelos construidos con pocos datos (comparado al modelo con todos los datos de entrenamiento). Por otra parte, cuando \\(k\\) es grande el método puede ser muy costoso (por ejemplo, si \\(k=N\\), hay que entrenar un modelo para cada dato de entrada). Por ejemplo, el paquete glmnet incluye la función cv.glmnet, que hace los \\(k\\) ajustes para cada una de las lambdas: library(glmnet) set.seed(291) cv_mod_ridge &lt;- cv.glmnet(x = x_e, y=dat_e$y, alpha = 0, family=&#39;binomial&#39;, intercept = F, nfolds = 10, nlambda=50) plot(cv_mod_ridge) cv_mod_ridge$lambda.min ## [1] 0.2155714 cv_mod_ridge$lambda.1se ## [1] 7.666755 Nótese que la estimación del error de predicción por validación cruzada incluye un error de estimación (intervalos). Esto nos da dos opciones para escoger la lambda final: Escoger la que de el mínimo valor de error por validación cruzada Escoger la lambda más grande que no esté a más de 1 error estándar del mínimo. En la gráfica anterior se muestran las dos posibilidades. La razón del segundo criterio es tomar el modelo más simple que tenga error consistente con el mejor modelo. 5.3.2 ¿Cómo se desempeña validación cruzada como estimación del error? cross_valid &lt;- data_frame(devianza_cv = cv_mod_ridge$cvm, modelo = attr(cv_mod_ridge$glmnet.fit$a0, &#39;names&#39;)[1:49]) devs &lt;- devianzas_prueba %&gt;% left_join(cross_valid) %&gt;% rename(devianza_prueba = devianza) %&gt;% gather(tipo, devianza, devianza_prueba, devianza_cv) ## Joining, by = &quot;modelo&quot; ggplot(devs, aes(x=log(lambda), y=devianza, colour=tipo)) + geom_point() ## Warning: Removed 1 rows containing missing values (geom_point). Vemos que la estimación en algunos casos no es tan buena, aún cuando todos los datos fueron usados. Pero el mínimo se encuentra en lugares muy similares. La razón es que validación cruzada en realidad considera perturbaciones del conjunto de entrenamiento, de forma que lo que intenta evaluar es el error producido, para cada lambda, sobre distintas muestras de entrenamiento. En realidad nosotros queremos evaluar el error de predicción del modelo que ajustamos. Validación cruzada es más un estimador del error esperado de predicción sobre los modelos que ajustaríamos con distintas muestras de entrenamiento. El resultado es que: Usamos validación cruzada para escoger la complejidad adecuada de la familia de modelos que consideramos. Como estimación del error de predicción del modelo que ajustamos, validación cruzada es más seguro que usar el error de entrenamiento, que muchas veces puede estar fuertemente sesgado hacia abajo. Sin embargo, lo mejor en este caso es utilizar una muestra de prueba. Ejercicio Consideremos el ejemplo de reconocimiento de dígitos y regresión logística multinomial. library(readr) digitos_entrena &lt;- read_csv(&#39;datos/zip-train.csv&#39;) digitos_prueba &lt;- read_csv(&#39;datos/zip-test.csv&#39;) names(digitos_entrena)[1] &lt;- &#39;digito&#39; names(digitos_entrena)[2:257] &lt;- paste0(&#39;pixel_&#39;, 1:256) names(digitos_prueba)[1] &lt;- &#39;digito&#39; names(digitos_prueba)[2:257] &lt;- paste0(&#39;pixel_&#39;, 1:256) Vamos a correr modelos con varias lambda, y estimar su error con validación cruzada: set.seed(2912) if(TRUE){ digitos_entrena_s &lt;- sample_n(digitos_entrena, size = 2000) } else { digitos_entrena_s &lt;- digitos_entrena } x_e &lt;- digitos_entrena_s %&gt;% select(-digito) %&gt;% as.matrix x_p &lt;- digitos_prueba %&gt;% select(-digito) %&gt;% as.matrix library(doMC) ## Loading required package: iterators ## Loading required package: parallel registerDoMC(cores=5) digitos_cv &lt;- cv.glmnet(x = x_e, y = factor(digitos_entrena_s$digito), family = &#39;multinomial&#39;, alpha = 0, parallel = TRUE, nfolds = 10, lambda = exp(seq(-12, 2, 1))) plot(digitos_cv) Ahora hacemos predicciones para el conjunto de prueba, usando la lambda que nos dio el menor error de validación cruzada: preds_prueba &lt;- predict(digitos_cv, newx = x_p, s = &#39;lambda.min&#39;)[,,1] # solo un grupo de coeficientes dim(preds_prueba) ## [1] 2007 10 Y evaluamos la tasa de clasificación incorrecta: preds_clase &lt;- apply(preds_prueba, 1, which.max) table(preds_clase, digitos_prueba$digito) ## ## preds_clase 0 1 2 3 4 5 6 7 8 9 ## 1 348 0 4 3 1 6 3 1 5 0 ## 2 0 252 0 0 1 0 0 0 0 3 ## 3 2 1 167 5 6 1 3 0 8 1 ## 4 2 2 8 140 0 11 0 1 6 0 ## 5 3 5 8 1 172 3 3 9 2 6 ## 6 0 0 0 12 1 126 3 2 8 1 ## 7 2 2 2 0 8 2 158 0 0 0 ## 8 0 0 1 1 1 3 0 131 0 2 ## 9 1 1 8 2 3 6 0 0 135 1 ## 10 1 1 0 2 7 2 0 3 2 163 mean(preds_clase -1 != digitos_prueba$digito) ## [1] 0.1071251 Este modelo mejora considerablemente al modelo sin regularización. Observación: Cuando vimos regresión multinomial, la última clase es uno menos la suma del resto de probabilidades de clase (\\((K-1)(p+1)\\) parámetros). La salida de glmnet, sin embargo, tiene coeficientes para todas las clases (\\(K(p+1)\\) parámetros). ¿Por qué en regresión ridge no está sobreparametrizado el modelo? 5.4 Regularización lasso Otra forma de regularización es el lasso, que en lugar de penalizar con la suma de cuadrados en los coeficientes, penaliza por la suma de su valor absoluto. En regresión lasso (lineal/logística), para \\(\\lambda&gt;0\\) fija minimizamos \\[D_{\\lambda}^2 (\\beta)=D(\\beta) + \\lambda\\sum_{i=1}^p |\\beta_j|\\], donde suponemos que las entradas están estandarizadas (centradas y escaladas por la desviación estándar). El problema de minimización de ridge y de lasso se pueden reescribir como problemas de restricción: En regresión lasso (lineal/logística), para \\(s&gt;0\\) fija minimizamos \\[D(\\beta), \\] sujeto a \\[\\sum_{i=1}^p |\\beta_j|&lt; s\\] donde suponemos que las entradas están estandarizadas (centradas y escaladas por la desviación estándar). En regresión ridge (lineal/logística), para \\(t&gt;0\\) fija minimizamos \\[D(\\beta), \\] sujeto a \\[\\sum_{i=1}^p \\beta_j^2 &lt; t\\] donde suponemos que las entradas están estandarizadas (centradas y escaladas por la desviación estándar). \\(s\\) y \\(t\\) chicas corresponden a valores de penalización \\(\\lambda\\) grandes. En un principio, puede parecer que ridge y lasso deben dar resultados muy similares, pues en ambos casos penalizamos por el tamaño de los coeficientes. Sin embargo, son distintos de una manera muy importante. En la siguiente gráfica representamos las curvas de nivel de \\(D(\\beta)\\). Recordemos que en mínimos cuadrados o regresión logística intentamos minimizar esta cantidad sin restricciones, y este mínimo se encuentra en el centro de estas curvas de nivel. Para el problema restringido, buscamos más bien la curva de nivel más baja que intersecta la restricción: knitr::include_graphics(&#39;./figuras/ridge_lasso.png&#39;) Y obsérvese ahora que la solución de lasso puede hacer algunos coeficientes igual a 0. Es decir, En regresión ridge, los coeficientes se encogen gradualmente desde la solución no restringida hasta el origen. Ridge es un método de encogimiento de coeficientes. En regresión lasso, los coeficientes se encogen gradualmente, pero también se excluyen variables del modelo. Por eso lasso es un método de encogimiento y selección de variables. Regresión ridge es especialmente útil cuando tenemos varias variables de entrada fuertemente correlacionadas. Regresión ridge intenta encoger juntos coeficientes de variables correlacionadas para reducir varianza en las predicciones. Lasso encoge igualmente coeficientes para reducir varianza, pero también comparte similitudes con regresión de mejor subconjunto, en donde para cada número de variables \\(l\\) buscamos escoger las \\(l\\) variables que den el mejor modelo. Sin embargo, el enfoque de lasso es más escalable y puede calcularse de manera más simple. Descenso en gradiente no es apropiado para regresión lasso (ver documentación de glmnet para ver cómo se hace en este paquete). El problema es que los coeficientes nunca se hacen exactamente cero, pues la restricción no es diferenciable en el origen (coeficientes igual a cero). Ejemplo Consideramos el ejemplo de bodyfat: library(readr) dat_grasa &lt;- read_csv(file = &#39;datos/bodyfat.csv&#39;) head(dat_grasa) ## # A tibble: 6 x 14 ## grasacorp edad peso estatura cuello pecho abdomen cadera muslo rodilla ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12.3 23 154. 67.8 36.2 93.1 85.2 94.5 59 37.3 ## 2 6.1 22 173. 72.2 38.5 93.6 83 98.7 58.7 37.3 ## 3 25.3 22 154 66.2 34 95.8 87.9 99.2 59.6 38.9 ## 4 10.4 26 185. 72.2 37.4 102. 86.4 101. 60.1 37.3 ## 5 28.7 24 184. 71.2 34.4 97.3 100 102. 63.2 42.2 ## 6 20.9 24 210. 74.8 39 104. 94.4 108. 66 42 ## # ... with 4 more variables: tobillo &lt;dbl&gt;, biceps &lt;dbl&gt;, antebrazo &lt;dbl&gt;, ## # muñeca &lt;dbl&gt; nrow(dat_grasa) ## [1] 252 set.seed(127) dat_grasa$unif &lt;- runif(nrow(dat_grasa), 0, 1) dat_grasa &lt;- arrange(dat_grasa, unif) dat_grasa$id &lt;- 1:nrow(dat_grasa) dat_e &lt;- dat_grasa[1:150,] dat_p &lt;- dat_grasa[151:252,] x_e &lt;- dat_e %&gt;% select(-grasacorp, -id, -unif) %&gt;% as.matrix x_p &lt;- dat_p %&gt;% select(-grasacorp, -id, -unif) %&gt;% as.matrix mod_bodyfat_cv &lt;- cv.glmnet(x = x_e, y = dat_e$grasacorp, alpha = 1) #alpha=1 para lasso plot(mod_bodyfat_cv) Veamos los coeficientes para un modelo regularizado con la \\(\\lambda\\) máxima con error consistente con el mínimo (por validación cruzada): coeficientes &lt;- predict(mod_bodyfat_cv, s =&#39;lambda.1se&#39;, type=&#39;coefficients&#39;) coeficientes ## 14 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) -20.75924245 ## edad 0.05179279 ## peso . ## estatura -0.09936002 ## cuello . ## pecho . ## abdomen 0.58019360 ## cadera . ## muslo . ## rodilla . ## tobillo . ## biceps . ## antebrazo . ## muñeca -0.51756816 Y nótese que este modelo solo incluye 4 variables. El error de predicción es similar al modelo que incluye todas las variables, y terminamos con un modelo considerablemente más simple: pred_prueba &lt;- predict(mod_bodyfat_cv, newx = x_p, s =&#39;lambda.1se&#39;) sqrt(mean((pred_prueba-dat_p$grasacorp)^2)) ## [1] 4.374339 La traza confirma que la regularización lasso, además de encoger coeficientes, saca variables del modelo conforme el valor de regularización aumenta: mod_bodyfat &lt;- glmnet(x = x_e, y = dat_e$grasacorp, alpha = 1) #alpha=1 para lasso plot(mod_bodyfat, xvar = &quot;lambda&quot;) Comparado con regresión lineal: pred_prueba &lt;- predict(lm(grasacorp ~., data = dat_e %&gt;% select(-id, -unif)), newdata=dat_p) sqrt(mean((pred_prueba-dat_p$grasacorp)^2)) ## [1] 4.311924 "],
["extensiones-para-regresion-lineal-y-logistica.html", "Clase 6 Extensiones para regresión lineal y logística 6.1 Cómo hacer más flexible el modelo lineal 6.2 Transformación de entradas 6.3 Variables cualitativas 6.4 Interacciones 6.5 Categorización de variables 6.6 Splines (opcional) 6.7 Modelando en escala logarítmica", " Clase 6 Extensiones para regresión lineal y logística Los modelos lineales son modelos simples que tienen la ventaja de que es relativamente fácil entender cómo contribuyen las variables de entrada a la predicción (simplemente describimos los coeficientes), y es relativamente fácil ajustarlos, y es fácil hacer cálculos con ellos. Sin embargo, puede ser que sean pobres desde el punto de vista predictivo. Hay dos razones: Los coeficientes tienen varianza alta, de modo que las predicciones resultantes son inestables (por ejemplo, por pocos datos o variables de entradas correlacionadas). En este caso, vimos que con el enfoque de regularización ridge o lasso podemos mejorar la estabilidad, las predicciones, y obtener modelos más parsimoniosos. El modelo tiene sesgo alto, en el sentido de que la estructura lineal es deficiente para describir patrones claros e importantes en los datos. Este problema puede suceder cuando tenemos relaciones complejas entre las variables. Cuando hay relativamente pocas entradas y suficientes datos, puede ser posible ajustar estructuras más realistas y complejas. Aunque veremos otros métodos para atacar este problema más adelante, a veces extensiones simples del modelo lineal pueden resolver este problema. Igualmente, esperamos encontrar mejores predicciones con modelos más realistas. 6.1 Cómo hacer más flexible el modelo lineal Podemos construir modelos lineales más flexibles expandiendo el espacio de entradas con transformaciones y combinaciones de las variables originales de entrada. La idea básica es entonces transformar a nuevas entradas, antes de ajustar un modelo: \\[(x_1,...,x_p) \\to (b_1(x),...,b_M (x)).\\] donde típicamente \\(M\\) es mayor que \\(p\\). Entonces, en lugar de ajustar el modelo lineal en las \\(x_1,\\ldots, x_p\\), que es \\[ f(x) = \\beta_0 + \\sum_{i=1}^p \\beta_jx_j\\] ajustamos un modelo lineal en las entradas transformadas: \\[ f(x) = \\beta_0 + \\sum_{i=1}^M \\beta_jb_j(x).\\] Como cada \\(b_j\\) es una función que toma valores numéricos, podemos considerarla como una entrada derivada de las entradas originales. Ejemplo Si \\(x_1\\) es compras totales de un cliente de tarjeta de crédito, y \\(x_2\\) es el número de compras, podemos crear una entrada derivada \\(b_1(x_1,x_2)=x_1/x_2\\) que representa el tamaño promedio por compra. Podríamos entonces poner \\(b_2(x_1,x_2)=x_1\\), \\(b_3(x_1,x_2)=x_2\\), y ajustar un modelo lineal usando las entradas derivadas \\(b_1,b_2, b_3\\). Lo conveniente de este enfoque es que lo único que hacemos para hacer más flexible el modelo es transformar en primer lugar las variables de entrada (quizá produciendo más entradas que el número de variables originales). Después construimos un modelo lineal, y todo lo que hemos visto aplica sin cambios: el modelo sigue siendo lineal, pero el espacio de entradas es diferente (generalmente expandido). Veremos las siguientes técnicas: Agregar versiones transformadas de las variables de entrada Incluir variables cualitativas (categóricas). Interacciones entre variables: incluir términos de la forma \\(x_1x_2\\) Regresión polinomial: incluír términos de la forma \\(x_1^2\\), \\(x_1^3\\), etcétera. Splines de regresión. 6.2 Transformación de entradas Una técnica útil para mejorar el sesgo de modelos de regresión consiste en incluir o sustituir valores transformados de las variables de entrada. Ejemplo: agregar entradas transformadas Empezamos por predecir el valor de una casa en función de calidad de terminados. Preparamos los datos: library(tidyverse) cbbPalette &lt;- c(&quot;#000000&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) theme_set(theme_minimal()) datos_casas &lt;- read_csv(&#39;./tareas/tarea_6_datos/data_train.csv&#39;, na=&quot;&quot;) set.seed(9911) indices_entrena &lt;- sample(1:nrow(datos_casas), 1000) casas_e &lt;- datos_casas[indices_entrena, ] %&gt;% mutate(log_price = log(SalePrice)) casas_p &lt;- datos_casas[-indices_entrena, ] %&gt;% mutate(log_price = log(SalePrice)) Ajustamos el modelo y lo probamos mod_1 &lt;- lm(SalePrice ~ OverallQual , data = casas_e) calc_error &lt;- function(mod, datos_p, y_name = &quot;SalePrice&quot;){ preds &lt;- predict(mod, newdata = datos_p) dat &lt;- data_frame(pred = preds, observado = datos_p[[y_name]]) error &lt;- sqrt(mean((preds - datos_p[[y_name]])^2)) grafica &lt;- ggplot(dat, aes(x = preds, y = observado)) + geom_point(alpha = 0.5) + geom_abline(colour = &quot;red&quot;) + annotate(&quot;text&quot;, x = 1000, y= 300000, label = paste0(&quot;rmse &quot;, round(error))) + geom_smooth(method=&quot;loess&quot;, span = 2, method.args=list(family= &quot;symmetric&quot;)) print(grafica) error } calc_error(mod_1, casas_p, &quot;SalePrice&quot;) ## [1] 42180.17 Y notamos que nuestras predicciones parecen estar sesgadas: tienden a ser bajas cuando el valor de la casa es alto o bajo. Esto es signo de sesgo, y usualmente implica que existen relaciones no lineales en las variables que estamos considerando, o interacciones que no estamos incluyendo en nuestro modelo. Una técnica es agregar entradas derivadas de las que tenemos, usando transformaciones no lineales. Por ejemplo, podríamos hacer: mod_2 &lt;- lm(SalePrice ~ OverallQual + I(OverallQual^2) , data = casas_e) calc_error(mod_2, casas_p, &quot;SalePrice&quot;) ## [1] 40830 Y redujimos el error de prueba. Esta reducción claramente proviene de una reducción de sesgo, pues usamos un modelo más complejo (una variable adicional). Ahora agregamos otras variables: el tamaño del área habitable, garage y sótano, y condición general, mod_3 &lt;- lm(SalePrice ~ OverallQual + I(OverallQual^2) + OverallCond + GrLivArea + TotalBsmtSF + GarageArea, data = casas_e) calc_error(mod_3, casas_p, &quot;SalePrice&quot;) ## [1] 32933.89 Podemos pensar en expandir el modelo de maneras distintas. Por ejemplo, podríamos incluir la relación que hay entre tamaño del sótano y área habitable: mod_4 &lt;- lm(SalePrice ~ OverallQual + I(OverallQual^2) + GrLivArea + TotalBsmtSF + GarageArea + OverallCond + I( TotalBsmtSF / GrLivArea) + I( GarageArea / GrLivArea), data = casas_e) calc_error(mod_4, casas_p, &quot;SalePrice&quot;) ## [1] 32957.83 Y estos cambios parecen no mejorar nuestro modelo. 6.3 Variables cualitativas Muchas veces queremos usar variables cualitativas como entradas de nuestro modelo. Pero en la expresión \\[ f(x) = \\beta_0 + \\sum_{i=1}^p \\beta_jx_j,\\] todas las entradas son numéricas. Podemos usar un truco simple para incluir variables cualitativas Ejemplo Supongamos que queremos incluir la variable CentralAir, si tiene aire acondicionado central o no. Podemos ver en este análisis simple que, por ejemplo, controlando por tamaño de la casa, agrega valor tener aire acondicionado central: casas_e %&gt;% group_by(CentralAir) %&gt;% count ## # A tibble: 2 x 2 ## # Groups: CentralAir [2] ## CentralAir n ## &lt;chr&gt; &lt;int&gt; ## 1 N 67 ## 2 Y 933 ggplot(casas_e, aes(x=GrLivArea, y=SalePrice, colour=CentralAir, group=CentralAir)) + geom_jitter(alpha=1) + geom_smooth(method=&#39;lm&#39;, se=FALSE, size=1.5) + scale_y_log10(breaks=c(0.25,0.5,1,2))+ scale_x_log10(breaks=c(500,1000,2000,4000,8000)) Podemos incluir de manera simple esta variable creando una variable dummy o indicadora, que toma el 1 cuando la casa tiene AC y 0 si no: casas_e &lt;- casas_e %&gt;% mutate(AC_present = as.numeric(CentralAir == &quot;Y&quot;)) casas_e %&gt;% select(Id, CentralAir, AC_present) ## # A tibble: 1,000 x 3 ## Id CentralAir AC_present ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1070 Y 1 ## 2 730 Y 1 ## 3 1326 N 0 ## 4 1193 Y 1 ## 5 1079 Y 1 ## 6 85 Y 1 ## 7 1143 Y 1 ## 8 910 Y 1 ## 9 1063 N 0 ## 10 371 Y 1 ## # ... with 990 more rows Y ahora podemos hacer: mod_5 &lt;- lm(SalePrice ~ OverallQual + I(OverallQual^2) + GrLivArea + TotalBsmtSF + GarageArea + OverallCond + CentralAir, data = casas_e) calc_error(mod_5, casas_p, &quot;SalePrice&quot;) ## [1] 32755.62 Que no es una gran mejora, pero esperado dado que pocas de estas casas tienen aire acondicionado. Cuando la variable categórica tiene \\(K\\) clases, solo creamos variables indicadores de las primeras \\(K-1\\) clases, pues la dummy de la última clase tiene información redundante: es decir, si para las primeras \\(K-1\\) clases las variables dummy son cero, entonces ya sabemos que se trata de la última clase \\(K\\), y no necesitamos incluir una indicadora para la última clase. Ejemplo Vamos a incluir la variable BsmtQual, que tiene los niveles: casas_e %&gt;% group_by(BsmtQual) %&gt;% count ## # A tibble: 5 x 2 ## # Groups: BsmtQual [5] ## BsmtQual n ## &lt;chr&gt; &lt;int&gt; ## 1 Ex 93 ## 2 Fa 26 ## 3 Gd 409 ## 4 NA 27 ## 5 TA 445 Podemos hacer una gráfica exploratoria como la anterior: ggplot(casas_e, aes(x=GrLivArea, y=SalePrice, colour=BsmtQual, group=BsmtQual)) + geom_jitter(alpha=1) + geom_smooth(method=&#39;lm&#39;, se=FALSE, size=1.5) + scale_y_log10(breaks=c(0.25,0.5,1,2))+ scale_x_log10(breaks=c(500,1000,2000,4000,8000)) donde vemos que esta variable puede aportar a la predicción. Ajustamos y evaluamos: mod_6 &lt;- lm(SalePrice ~ OverallQual + I(OverallQual^2) + GrLivArea + TotalBsmtSF + GarageArea + OverallCond + CentralAir + BsmtQual, data = casas_e) calc_error(mod_6, casas_p, &quot;SalePrice&quot;) ## [1] 31340.03 Si examinamos los coeficientes, vemos que lm automáticamente convirtió esta variable con dummy coding: coef(mod_6) ## (Intercept) OverallQual I(OverallQual^2) GrLivArea ## 119263.78011 -44860.91908 5157.93509 49.39657 ## TotalBsmtSF GarageArea OverallCond CentralAirY ## 19.98058 40.75125 6905.50493 22902.19901 ## BsmtQualFa BsmtQualGd BsmtQualNA BsmtQualTA ## -66355.09397 -34231.48579 -50546.75887 -54621.42055 bsmt_ind &lt;- str_detect(names(coef(mod_6)), &quot;BsmtQual&quot;) coef(mod_6)[bsmt_ind] %&gt;% sort ## BsmtQualFa BsmtQualTA BsmtQualNA BsmtQualGd ## -66355.09 -54621.42 -50546.76 -34231.49 Nótese que la categoría base (con coeficiente 0 es ‘Ex’, es decir, no aparece TotalBsmtEx). Esta es la razón por la que todos estos coeficientes son negativos (Ex es el mejor nivel). Observaciones: - Nótese también que no hay coeficiente para una de las clases, por lo que discutimos arriba. También podemos pensar que el coeficiente de esta clase es 0, y así comparamos con las otras clases. - Cuando tenemos variables dummy, el intercept se interpreta con el nivel esperado cuando las variables cuantitativas valen cero, y la variable categórica toma la clase que se excluyó en la construcción de las indicadoras. Podemos incluir variables cualitativas usando este truco de codificación dummy (también llamado a veces one-hot encoding). Ojo: variables con muchas categorías pueden inducir varianza alta en el modelo (dependiendo del tamaño de los datos). En estos casos conviene usar regularización y quizá (si es razonable) usar categorizaciones más gruesas. En nuestro ejemplo anterior, observamos que el nivel Fair queda por debajo de Typical y NA. Esto podría se un signo de sobreajuste (estimación con alta varianza de estos coeficientes). 6.4 Interacciones En el modelo lineal, cada variable contribuye de la misma manera independientemente de los valores de las otras variables. Esta es un simplificación o aproximación útil, pero muchas veces puede producir sesgo demasiado grande en el modelo. Por ejemplo: consideremos los siguientes datos de la relación de mediciones de temperatura y ozono en la atmósfera: Ejemplo head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 air &lt;- filter(airquality, !is.na(Ozone) &amp; !is.na(Wind) &amp; !is.na(Temp)) lm(Ozone ~Temp, data = air[1:80,]) ## ## Call: ## lm(formula = Ozone ~ Temp, data = air[1:80, ]) ## ## Coefficients: ## (Intercept) Temp ## -136.474 2.306 set.seed(9132) air &lt;- sample_n(air, 116) ggplot(air[1:50,], aes(x = Temp, y = Ozone)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = FALSE) Y notamos un sesgo posible en nuestro modelo. Si coloreamos por velocidad del viento: cuantiles &lt;- quantile(air$Wind) ggplot(air[1:50,], aes(x = Temp, y = Ozone, colour= cut(Wind, cuantiles))) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = FALSE) Nótese que parece ser que cuando los niveles de viento son altos, entonces hay una relación más fuerte entre temperatura y Ozono. Esto es una interacción de temperatura y viento. Podemos hacer los siguiente: incluír un factor adicional, el producto de temperatura con viento: air$temp_wind &lt;- air$Temp*air$Wind mod_0 &lt;- lm(Ozone ~ Temp, data = air[1:50,]) mod_1 &lt;- lm(Ozone ~ Temp + Wind, data = air[1:50,]) mod_2 &lt;- lm(Ozone ~ Temp + Wind + temp_wind, air[1:50,]) mod_2 ## ## Call: ## lm(formula = Ozone ~ Temp + Wind + temp_wind, data = air[1:50, ## ]) ## ## Coefficients: ## (Intercept) Temp Wind temp_wind ## -317.8272 4.8036 15.9498 -0.2311 pred_0 &lt;- predict(mod_0, newdata = air[51:116,]) pred_1 &lt;- predict(mod_1, newdata = air[51:116,]) pred_2 &lt;- predict(mod_2, newdata = air[51:116,]) mean(abs(pred_0-air[51:116,&#39;Ozone&#39;])) ## [1] 19.88217 mean(abs(pred_1-air[51:116,&#39;Ozone&#39;])) ## [1] 17.13767 mean(abs(pred_2-air[51:116,&#39;Ozone&#39;])) ## [1] 15.52405 Podemos interpretar el modelo con interacción de la siguiente forma: Si \\(Wind = 5\\), entonces la relación Temperatura Ozono es: \\[ Ozono = -290 + 4.5Temp + 14.6(5) - 0.2(Temp)(5) = -217 + 3.5Temp\\] Si \\(Wind=10\\), entonces la relación Temperatura Ozono es: \\[ Ozono = -290 + 4.5Temp + 14.6(15) - 0.2(Temp)(15) = -71 + 1.5Temp\\] Incluir interacciones en modelos lineales es buena idea para problemas con un número relativamente chico de variables (por ejemplo, \\(p &lt; 10\\)). En estos casos, conviene comenzar agregando interacciones entre variables que tengan efectos relativamente grandes en la predicción. No es tan buena estrategia para un número grande de variables: por ejemplo, para clasificación de dígitos, hay 256 entradas. Poner todas las interacciones añadiría más de 30 mil variables adicionales, y es difícil escoger algunas para incluir en el modelo a priori. Pueden escribirse interacciones en fórmulas de lm y los cálculos se hacen automáticamente: mod_3 &lt;- lm(Ozone ~ Temp + Wind + Temp:Wind, air[1:50,]) mod_3 ## ## Call: ## lm(formula = Ozone ~ Temp + Wind + Temp:Wind, data = air[1:50, ## ]) ## ## Coefficients: ## (Intercept) Temp Wind Temp:Wind ## -317.8272 4.8036 15.9498 -0.2311 Podemos incluir interacciones para pares de variables que son importantes en la predicción, o que por conocimiento del dominio sabemos que son factibles. Conviene usar regularización si necesitamos incluir varias interacciones. Ejemplo En nuestro ejemplo de precios de casas ya habíamos intentado utilizar una interacción, considerando el cociente de dos variables. Aquí veremos una que es importante: la relación entre precio y superficie debe tener interacción con el vecindario, pues distintos vecindarios tienen distintos precios por metro cuadrado: agrupamiento &lt;- casas_e %&gt;% group_by(Neighborhood) %&gt;% summarise(media_ft2 = mean(SalePrice / GrLivArea), n = n()) %&gt;% arrange(desc(media_ft2)) %&gt;% mutate(Neighborhood_grp = ifelse(n &lt; 60, &#39;Other&#39;, Neighborhood)) agrupamiento ## # A tibble: 25 x 4 ## Neighborhood media_ft2 n Neighborhood_grp ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 StoneBr 170. 16 Other ## 2 NridgHt 167. 50 Other ## 3 Veenker 155. 8 Other ## 4 Timber 141. 27 Other ## 5 Somerst 141. 61 Somerst ## 6 CollgCr 137. 99 CollgCr ## 7 Blmngtn 135. 13 Other ## 8 NoRidge 132. 32 Other ## 9 ClearCr 126. 20 Other ## 10 Mitchel 126. 34 Other ## # ... with 15 more rows casas_e &lt;- casas_e %&gt;% left_join(agrupamiento) ## Joining, by = &quot;Neighborhood&quot; casas_p &lt;- casas_p %&gt;% left_join(agrupamiento) ## Joining, by = &quot;Neighborhood&quot; En la siguiente gráfica ordenamos la variable Neighborhood_grp de manera que aparecen antes vecindarios más baratos: casas_e$Neighborhood_grp &lt;- reorder(casas_e$Neighborhood_grp, casas_e$media_ft2, mean) ggplot(casas_e, aes(x=GrLivArea, y=SalePrice, colour=Neighborhood_grp, group=Neighborhood_grp)) + geom_jitter(alpha=0.1) + geom_smooth(method=&#39;lm&#39;, se=FALSE, size=1.5) + scale_x_sqrt() + scale_y_sqrt() + scale_colour_manual(values = cbbPalette) Nótese que no solo las curvas se desplazan verticalmente según el vecindario, sino que también su pendiente cambia con el vecindario. Podemos agregar esta interacción a nuestro modelo: mod_6 &lt;- lm(SalePrice ~ OverallQual + I(OverallQual^2) + GrLivArea + TotalBsmtSF + GarageArea + OverallCond + CentralAir + BsmtQual + Neighborhood_grp + Neighborhood_grp:GrLivArea, data = casas_e) calc_error(mod_6, casas_p, &quot;SalePrice&quot;) ## [1] 25027.35 6.5 Categorización de variables En categorización de variable, intentamos hacer un ajuste local en distintas partes del espacio de entrada. La idea es contruir cubetas, particionando el rango de una variable dada, y ajustar entonces un modelo usando la variable dummy indicadora de cada cubeta. dat_wage &lt;- ISLR::Wage ggplot(dat_wage, aes(x=age, y=wage)) + geom_point() Cuando la relación entre entradas y salida no es lineal, podemos obtener menor sesgo en nuestros modelos usando esta técnica. En este ejemplo, escogimos edades de corte aproximadamente separadas por 10 años, por ejemplo: #cuantiles_age &lt;- quantile(dat_wage$age, probs=seq(0,1,0.2)) #cuantiles_age dat_wage &lt;- dat_wage %&gt;% mutate(age_cut = cut(age, c(18, 25, 35, 45, 55, 65, 80), include.lowest=TRUE)) head(dat_wage) ## year age maritl race education region ## 1 2006 18 1. Never Married 1. White 1. &lt; HS Grad 2. Middle Atlantic ## 2 2004 24 1. Never Married 1. White 4. College Grad 2. Middle Atlantic ## 3 2003 45 2. Married 1. White 3. Some College 2. Middle Atlantic ## 4 2003 43 2. Married 3. Asian 4. College Grad 2. Middle Atlantic ## 5 2005 50 4. Divorced 1. White 2. HS Grad 2. Middle Atlantic ## 6 2008 54 2. Married 1. White 4. College Grad 2. Middle Atlantic ## jobclass health health_ins logwage wage age_cut ## 1 1. Industrial 1. &lt;=Good 2. No 4.318063 75.04315 [18,25] ## 2 2. Information 2. &gt;=Very Good 2. No 4.255273 70.47602 [18,25] ## 3 1. Industrial 1. &lt;=Good 1. Yes 4.875061 130.98218 (35,45] ## 4 2. Information 2. &gt;=Very Good 1. Yes 5.041393 154.68529 (35,45] ## 5 2. Information 1. &lt;=Good 1. Yes 4.318063 75.04315 (45,55] ## 6 2. Information 2. &gt;=Very Good 1. Yes 4.845098 127.11574 (45,55] mod_age &lt;- lm(wage ~ age_cut, data=dat_wage) mod_age ## ## Call: ## lm(formula = wage ~ age_cut, data = dat_wage) ## ## Coefficients: ## (Intercept) age_cut(25,35] age_cut(35,45] age_cut(45,55] ## 76.28 27.88 42.79 41.34 ## age_cut(55,65] age_cut(65,80] ## 42.73 26.27 dat_wage$pred_wage &lt;- predict(mod_age) ggplot(dat_wage) + geom_point(aes(x=age, y=wage)) + geom_line(aes(x=age, y=pred_wage), colour = &#39;red&#39;, size=1.1) Podemos escoger los puntos de corte en lugares que son razonables para el problema (rangos en los es razonable modelar como una constante). También podemos hacer cortes automáticos usando percentiles de los datos: por ejemplo, cortar en cuatro usando los percentiles 25%, 0.5% y 0.75%. Con más datos es posible incrementar el número de cortes. Nótese que cuando hacemos estas categorizaciones estamos incrementando el número de parámetros a estimar del modelo (si hacemos tres cortes, por ejemplo, aumentamos en 3 el número de parámetros). Las categorizaciones de variables son útiles cuando sabemos que hay efectos no lineales de la variable subyacente (por ejemplo, edad o nivel socioeconómico), y las categorías son suficientemente chicas para que el modelo localmente constante sea razonable. Muchas veces los splines son mejores opciones: 6.6 Splines (opcional) En estos ejemplos, también es posible incluir términos cuadráticos para modelar la relación, por ejemplo: dat_wage$age_2 &lt;- dat_wage$age^2 mod_age &lt;- lm(wage ~ age + age_2, data=dat_wage) mod_age ## ## Call: ## lm(formula = wage ~ age + age_2, data = dat_wage) ## ## Coefficients: ## (Intercept) age age_2 ## -10.42522 5.29403 -0.05301 dat_wage$pred_wage &lt;- predict(mod_age) ggplot(dat_wage) + geom_point(aes(x=age, y=wage)) + geom_line(aes(x=age, y=pred_wage), colour = &#39;red&#39;, size=1.1) Estas dos técnicas para hacer más flexible el modelo lineal tienen algunas deficiencias: Muchas veces usar potencias de variables de entrada es una mala idea, pues fácilmente podemos encontrar problemas numéricos (potencias altas pueden dar valores muy chicos o muy grandes). La categorización de variables numéricas puede resultar en predictores con discontinuidades, lo cual no siempre es deseable (interpretación). Una alternativa es usar splines, que son familias de funciones con buenas propiedades que nos permiten hacer expansiones del espacio de entradas. No las veremos con detalle, pero aquí hay unos ejemplos: Por ejemplo, podemos usar B-spines, que construyen “chipotes” en distintos rangos de la variable de entrada (es como hacer categorización, pero con funciones de respuesta suaves): library(splines2) age &lt;- seq(18,80, 0.2) splines_age &lt;- bSpline(age, knots = c(25, 35, 45, 55, 65), degree = 3) matplot(x = age, y = splines_age, type = &#39;l&#39;) Observación: estos splines son como una versión suave de categorización de variables numéricas. En particular, los splines de grado 0 son justamente funciones que categorizan variables: splines_age &lt;- bSpline(age, knots = c(25, 35, 45, 55, 65), degree = 0) matplot(splines_age, type=&#39;l&#39;) Por ejemplo: si expandimos el espacio de entradas con estos splines y corremos el modelo: dat_wage &lt;- ISLR::Wage splines_age &lt;- bSpline(dat_wage$age, knots = c(25, 35, 45, 65), degree = 3) %&gt;% data.frame colnames(splines_age) &lt;- paste0(&#39;spline_&#39;, 1:6) dat_wage &lt;- bind_cols(dat_wage, splines_age) dat_sp &lt;- dat_wage %&gt;% dplyr::select(wage, contains(&#39;spline&#39;)) head(dat_sp) ## wage spline_1 spline_2 spline_3 spline_4 spline_5 ## 1 75.04315 0.0000000 0.000000000 0.00000000 0.0000000 0.00000000 ## 2 70.47602 0.4555974 0.474260292 0.06722689 0.0000000 0.00000000 ## 3 130.98218 0.0000000 0.000000000 0.33333333 0.5925926 0.07407407 ## 4 154.68529 0.0000000 0.001481481 0.44018519 0.5204074 0.03792593 ## 5 75.04315 0.0000000 0.000000000 0.14062500 0.6272321 0.22704082 ## 6 127.11574 0.0000000 0.000000000 0.05545833 0.5406104 0.37417611 ## spline_6 ## 1 0.000000000 ## 2 0.000000000 ## 3 0.000000000 ## 4 0.000000000 ## 5 0.005102041 ## 6 0.029755102 mod_age &lt;- lm(wage ~. , data=dat_sp) mod_age ## ## Call: ## lm(formula = wage ~ ., data = dat_sp) ## ## Coefficients: ## (Intercept) spline_1 spline_2 spline_3 spline_4 ## 65.044 1.464 27.176 54.472 52.121 ## spline_5 spline_6 ## 58.230 31.771 dat_wage$pred_wage &lt;- predict(mod_age) ggplot(dat_wage) + geom_point(aes(x=age, y=wage)) + geom_line(aes(x=age, y=pred_wage), colour = &#39;red&#39;, size=1.1) O podemos usar i-splines (b-splines integrados), por ejemplo: splines_age &lt;- iSpline(age, knots = c(25, 35, 45, 65), degree = 2) matplot(splines_age, type=&#39;l&#39;) dat_wage &lt;- ISLR::Wage splines_age &lt;- iSpline(dat_wage$age, knots = c(25, 35, 45, 65), degree = 2) %&gt;% data.frame colnames(splines_age) &lt;- paste0(&#39;spline_&#39;, 1:6) dat_wage &lt;- bind_cols(dat_wage, splines_age) dat_sp &lt;- dat_wage %&gt;% dplyr::select(wage, contains(&#39;spline&#39;)) head(dat_sp) ## wage spline_1 spline_2 spline_3 spline_4 spline_5 spline_6 ## 1 75.04315 0.0000000 0.00000000 0.0000000 0.00000000 0.000000000 0 ## 2 70.47602 0.5414872 0.06722689 0.0000000 0.00000000 0.000000000 0 ## 3 130.98218 1.0000000 1.00000000 0.6666667 0.07407407 0.000000000 0 ## 4 154.68529 1.0000000 0.99851852 0.5583333 0.03792593 0.000000000 0 ## 5 75.04315 1.0000000 1.00000000 0.8593750 0.23214286 0.005102041 0 ## 6 127.11574 1.0000000 1.00000000 0.9445417 0.40393122 0.029755102 0 mod_age &lt;- lm(wage ~. , data=dat_sp) mod_age ## ## Call: ## lm(formula = wage ~ ., data = dat_sp) ## ## Coefficients: ## (Intercept) spline_1 spline_2 spline_3 spline_4 ## 64.643 28.331 26.442 -2.510 7.600 ## spline_5 spline_6 ## -31.903 -5.441 dat_wage$pred_wage &lt;- predict(mod_age) ggplot(dat_wage) + geom_point(aes(x=age, y=wage)) + geom_line(aes(x=age, y=pred_wage), colour = &#39;red&#39;, size=1.1) 6.7 Modelando en escala logarítmica En muchos problemas, es natural transformar variables numéricas con el logaritmo. Supongamos por ejemplo que en nuestro problema la variable \\(y\\) es positiva, y también las entradas son positivas. En primer lugar podríamos intentar modelar \\[ y = b_0 + \\sum b_j x_j \\] pero también podemos transformar las entradas y la salida para construir un modelo multiplicativo: \\(y&#39; = log(y) = b_0 + \\sum b_k \\log(x_j)\\) y ahora queremos predecir el logaritmo de \\(y\\), no \\(y\\) directamente. Esta tipo de transformación tiene dos efectos: Convierte modelos aditivos (regresión lineal) en modelos multiplicativos en las variables no transformadas (pero lineales en escala logarítmica). Esta estructura tiene más sentido para algunos problemas, y es más razonable que la forma lineal aplique para este tipo de problemas. Comprime la parte superior de la escala en relación a la parte baja, y esto es útil para aminorar el efecto de valores atípicos grandes (que puede tener malos efectos numéricos y también pueden producir que los atipicos dominen el error o la estimación de los coeficientes). Ejemplo Consideramos predecir el quilataje de set.seed(22) diamonds_muestra &lt;- sample_n(diamonds, 1000) ggplot(diamonds_muestra, aes(x=carat, y=price)) + geom_point() + geom_smooth(method=&quot;lm&quot;) Nótese que el modelo lineal está sesgado, y produce sobrestimaciones y subestimaciones para distintos valores de \\(x\\). Aunque podríamos utilizar un método más flexible para este modelo, una opción es transformar entrada y salida con logaritmo: diamonds_muestra &lt;- diamonds_muestra %&gt;% mutate(log_price = log(price), log_carat = log(carat)) ggplot(diamonds_muestra, aes(x=log_carat, y=log_price)) + geom_point() + geom_smooth(method = &quot;lm&quot;) Podemos graficar también en unidades originales: ggplot(diamonds_muestra, aes(x=carat, y=price/1000)) + geom_point() + geom_smooth(method = &#39;lm&#39;) + scale_x_log10(breaks=2^seq(-1,5,1)) + scale_y_log10(breaks=2^seq(-2,5,1)) Y vemos que la relación entre los logaritmos es lineal: redujimos el sesgo sin los costos adicionales de varianza que implica agregar más variables e interacciones. En este caso, esta relación es naturalmente multiplicativa (un 10% de incremento relativo en el peso produce un incremento constante en el precio). Cuando una variable toma valores positivos y recorre varios órdenes de magnitud, puede ayudar transformar con logaritmo o raíz cuadrada (esto incluye transformar la variable respuesta). Muchas veces es natural modelar en la escala logarítmica, como en el ejemplo de los diamantes. También tiene utilidad cuando las variables de respuesta o entrada tienen distribuciones muy sesgadas a la derecha (con algunos valores órdenes de magnitud más grandes que la mayoría del grueso de los datos). Tomar logaritmos resulta en mejoras numéricas, y evita que algunos valores atipicos dominen el cálculo del error. Menos común: variables que son proporciones \\(p\\) pueden transformarse mediante la transformación inversa de la logística (\\(x = \\log(\\frac{p}{1-p})\\).) Discusión: En un modelo lineal usual, tenemos que si cambiamos \\(x_j \\to x_j + \\Delta x\\), entonces la predicción \\(y\\) tiene un cambio de \\[\\Delta y = b_j \\Delta x.\\] Es decir, mismos cambios absolutos en alguna variable de entrada produce mismos cambios absolutos en las predicciones, independientemente del nivel de las entradas. Sin embargo, el modelo logarítmico es multiplicativo, pues tomando exponencial de ambos lados, obtenemos: \\[y = B_0\\prod x_j^{b_j}\\] Entonces, si cambiamos \\(x_j \\to x_j + \\Delta x\\), el cambio porcentual en \\(y\\) es \\[ \\frac{y+\\Delta y}{y} = \\left ( \\frac{x_j +\\Delta x}{x_j}\\right )^{b_j}\\] De modo que mismos cambios porcentuales en \\(x\\) resultan en los mismos cambios porcentuales de \\(y\\), independientemente del nivel de las entradas. Adicionalmente, es útil notar que si \\(\\frac{\\Delta x}{x_j}\\) es chica, entonces aproximadamente \\[ \\frac{\\Delta y}{y} \\approx b_j \\frac{\\Delta x}{x_j}\\] Es decir, el cambio relativo en \\(y\\) es proporcional al cambio relativo en \\(x_j\\) para cambios relativamente chicos en \\(x_j\\), y el coeficiente es la constante de proporcionalidad Ejercicio Puedes repetir el ejercicio de la tarea 6 transformando las variables numéricas con logaritmo (o \\(\\log(1+x)\\) cuando \\(x\\) tiene ceros). Utiliza el mismo error del concurso de kaggle, que es el error cuadrático medio en escala logarítmica (en el concurso, esta es otra razón para usar escala logarítmica en la variable respuesta.) 6.7.1 ¿Cuándo usar estas técnicas? Estas técnicas pueden mejorar considerablemente nuestros modelos lineales, pero a veces puede ser difícil descubrir exactamente que transformaciones pueden ser útiles, y muchas veces requiere conocimiento experto del problema que enfrentamos. En general, Es mejor usar regularización al hacer este tipo de trabajo, para protegernos de varianza alta cuando incluimos varias entradas derivadas. Es buena idea probar incluir interacciones entre variables que tienen efectos grandes en la predicción, o interacciones que creemos son importantes en nuestro problema (por ejemplo, temperatura y viento en nuestro ejemplo de arriba, o existencia de estacionamiento y tráfico vehicular como en nuestro ejemplo de predicción de ventas de una tienda). Gráficas como la de arriba (entrada vs respuesta) pueden ayudarnos a decidir si conviene categorizar alguna variable o añadir un efecto no lineal. Este es un trabajo que no es tan fácil, pero para problema con relativamente pocas variables es factible. En situaciones con muchas variables de entrada y muchos datos, existen mejores opciones. "],
["redes-neuronales-parte-1.html", "Clase 7 Redes neuronales (parte 1) 7.1 Introducción a redes neuronales 7.2 Interacciones en redes neuronales 7.3 Cálculo en redes: feed-forward Notación 7.4 Feed forward 7.5 Backpropagation: cálculo del gradiente (clasificación binaria) 7.6 Ajuste de parámetros (introducción)", " Clase 7 Redes neuronales (parte 1) 7.1 Introducción a redes neuronales En la parte anterior, vimos cómo hacer más flexibles los métodos de regresión: la idea es construir entradas derivadas a partir de las variables originales, e incluirlas en el modelo de regresión. Este enfoque es bueno cuando tenemos relativamente pocas variables originales de entrada, y tenemos una idea de qué variables derivadas es buena idea incluir (por ejemplo, splines para una variable como edad, interacciones para variables importantes, etc). Sin embargo, si hay una gran cantidad de entradas, esta técnica puede ser prohibitiva en términos de cálculo y trabajo manual. Por ejemplo, si tenemos unas 100 entradas numéricas, al crear todas las interacciones \\(x_i x_j\\) y los cuadrados \\(x_i^2\\) terminamos con unas 5150 variables. Para el problema de dígitos (256 entradas o pixeles) terminaríamos con unas 32 mil entradas adicionales. Aún cuando es posible regularizar, en estos casos suena más conveniente construir entradas derivadas a partir de los datos. Para hacer esto, consideramos entradas \\(X_1, . . . , X_p\\), y supongamos que tenemos un problema de clasificación binaria, con \\(G = 1\\) o \\(G = 0\\). Aunque hay muchas maneras de construir entradas derivadas, una manera simple sería construir \\(m\\) nuevas entradas mediante: \\[a_k = h \\left ( \\theta_{k,0} + \\sum_{j=1}^p \\theta_{k,j}x_j \\right)\\] para \\(k=1,\\ldots, m\\), donde \\(h\\) es la función logística, y las \\(\\theta\\) son parámetros que seleccionaremos más tarde. La idea es hacer combinaciones lineales de variables transformadas. Modelamos ahora la probabilidad de clase 1 con regresión logística -pero en lugar de usar las entradas originales X usamos las entradas derivadas \\(a_1, . . . , a_m\\): \\[p_1(x) = h \\left ( \\beta_0 + \\sum_{j=1}^m \\beta_ja_j \\right)\\] Podemos representar este esquema con una red dirigida (\\(m=3\\) variables derivadas): Observaciones: ¿Por qué usar \\(h\\) para las entradas derivadas \\(a_k\\)? En primer lugar, nótese que si no transformamos con alguna función no lineal \\(h\\), el modelo final \\(p_1\\) para la probabilidad condicional es el mismo que el de regresión logística (combinaciones lineales de combinaciones lineales son combinaciones lineales). Sin embargo, al transformar con \\(h\\), las \\(x_j\\) contribuyen de manera no lineal a las entradas derivadas. Las variables \\(a_k\\) que se pueden obtener son similares (para una variable de entrada) a los I-splines que vimos en la parte anterior. Es posible demostrar que si se crean suficientes entradas derivadas (\\(m\\) es suficientemente grande), entonces la función \\(p_1(x)\\) puede aproximar cualquier función continua. La función \\(h\\) (que se llama función de activación no es especial: funciones continuas con forma similar a la sigmoide (logística) pueden usarse también (por ejemplo, arcotangente, o lineal rectificada). La idea es que cualquier función se puede aproximar mediante superposición de funciones tipo sigmoide (ver por ejemplo Cybenko 1989, Approximation by Superpositions of a Sigmoidal Function). ¿Cómo construyen entradas las redes neuronales? Comencemos por un ejemplo simple de clasificación binaria con una sola entrada \\(x\\). Supondremos que el modelo verdadero está dado por: h &lt;- function(x){ 1/(1 + exp(-x)) # es lo mismo que exp(x)/(1 + exp(x)) } x &lt;- seq(-2, 2, 0.1) p &lt;- h(2 - 3 * x^2) #probabilidad condicional de clase 1 (vs. 0) set.seed(2805721) x_1 &lt;- runif(30, -2, 2) g_1 &lt;- rbinom(30, 1, h(2 - 3 * x_1^2)) datos &lt;- data.frame(x_1, g_1) dat_p &lt;- data.frame(x, p) g &lt;- qplot(x, p, geom=&#39;line&#39;, colour=&quot;red&quot;) g + geom_point(data = datos, aes(x = x_1, y = g_1), colour = &#39;red&#39;) donde adicionalmente graficamos 30 datos simulados. Recordamos que queremos ajustar la curva roja, que da la probabilidad condicional de clase. Podríamos ajustar un modelo de regresión logística expandiendo manualmente el espacio de entradas agregando \\(x^2\\), y obtendríamos un ajuste razonable. Pero la idea aquí es que podemos crear entradas derivadas de forma automática. Supongamos entonces que pensamos crear dos entradas \\(a_1\\) y \\(a_2\\), funciones de \\(x_1\\), y luego predecir \\(g.1\\), la clase, en función de estas dos entradas. Por ejemplo, podríamos tomar: donde hacemos una regresión logística para predecir \\(G\\) mediante \\[p_1(a) = h(\\beta_0 + \\beta_1a_1+\\beta_2 a_2),\\] \\(a_1\\) y \\(a_2\\) están dadas por \\[a_1(x)=h(\\beta_{1,0} + \\beta_{1,1} x_1),\\] \\[a_2(x)=h(\\beta_{2,0} + \\beta_{2,1} x_1).\\] Por ejemplo, podríamos tomar a_1 &lt;- h( 1 + 2*x) # 2(x+1/2) a_2 &lt;- h(-1 + 2*x) # 2(x-1/2) # una es una versión desplazada de otra. Las funciones \\(a_1\\) y \\(a_2\\) dependen de \\(x\\) de la siguiente forma: dat_a &lt;- data_frame(x = x, a_1 = a_1, a_2 = a_2) dat_a_2 &lt;- dat_a %&gt;% gather(variable, valor, a_1:a_2) ggplot(dat_a_2, aes(x=x, y=valor, colour=variable, group=variable)) + geom_line() Si las escalamos y sumamos, obtenemos dat_a &lt;- data.frame(x=x, a_1 = -4 + 12 * a_1, a_2 = -12 * a_2, suma = -4 + 12 * a_1 - 12 * a_2) dat_a_2 &lt;- dat_a %&gt;% gather(variable, valor, a_1:suma) ggplot(dat_a_2, aes(x = x, y = valor, colour = variable, group = variable)) + geom_line() y finalmente, aplicando \\(h\\): dat_2 &lt;- data.frame(x, p2 = h(-4 + 12 * a_1 - 12 * a_2)) ggplot(dat_2, aes(x=x, y=p2)) + geom_line()+ geom_line(data=dat_p, aes(x=x,y=p), col=&#39;red&#39;) +ylim(c(0,1))+ geom_point(data = datos, aes(x = x_1, y = g_1)) que da un ajuste razonable. Este es un ejemplo de cómo la mezcla de dos funciones logísticas puede replicar esta función con forma de chipote. ¿Cómo ajustar los parámetros? Para encontrar los mejores parámetros, minimizamos la devianza sobre los parámetros \\(\\beta_0,\\beta_1,\\beta_{1,0},\\beta_{1,1}, \\beta_{2,0},\\beta_{2,1}\\). Veremos más adelante que conviene hacer esto usando descenso o en gradiente o descenso en gradiente estocástico, pero por el momento usamos la función optim de R para minimizar la devianza. En primer lugar, creamos una función que para todas las entradas calcula los valores de salida. En esta función hacemos feed-forward de las entradas a través de la red para calcular la salida ## esta función calcula los valores de cada nodo en toda la red, ## para cada entrada feed_fow &lt;- function(beta, x){ a_1 &lt;- h(beta[1] + beta[2] * x) # calcula variable 1 de capa oculta a_2 &lt;- h(beta[3] + beta[4] * x) # calcula variable 2 de capa oculta p &lt;- h(beta[5] + beta[6] * a_1 + beta[7] * a_2) # calcula capa de salida p } Nótese que simplemente seguimos el diagrama mostrado arriba para hacer los cálculos, combinando linealmente las entradas en cada capa. Ahora definimos una función para calcular la devianza. Conviene crear una función que crea funciones, para obtener una función que sólo se evalúa en los parámetros para cada conjunto de datos de entrenamiento fijos: devianza_fun &lt;- function(x, y){ # esta función es una fábrica de funciones devianza &lt;- function(beta){ p &lt;- feed_fow(beta, x) - 2 * mean(y*log(p) + (1-y)*log(1-p)) } devianza } Por ejemplo: dev &lt;- devianza_fun(x_1, g_1) # crea función dev ## ahora dev toma solamente los 7 parámetros beta: dev(c(0,0,0,0,0,0,0)) ## [1] 1.386294 Finalmente, optimizamos la devianza. Para esto usaremos la función optim de R: set.seed(5) salida &lt;- optim(rnorm(7), dev, method = &#39;BFGS&#39;) # inicializar al azar punto inicial salida ## $par ## [1] -24.8192568 23.0201169 -8.4364869 -6.7633494 0.9849461 -14.0157655 ## [7] -14.3394673 ## ## $value ## [1] 0.654347 ## ## $counts ## function gradient ## 103 100 ## ## $convergence ## [1] 1 ## ## $message ## NULL beta &lt;- salida$par Y ahora podemos graficar con el vector \\(\\beta\\) encontrado: ## hacer feed forward con beta encontrados p_2 &lt;- feed_fow(beta, x) dat_2 &lt;- data.frame(x, p_2 = p_2) ggplot(dat_2, aes(x = x, y = p_2)) + geom_line()+ geom_line(data = dat_p, aes(x = x, y = p), col=&#39;red&#39;) +ylim(c(0,1))+ geom_point(data = datos, aes(x = x_1, y = g_1)) Los coeficientes estimados, que en este caso muchas veces se llaman pesos, son: beta ## [1] -24.8192568 23.0201169 -8.4364869 -6.7633494 0.9849461 -14.0157655 ## [7] -14.3394673 que parecen ser muy grandes. Igualmente, de la figura vemos que el ajuste no parece ser muy estable (esto se puede confirmar corriendo con distintos conjuntos de entrenamiento). Podemos entonces regularizar ligeramente la devianza para resolver este problema. En primer lugar, definimos la devianza regularizada (ridge), donde penalizamos todos los coeficientes que multiplican a una variable, pero no los intercepts: devianza_reg &lt;- function(x, y, lambda){ # esta función es una fábrica de funciones devianza &lt;- function(beta){ p &lt;- feed_fow(beta, x) # en esta regularizacion quitamos sesgos, pero puede hacerse también con sesgos. - 2 * mean(y*log(p) + (1-y)*log(1-p)) + lambda*sum(beta[-c(1,3,5)]^2) } devianza } dev_r &lt;- devianza_reg(x_1, g_1, 0.001) # crea función dev set.seed(5) salida &lt;- optim(rnorm(7), dev_r, method=&#39;BFGS&#39;) # inicializar al azar punto inicial salida ## $par ## [1] -4.826652 4.107146 -4.845864 -4.561488 1.067216 -5.236453 -5.195981 ## ## $value ## [1] 0.8322745 ## ## $counts ## function gradient ## 102 100 ## ## $convergence ## [1] 1 ## ## $message ## NULL beta &lt;- salida$par dev(beta) ## [1] 0.74018 p_2 &lt;- feed_fow(beta, x) dat_2 &lt;- data.frame(x, p_2 = p_2) ggplot(dat_2, aes(x = x, y = p_2)) + geom_line()+ geom_line(data = dat_p, aes(x = x, y = p), col=&#39;red&#39;) +ylim(c(0,1))+ geom_point(data = datos, aes(x = x_1, y = g_1)) y obtenemos un ajuste mucho más estable. Podemos también usar la función nnet del paquete nnet. Ojo: en nnet, el error es la devianza no está normalizada por número de casos y dividida entre dos: library(nnet) set.seed(12) nn &lt;- nnet(g_1 ~ x_1, data = datos, size = 2, decay = 0.0, entropy = T) ## # weights: 7 ## initial value 19.318858 ## iter 10 value 11.967705 ## iter 20 value 10.251964 ## iter 30 value 9.647707 ## iter 40 value 9.573030 ## iter 50 value 9.569389 ## iter 60 value 9.555125 ## iter 70 value 9.546210 ## iter 80 value 9.544512 ## iter 90 value 9.539825 ## iter 100 value 9.535977 ## final value 9.535977 ## stopped after 100 iterations nn$wts ## [1] -51.274012 48.789640 8.764849 6.219901 -29.155181 -24.998108 ## [7] 30.125349 nn$value ## [1] 9.535977 2*nn$value/30 ## [1] 0.6357318 dev(nn$wts) ## [1] 0.6357318 qplot(x, predict(nn, newdata=data.frame(x_1 = x)), geom=&#39;line&#39;) 7.1.0.1 Ejercicio Un ejemplo más complejo. Utiliza los siguientes datos, y agrega si es necesario variables derivadas \\(a_3, a_4\\) en la capa oculta. h &lt;- function(x){ exp(x)/(1 + exp(x)) } x &lt;- seq(-2,2,0.05) p &lt;- h(3 + x- 3 * x ^ 2 + 3 * cos(4 * x)) set.seed(280572) x.2 &lt;- runif(300, -2, 2) g.2 &lt;- rbinom(300, 1, h(3 + x.2 - 3 * x.2 ^ 2 + 3 * cos(4 * x.2))) datos &lt;- data.frame(x.2,g.2) dat.p &lt;- data.frame(x,p) g &lt;- qplot(x,p, geom=&#39;line&#39;, col=&#39;red&#39;) g + geom_jitter(data = datos, aes(x=x.2,y=g.2), col =&#39;black&#39;, position =position_jitter(height=0.05), alpha=0.4) 7.2 Interacciones en redes neuronales Es posible capturar interacciones con redes neuronales. Consideremos el siguiente ejemplo simple: p &lt;- function(x1, x2){ h(-5 + 10*x1 + 10*x2 - 30*x1*x2) } dat &lt;- expand.grid(x1 = seq(0, 1, 0.05), x2 = seq(0, 1, 0.05)) dat &lt;- dat %&gt;% mutate(p = p(x1, x2)) ggplot(dat, aes(x=x1, y=x2)) + geom_tile(aes(fill=p)) Esta función puede entenderse como un o exclusivo: la probabilidad es alta sólo cuando x1 y x2 tienen valores opuestos (x1 grande pero x2 chica y viceversa). No es posible modelar esta función mediante el modelo logístico (sin interacciones). Pero podemos incluir la interacción en el modelo logístico o intentar usar una red neuronal. Primero simulamos unos datos y probamos el modelo logístico con y sin interacciones: set.seed(322) n &lt;- 500 dat_ent &lt;- data_frame(x1=runif(n,0,1), x2 = runif(n, 0, 1)) %&gt;% mutate(p = p(x1, x2)) %&gt;% mutate(y = rbinom(n, 1, p)) mod_1 &lt;- glm(y ~ x1 + x2, data = dat_ent, family = &#39;binomial&#39;) mod_1 ## ## Call: glm(formula = y ~ x1 + x2, family = &quot;binomial&quot;, data = dat_ent) ## ## Coefficients: ## (Intercept) x1 x2 ## -0.01011 -1.47942 -1.19196 ## ## Degrees of Freedom: 499 Total (i.e. Null); 497 Residual ## Null Deviance: 529.4 ## Residual Deviance: 504.5 AIC: 510.5 table(predict(mod_1) &gt; 0.5, dat_ent$y) ## ## 0 1 ## FALSE 389 111 mod_2 &lt;- glm(y ~ x1 + x2 + x1:x2, data = dat_ent, family = &#39;binomial&#39;) mod_2 ## ## Call: glm(formula = y ~ x1 + x2 + x1:x2, family = &quot;binomial&quot;, data = dat_ent) ## ## Coefficients: ## (Intercept) x1 x2 x1:x2 ## -4.726 9.641 9.831 -32.466 ## ## Degrees of Freedom: 499 Total (i.e. Null); 496 Residual ## Null Deviance: 529.4 ## Residual Deviance: 305.6 AIC: 313.6 table(predict(mod_2) &gt; 0.5, dat_ent$y) ## ## 0 1 ## FALSE 374 60 ## TRUE 15 51 Observese la gran diferencia de devianza entre los dos modelos (en este caso, el sobreajuste no es un problema). Ahora consideramos qué red neuronal puede ser apropiada set.seed(11) nn &lt;- nnet(y ~ x1 + x2, data = dat_ent, size = 3, decay = 0.001, entropy = T, maxit = 500) ## # weights: 13 ## initial value 294.186925 ## iter 10 value 233.560013 ## iter 20 value 195.096851 ## iter 30 value 190.466423 ## iter 40 value 184.454612 ## iter 50 value 170.767082 ## iter 60 value 156.347417 ## iter 70 value 153.521658 ## iter 80 value 153.069566 ## iter 90 value 152.852374 ## iter 100 value 152.835812 ## iter 110 value 152.826924 ## iter 120 value 152.825819 ## final value 152.825815 ## converged #primera capa matrix(round(nn$wts[1:9], 1), 3,3, byrow=T) ## [,1] [,2] [,3] ## [1,] -2.2 3.0 -2.4 ## [2,] -8.2 5.9 8.7 ## [3,] -2.7 -1.6 3.6 #segunda capa round(nn$wts[10:13], 1) ## [1] -5.7 15.1 -8.6 19.8 #2*nn$value El cálculo de esta red es: feed_fow &lt;- function(beta, x){ a_1 &lt;- h(beta[1] + beta[2]*x[1] + beta[3]*x[2]) a_2 &lt;- h(beta[4] + beta[5]*x[1] + beta[6]*x[2]) a_3 &lt;- h(beta[7] + beta[8]*x[1] + beta[9]*x[2]) p &lt;- h(beta[10]+beta[11]*a_1 + beta[12]*a_2 + beta[13]*a_3) # calcula capa de salida p } Y vemos que esta red captura la interacción: feed_fow(nn$wts, c(0,0)) ## [1] 0.04946031 feed_fow(nn$wts, c(0,1)) ## [1] 0.9560235 feed_fow(nn$wts, c(1,0)) ## [1] 0.9830594 feed_fow(nn$wts, c(1,1)) ## [1] 0.004197137 dat &lt;- dat %&gt;% rowwise %&gt;% mutate(p_red = feed_fow(nn$wts, c(x1, x2))) ggplot(dat, aes(x=x1, y=x2)) + geom_tile(aes(fill=p_red)) Observación: ¿cómo funciona esta red? Consideremos la capa intermedia dat_entrada &lt;- data_frame(x_1 = c(0,0,1,1), x_2 = c(0,1,0,1)) a_1 &lt;- dat_entrada %&gt;% rowwise() %&gt;% mutate(a_1 = h(sum(nn$wts[1:3] * c(1,x_1,x_2) ))) a_2 &lt;- dat_entrada %&gt;% rowwise() %&gt;% mutate(a_2 = h(sum(nn$wts[4:6] * c(1,x_1,x_2) ))) a_3 &lt;- dat_entrada %&gt;% rowwise() %&gt;% mutate(a_3 = h(sum(nn$wts[7:9] * c(1,x_1,x_2) ))) capa_intermedia &lt;- left_join(a_1, a_2) %&gt;% left_join(a_3) ## Joining, by = c(&quot;x_1&quot;, &quot;x_2&quot;) ## Joining, by = c(&quot;x_1&quot;, &quot;x_2&quot;) a_1 ## Source: local data frame [4 x 3] ## Groups: &lt;by row&gt; ## ## # A tibble: 4 x 3 ## x_1 x_2 a_1 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0.102 ## 2 0 1 0.0101 ## 3 1 0 0.686 ## 4 1 1 0.164 a_3 ## Source: local data frame [4 x 3] ## Groups: &lt;by row&gt; ## ## # A tibble: 4 x 3 ## x_1 x_2 a_3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0.0629 ## 2 0 1 0.709 ## 3 1 0 0.0130 ## 4 1 1 0.324 a_2 ## Source: local data frame [4 x 3] ## Groups: &lt;by row&gt; ## ## # A tibble: 4 x 3 ## x_1 x_2 a_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0.000284 ## 2 0 1 0.621 ## 3 1 0 0.0960 ## 4 1 1 0.998 Y observamos que las unidades \\(a_1\\) y \\(a_3\\) tienen valor alto cuando las variables \\(x_1\\) y \\(x_2\\), correspondientemente, tienen valores altos. La unidad \\(a_2\\) responde cuando tanto como \\(x_1\\)y \\(x_2\\) tienen valores altos. En la capa final, le damos peso relativamente alto a las unidades \\(a_1\\) y \\(a_3\\), y peso negativo a la unidad \\(a_2\\) nn$wts[10:13] ## [1] -5.747250 15.138708 -8.628917 19.801144 capa_final &lt;- capa_intermedia %&gt;% rowwise() %&gt;% mutate(p= h(sum(nn$wts[10:13]*c(1,a_1,a_2,a_3) ))) %&gt;% mutate(p=round(p,2)) capa_final ## Source: local data frame [4 x 6] ## Groups: &lt;by row&gt; ## ## # A tibble: 4 x 6 ## x_1 x_2 a_1 a_2 a_3 p ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0.102 0.000284 0.0629 0.05 ## 2 0 1 0.0101 0.621 0.709 0.96 ## 3 1 0 0.686 0.0960 0.0130 0.98 ## 4 1 1 0.164 0.998 0.324 0 7.3 Cálculo en redes: feed-forward Ahora generalizamos lo que vimos arriba para definir la arquitectura básica de redes neuronales y cómo se hacen cálculos en las redes. A las variables originales les llamamos capa de entrada de la red, y a la variable de salida capa de salida. Puede haber más de una capa intermedia. A estas les llamamos capas ocultas. Cuando todas las conexiones posibles de cada capa a la siguiente están presente, decimos que la red es completamente conexa. Como vimos en el ejemplo de arriba, para hacer cálculos en la red empezamos con la primera capa, hacemos combinaciones lineales y aplicamos nuestra función no lineal \\(h\\). Una vez que calculamos la segunda capa, podemos calcular la siguiente de la misma forma: combinaciones lineales y aplicación de \\(h\\). Y así sucesivamente hasta que llegamos a la capa final. Notación Sea \\(L\\) el número total de capas. En primer lugar, para un cierto caso de entrada \\(x = (x_1,x_2,\\ldots, x_p)\\), denotamos por: \\(a^{(l)}_j\\) el valor que toma la unidad \\(j\\) de la capa \\(l\\), para \\(j=0,1,\\ldots, n_{l}\\), donde \\(n_l\\) es el número de unidades de la capa \\(l\\). Ponemos \\(a^{(l)}_0=1\\) para lidiar con los sesgos. En particular, ponemos \\(a^{(1)}_j = x_j\\), que son los valores de las entradas (primera capa) Para clasificación binaria, la última capa solo tiene un elemento, que es \\(p_1 = a^{(L)}\\). Para un problema de clasificación en \\(K&gt;2\\) clases, tenemos que la última capa es de tamaño \\(K\\): \\(p_1 = a^{(L)}_1, p_2 = a^{(L)}_2,\\ldots, p_K = a^{(L)}_K\\) Adicionalmente, escribimos \\(\\theta_{i,k}^{(l)}=\\) es el peso de entrada \\(a_{k}^{(l-1)}\\) de capa \\(l-1\\) en la entrada \\(a_{i}^{(l)}\\) de la capa \\(l\\). Los sesgos están dados por \\[\\theta_{i,0}^{(l)}\\] Ejemplo En nuestro ejemplo, tenemos que en la capa \\(l=3\\) hay dos unidades. Así que podemos calcular los valores \\(a^{(3)}_1\\) y \\(a^{(3)}_2\\). Están dados por \\[a_1^{(3)} = h(\\theta_{1,0}^{(2)} + \\theta_{1,1}^{(2)} a_1^{(2)}+ \\theta_{1,2}^{(2)}a_2^{(2)}+ \\theta_{1,3}^{(2)} a_3^{(2)})\\] \\[a_2^{(3)} = h(\\theta_{2,0}^{(2)} + \\theta_{2,1}^{(2)} a_1^{(2)}+ \\theta_{2,2}^{(2)}a_2^{(2)}+ \\theta_{2,3}^{(2)} a_3^{(2)})\\] Como se ilustra en la siguiente gráfica: Para visualizar las ordenadas (que también se llaman sesgos en este contexto), ponemos \\(a_0^2=1\\). Ejemplo Consideremos propagar a la capa 3 a partir de la capa 2. Usaremos los siguientes pesos para capa 3 y valores de la capa 2 (en gris están los sesgos): Que en nuestra notación escribimos como \\[a^{(2)}_0 = 1, a^{(2)}_1 = -2, a^{(2)}_2 = 5, a^{(2)}=3\\] y los pesos son, para la primera unidad: \\[\\theta^{(2)}_{1,0} = 3, \\,\\,\\, \\theta^{(2)}_{1,1} = 1.5,\\,\\,\\,\\theta^{(2)}_{1,2} = -1,\\,\\,\\theta^{(2)}_{1,3} = -0.5 \\] y para la segunda unidad \\[\\theta^{(2)}_{2,0} = 1, \\,\\,\\, \\theta^{(2)}_{2,1} = 2,\\,\\,\\,\\theta^{(2)}_{2,2} = 0.5,\\,\\, \\theta^{(2)}_{2,3} = -0.2\\] Y ahora queremos calcular los valores que toman las unidades de la capa 3, que son \\(a^{(3)}_1\\) y \\(a^{(3)}_2\\)$ Para hacer feed forward a la siguiente capa, hacemos entonces \\[a^{(3)}_1 = h(3 + a^{(2)}_1 - a^{(2)}_2 -0.5 a_3^{(2)}),\\] \\[a^{(3)}_2 = h(1 + 2a^{(2)}_1 + 0.5a^{(2)}_2 - 0.2 a_3^{(2)}),\\] Ponemos los pesos y valores de la capa 2 (incluyendo sesgo): a_2 &lt;- c(1, -2, 5, 3) # ponemos un 1 al principio para el sesgo theta_2_1 = c(3, 1.5, -1.0, -0.5) theta_2_2 = c(1, 2, 0.5, -0.2) y calculamos a_3 &lt;- c(1, h(sum(theta_2_1*a_2)),h(sum(theta_2_2*a_2))) # ponemos un 1 al principio a_3 ## [1] 1.000000000 0.001501182 0.249739894 7.4 Feed forward Para calcular los valores de salida de una red a partir de pesos y datos de entrada, usamos el algoritmo feed-forward, calculando capa por capa. Cálculo en redes: Feed-forward Para la primera capa, escribimos las variables de entrada: \\[a^{(1)}_j = x_j, j=1\\ldots,n_1\\] Para la primera capa oculta, o la segunda capa \\[a^{(2)}_j = h\\left( \\theta_{j,0}^{(1)}+ \\sum_{k=1}^{n_1} \\theta_{j,k}^{(1)} a^{(1)}_k \\right), j=1\\ldots,n_2\\] para la \\(l\\)-ésima capa: \\[a^{(l)}_j = h\\left( \\theta_{j,0}^{(l-1)}+ \\sum_{k=1}^{n_{l-1}} \\theta_{j,k}^{(l-1)} a^{(l-1)}_k \\right), j=1\\ldots,n_{l}\\] y así sucesivamente. Para la capa final o capa de salida (para problema binario), suponiendo que tenemos \\(L\\) capas (\\(L-2\\) capas ocultas): \\[p_1 = h\\left( \\theta_{1,0}^{(L-1)}+ \\sum_{k=1}^{n_{L-1}} \\theta_{1,k}^{(L-1)} a^{(L-1)}_k \\right).\\] Nótese que entonces: Cada capa se caracteriza por el conjunto de parámetros \\(\\Theta^{(l)}\\), que es una matriz de \\(n_l\\times n_{l-1}\\). La red completa entonces se caracteriza por: La estructura elegida (número de capas ocultas y número de nodos en cada capa oculta). Las matrices de pesos en cada capa \\(\\Theta^{(1)},\\Theta^{(2)},\\ldots, \\Theta^{(L-1)}\\) Adicionalmente, escribimos en forma vectorial: \\[a^{(l)} = (a^{(l)}_0, a^{(l)}_1, a^{(l)}_2, \\ldots, a^{(l)}_{n_l})^t\\] Para calcular la salidas, igual que hicimos, antes, propagaremos hacia adelante los valores de las variables de entrada usando los pesos. Agregando entradas adicionales en cada capa \\(a_0^{(l)}\\), \\(l=1,2,\\ldots, L-1\\), donde \\(a_0^{l}=1\\), y agregando a \\(\\Theta^{(l)}\\) una columna con las ordenadas al origen (o sesgos) podemos escribir: Feed-forward(matricial) Capa 1 (vector de entradas) \\[ a^{(1)} = x\\] Capa 2 \\[ a^{(2)} = h(\\Theta^{(1)}a^{(1)})\\] Capa \\(l\\) (oculta) \\[ a^{(l)} = h(\\Theta^{(l-1)}a^{(l-1)})\\] Capa de salida: En un problema de clasificación binaria, la capa de salida se calcula como en regresión logística: \\[a^{(L)}= p = h(\\Theta^{(L-1)}a^{(L-1)})\\] donde \\(h\\) se aplica componente a componente sobre los vectores correspondientes. Nótese que feed-foward consiste principalmente de multiplicaciones de matrices con algunas aplicaciones de \\(h\\) Para un problema de regresión, la última capa se calcula como en regresión lineal: \\[a^{(L)} = p = \\Theta^{(L-1)}a^{(L-1)}\\] 7.5 Backpropagation: cálculo del gradiente (clasificación binaria) Más adelante, para ajustar los pesos y sesgos de las redes (valores \\(\\theta\\)), utilizaremos descenso en gradiente y otros algoritmos derivados del gradiente (descenso estocástico). En esta parte entonces veremos cómo calcular estos gradientes con el algoritmo de back-propagation, que es una aplicación de la regla de la cadena para derivar. Back-propagation resulta en una fórmula recursiva donde propagamos errores de la red como gradientes desde el final de red (capa de salida) hasta el principio, capa por capa. Consideramos el problema de clasificación binaria Recordamos la devianza (con regularización ridge) es \\[D = -\\frac{2}{n}\\sum_{i=1}^n y_i\\log(p_1(x_i)) +(1-y_i)\\log(1-p_1(x_i)) + \\lambda \\sum_{l=2}^{L} \\sum_{k=1}^{n_{l-1}} \\sum_{j=1}^{n_l}(\\theta_{j,k}^{(l)})^2.\\] Queremos entonces calcular las derivadas de la devianza con respecto a cada parámetro \\(\\theta_{j,k}^{(l)}\\). Esto nos proporciona el gradiente para nuestro algoritmo de descenso. Consideramos aquí el problema de clasificación binaria con devianza como función de pérdida, y sin regularización. La parte de la parcial que corresponde al término de regularización es fácil de agregar al final. Recordamos también nuestra notación para la función logística (o sigmoide): \\[h(z)=\\frac{1}{1+e^{-z}}.\\] Necesitaremos su derivada, que está dada por (cálculala): \\[h&#39;(z) = h(z)(1-h(z))\\] 7.5.1 Cálculo para un caso de entrenamiento Como hicimos en regresión logística, primero simplificamos el problema y consideramos calcular las parciales para un solo caso de entrenamiento \\((x,y)\\): \\[ D= -\\left ( y\\log (p_1(x)) + (1-y)\\log (1-p_1(x))\\right) . \\] Después sumaremos sobre toda la muestra de entrenamiento. Entonces queremos calcular \\[\\frac{\\partial D}{\\partial \\theta_{j,k}^{(l)}}\\] Y escribiremos, con la notación de arriba, \\[a^{(l+1)}_j = h(z^{(l+1)}_j)\\] donde \\[z^{(l+1)} = \\Theta^{(l)} a^{(l)},\\] que coordenada a coordenada se escribe como \\[z^{(l+1)}_j = \\sum_{k=0}^{n_{l}} \\theta_{j,k}^{(l)} a^{(l)}_k\\] Paso 1: Derivar respecto a capa \\(l+1\\) Como los valores de cada capa determinan los valores de salida y la devianza, podemos escribir (recordemos que \\(a_0^{(l)}=1\\) es constante): \\[D=D(a_0^{(l+1)},a_1^{(l+1)},a_2^{(l+1)},\\ldots, a_{n_{l+1}}^{(l+1)})=D(a_1^{(l+1)},a_2^{(l+1)},\\ldots, a_{n_{l+1}}^{(l+1)})\\] Así que por la regla de la cadena para varias variables: \\[\\frac{\\partial D}{\\partial \\theta_{j,k}^{(l)}} = \\sum_{t=1}^{n_{l}} \\frac{\\partial D}{\\partial a_t^{(l+1)}}\\frac{\\partial a_t^{(l+1)}} {\\partial \\theta_{j,k}^{(l)} }\\] Pero si vemos dónde aparece \\(\\theta_{j,k}^{(l)}\\) en la gráfica de la red: \\[ \\cdots a^{(l)}_k \\xrightarrow{\\theta_{j,k}^{(l)}} a^{(l+1)}_j \\cdots \\rightarrow D\\] Entonces podemos concluir que \\(\\frac{\\partial a_t^{(l+1)}}{\\partial \\theta_{j,k}^{(l)}} =0\\) cuando \\(t\\neq j\\) (pues no dependen de \\(\\theta_{j,k}^{(l)}\\)), y entonces, para toda \\(j=1,2,\\ldots, n_{l+1}, k=0,1,\\ldots, n_{l}\\) \\[\\begin{equation} \\frac{\\partial D}{\\partial \\theta_{j,k}^{(l)}} = \\frac{\\partial D}{\\partial a_j^{(l+1)}}\\frac{\\partial a_j^{(l+1)}}{\\partial \\theta_{j,k}^{(l)} } . \\tag{7.1} \\end{equation}\\] Adicionalmente, como \\[a_j^{(l+1)} = h(z_j^{(l+1)}) = h\\left (\\sum_{k=0}^{n_{l}} \\theta_{j,k}^{(l)} a^{(l)}_k \\right )\\] y las \\(a_k^{(l)}\\) no dependen de \\(\\theta_{j,k}^{(l)}\\), tenemos por la regla de la cadena que \\[\\begin{equation} \\frac{\\partial a_j^{(l+1)}}{\\partial \\theta_{j,k}^{(l)} } = h&#39;(z_j^{(l+1)})a_k^{(l)}. \\end{equation}\\] Esta última expresión podemos calcularla pues sólo requiere la derivada de \\(h\\) y los valores otenidos en el paso de feed-forward. Paso 2: Obtener fórmula recursiva Así que sólo nos queda calcular las parciales (\\(j = 1,\\ldots, n_l\\)) \\[\\frac{\\partial D}{\\partial a_j^{(l)}}\\] Para obtener una fórmula recursiva para esta cantidad (hacia atrás), aplicamos otra vez regla de la cadena, pero con respecto a la capa \\(l\\) (ojo: queremos obtener una fórmula recursiva!): \\[\\frac{\\partial D}{\\partial a_j^{(l)}}= \\sum_{s=1}^{n_{l+1}} \\frac{\\partial D}{\\partial a_s^{(l+1)}}\\frac{\\partial a_s^{(l+1)}}{\\partial a_j^{(l)}},\\] que se puede entender a partir de este diagrama: Nótese que la suma empieza en \\(s=1\\), no en \\(s=0\\), pues \\(a_0^{(l+1)}\\) no depende de \\(a_k^{(l)}\\). En este caso los elementos de la suma no se anulan necesariamente. Primero consideramos la derivada de: \\[\\frac{\\partial a_s^{(l+1)}}{\\partial a_j^{(l)}}=h&#39;(z_s^{(l+1)})\\theta_{s,j}^{(l)},\\] de modo que \\[\\frac{\\partial D}{\\partial a_j^{(l)}}= \\sum_{s=1}^{n_l} \\frac{\\partial D}{\\partial a_s^{(l+1)}} h&#39;(z_s^{(l+1)})\\theta_{s,j}^{(l)}.\\] Nótese que esto nos da una fórmula recursiva para las parciales que nos falta calcular (de \\(D\\) con respecto a \\(a\\)), pues las otras cantidades las conocemos por backpropagation. Paso 3: Simplificación de la recursión \\[\\begin{equation} \\delta_s^{ (l+1)}=\\frac{\\partial D}{\\partial a_s^{(l+1)}} h&#39;(z_s^{(l+1)}) \\tag{7.2} \\end{equation}\\] de manera que la ecuación recursiva es \\[\\begin{equation} \\frac{\\partial D}{\\partial a_j^{(l)}} = \\sum_{s=1}^{n_{l+1}} \\delta_s^{(l+1)}\\theta_{s,j}^{(l)}. \\tag{7.3} \\end{equation}\\] Tenemos que si \\(l=2,\\ldots,L-1\\), entonces podemos escribir (usando (7.3)) como fórmula recursiva: \\[\\begin{equation} \\delta_j^{(l)} = \\left (\\sum_{s=1}^{n_l} \\delta_s^{(l+1)} \\theta_{s,j}^{(l)}\\right ) h&#39;(z_j^{(l)}), \\tag{7.4} \\end{equation}\\] para \\(j=1,2,\\ldots, n_{l}\\). Paso 4: Condiciones inciales Para la última capa, tenemos que (demostrar!) \\[\\delta_1^{(L)}=p - y.\\] Paso 5: Cálculo de parciales Finalmente, usando (7.1) y (7.2) , obtenemos \\[\\frac{\\partial D}{\\partial \\theta_{j,k}^{(l)}} = \\delta_j^{(l+1)}a_k^{(l)},\\] y con esto ya podemos hacer backpropagation para calcular el gradiente sobre cada caso de entrenamiento, y solo resta acumular para obtener el gradiente sobre la muestra de entrenamiento. Muchas veces es útil escribir una versión vectorizada (importante para implementar): Paso 6: Versión matricial Ahora podemos escribir estas ecuaciones en forma vectorial. En primer lugar, \\[\\delta^{(L)}=p-y.\\] Y además se puede ver de la ecuación (7.4) que (\\(\\Theta_{*}^{(l+1)}\\) denota la matriz de pesos sin la columna correspondiente al sesgo): \\[\\begin{equation} \\delta^{(l)}=\\left( \\Theta_{*}^{(l)} \\right)^t\\delta^{(l+1)} \\circ h&#39;(z^{(l)}) \\tag{7.5} \\end{equation}\\] donde \\(\\circ\\) denota el producto componente a componente. Ahora todo ya está calculado. Lo interesante es que las \\(\\delta^{(l)}\\) se calculan de manera recursiva. 7.5.2 Algoritmo de backpropagation Backpropagation Para problema de clasificación con regularización $0 $. Para \\(i=1,\\ldots, N\\), tomamos el dato de entrenamiento \\((x^{(i)}, y^{(i)})\\) y hacemos: Ponemos \\(a^{(1)}=x^{(i)}\\) (vector de entradas, incluyendo 1). Calculamos \\(a^{(2)},a^{(3)},\\ldots, a^{(L)}\\) usando feed forward para la entrada \\(x^{(i)}\\) Calculamos \\(\\delta^{(L)}=a^{ (L)}-y^{(i)}\\), y luego \\(\\delta^{(L-1)},\\ldots, \\delta^{(2)}\\) según la recursión (7.4). Acumulamos \\(\\Delta_{j,k}^{(l)}=\\Delta_{j,k}^{(l)} + \\delta_j^{(l+1)}a_k^{(l)}\\). Finalmente, ponemos, si \\(k\\neq 0\\), \\[D_{j,k}^{(l)} = \\frac{2}{N}\\Delta_{j,k}^{(l)} + 2\\lambda\\theta_{j,k}^{(l)}\\] y si \\(k=0\\), \\[D_{j,k}^{(l)} = \\frac{2}{N}\\Delta_{j,k}^{(l)} .\\] Entonces: \\[D_{j,k}^{(l)} =\\frac{\\partial D}{\\partial \\theta_{j,k}^{(l)}}.\\] Nótese que back-propagation consiste principalmente de mutliplicaciones de matrices con algunas aplicaciones de \\(h\\) y acumulaciones, igual que feed-forward. 7.6 Ajuste de parámetros (introducción) Consideramos la versión con regularización ridge (también llamada L2) de la devianza de entrenamiento como nuestro función objetivo: Ajuste de redes neuronales Para un problema de clasificación binaria con \\(y_i=0\\) o \\(y_i=1\\), ajustamos los pesos \\(\\Theta^{(1)},\\Theta^{(2)},\\ldots, \\Theta^{(L)}\\) de la red minimizando la devianza (penalizada) sobre la muestra de entrenamiento: \\[D = -\\frac{2}{n}\\sum_{i=1}^n y_i\\log(p_1(x_i)) +(1-y_i)\\log(1-p_1(x_i)) + \\lambda \\sum_{l=2}^{L} \\sum_{k=1}^{n_{l-1}} \\sum_{j=1}^{n_l}(\\theta_{j,k}^{(l)})^2.\\] Este problema en general no es convexo y puede tener múltiples mínimos. Veremos el proceso de ajuste, selección de arquitectura, etc. más adelante. Por el momento hacemos unas observaciones acerca de este problema de minimización: Hay varios algoritmos para minimizar esta devianza, algunos avanzados incluyendo información de segundo orden (como Newton), pero actualmente las técnicas más populares, para redes grandes, están derivadas de descenso en gradiente. Más específicamente, una variación, que es descenso estocástico. Que el algoritmo depende principalmente de multiplicaciones de matrices y acumulaciones implica que puede escalarse de diversas maneras. Una es paralelizando sobre la muestra de entrenamiento (y acumular acumulados al final), pero también se puede paralelizar la de multiplicaciones de matrices (para lo cual los GPUs se prestan muy bien). Para redes neuronales, el gradiente se calcula con un algoritmo que se llama back-propagation, que es una aplicación de la regla de la cadena para propagar errores desde la capa de salida a lo largo de todas las capas para ajustar los pesos y sesgos. En estos problemas no buscamos el mínimo global, sino un mínimo local de buen desempeño. Puede haber múltiples mínimos, puntos silla, regiones relativamente planas, precipicios (curvatura alta). Todo esto dificulta el entrenamiento de redes neuronales grandes. Para redes grandes, ni siquiera esperamos a alcanzar un mínimo local, sino que nos detenemos prematuramente cuando obtenemos el mejor desempeño posible. Nótese que la simetría implica que podemos obtener la misma red cambiando pesos entre neuronas y las conexiones correspondientes. Esto implica que necesariamente hay varios mínimos. Para este problema, no tiene sentido comenzar las iteraciones con todos los pesos igual a cero, pues las unidades de la red son simétricas: no hay nada que distinga una de otra si todos los pesos son iguales. Esto quiere decir que si iteramos, ¡todas las neuronas van a aprender lo mismo! Es importante no comenzar valores de los pesos grandes, pues las funciones logísticas pueden quedar en regiones planas donde la minimización es lenta, o podemos tener gradientes demasiado grandes y produzcan inestabilidad en el cálculo del gradiente. Generalmente los pesos se inicializan al azar con variables independientes gaussianas o uniformes centradas en cero, y con varianza chica (por ejemplo \\(U(-0.5,0.5)\\)). Una recomendación es usar \\(U(-1/\\sqrt(m), 1/\\sqrt(m))\\) donde \\(m\\) es el número de entradas. En general, hay que experimentar con este parámetro. El proceso para ajustar una red es entonces: Definir número de capas ocultas, número de neuronas por cada capa, y un valor del parámetro de regularización. Estandarizar las entradas. Seleccionar parámetros al azar para \\(\\Theta^{(2)},\\Theta^{(3)},\\ldots, \\Theta^{(L)}\\). Se toman, por ejemplo, normales con media 0 y varianza chica. Correr un algoritmo de minimización de la devianza mostrada arriba. Verificar convergencia del algoritmo a un mínimo local (o el algoritmo no está mejorando). Predecir usando el modelo ajustado. Finalmente, podemos probar distintas arquitecturas y valores del parámetros de regularización, para afinar estos parámetros según validación cruzada o una muestra de validación. 7.6.1 Ejemplo Consideramos una arquitectura de dos capas para el problema de diabetes library(keras) ## ## Attaching package: &#39;keras&#39; ## The following objects are masked from &#39;package:igraph&#39;: ## ## %&lt;-%, normalize Escalamos y preparamos los datos: diabetes_ent &lt;- MASS::Pima.tr diabetes_pr &lt;- MASS::Pima.te x_ent &lt;- diabetes_ent %&gt;% select(-type) %&gt;% as.matrix x_ent_s &lt;- scale(x_ent) x_valid &lt;- diabetes_pr %&gt;% select(-type) %&gt;% as.matrix x_valid_s &lt;- x_valid %&gt;% scale(center = attr(x_ent_s, &#39;scaled:center&#39;), scale = attr(x_ent_s, &#39;scaled:scale&#39;)) y_ent &lt;- as.numeric(diabetes_ent$type == &#39;Yes&#39;) y_valid &lt;- as.numeric(diabetes_pr$type == &#39;Yes&#39;) Para definir la arquitectura de dos capas con: 10 unidades en cada capa función de activación sigmoide, regularización L2 (ridge), salida logística (\\(p_1\\)), escribimos: set.seed(923) modelo_tc &lt;- keras_model_sequential() # no es necesario asignar a nuevo objeto, modelo_tc es modificado al agregar capas modelo_tc %&gt;% layer_dense(units = 10, activation = &#39;sigmoid&#39;, kernel_regularizer = regularizer_l2(l = 1e-3), kernel_initializer = initializer_random_uniform(minval = -0.5, maxval = 0.5), input_shape=7) %&gt;% layer_dense(units = 10, activation = &#39;sigmoid&#39;, kernel_regularizer = regularizer_l2(l = 1e-3), kernel_initializer = initializer_random_uniform(minval = -0.5, maxval = 0.5)) %&gt;% layer_dense(units = 1, activation = &#39;sigmoid&#39;, kernel_regularizer = regularizer_l2(l = 1e-3), kernel_initializer = initializer_random_uniform(minval = -0.5, maxval = 0.5) ) Ahora difinimos la función de pérdida (devianza es equivalente a entropía cruzada binaria), y pedimos registrar porcentaje de correctos (accuracy) y compilamos en tensorflow: modelo_tc %&gt;% compile( loss = &#39;binary_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.8), metrics = c(&#39;accuracy&#39;,&#39;binary_crossentropy&#39;)) Iteramos con descenso en gradiente y monitoreamos el error de validación. Hacemos 100 iteraciones de descenso en gradiente (épocas=100) iteraciones &lt;- modelo_tc %&gt;% fit( x_ent_s, y_ent, #batch size mismo que nrow(x_ent_s) es descenso en grad. epochs = 1000, batch_size = nrow(x_ent_s), verbose = 1, validation_data = list(x_valid_s, y_valid) ) score &lt;- modelo_tc %&gt;% evaluate(x_valid_s, y_valid) score ## $loss ## [1] 0.4752598 ## ## $acc ## [1] 0.7771084 ## ## $binary_crossentropy ## [1] 0.4362988 tab_confusion &lt;- table(modelo_tc %&gt;% predict_classes(x_valid_s),y_valid) tab_confusion ## y_valid ## 0 1 ## 0 191 42 ## 1 32 67 prop.table(tab_confusion, 2) ## y_valid ## 0 1 ## 0 0.8565022 0.3853211 ## 1 0.1434978 0.6146789 Es importante monitorear las curvas de aprendizaje (entrenamiento y validación) para diagnosticar mejoras: df_iteraciones &lt;- as.data.frame(iteraciones) ggplot(df_iteraciones, aes(x=epoch, y=value, colour=data, group=data)) + geom_line() + geom_point() + facet_wrap(~metric, ncol=1, scales = &#39;free&#39;) Observación: puedes utilizar Tensorboard, una herramienta para visualizar resultados del entrenamiento de modelos incluída en Tensorflow (que es lo que usa keras para hacer los cálculos): iteraciones &lt;- modelo_tc %&gt;% fit( x_ent_s, y_ent, #batch size mismo que nrow(x_ent_s) es descenso en grad. epochs = 500, batch_size = nrow(x_ent_s), verbose = 0, callbacks = callback_tensorboard(&quot;logs/diabetes/run_1&quot;), validation_data = list(x_valid_s, y_valid) ) y después puedes hacer: tensorboard(&quot;logs/diabetes/&quot;) Ejercicio Corre el ejemplo anterior con distintos parámetros de tasa de aprendizaje, número de unidades en las capas de intermedia y regularización (cambia arriba verbose=1 para monitorear al correr). "],
["redes-neuronales-parte-2.html", "Clase 8 Redes neuronales (parte 2) 8.1 Descenso estocástico 8.2 Algoritmo de descenso estocástico 8.3 ¿Por qué usar descenso estocástico por minilotes? 8.4 Escogiendo la tasa de aprendizaje 8.5 Mejoras al algoritmo de descenso estocástico. 8.6 Ajuste de redes con descenso estocástico 8.7 Activaciones relu 8.8 Dropout para regularización", " Clase 8 Redes neuronales (parte 2) En esta parte veremos aspectos más modernos de redes neuronales (incluyendo aprendizaje profundo). Estoy incluye métodos de ajuste, regularización, y definición de activaciones. 8.1 Descenso estocástico El algoritmo más popular para ajustar redes grandes es descenso estocástico, que es una modificación de nuestro algoritmo de descenso en gradiente. Antes de presentar las razones para usarlo, veremos cómo funciona para problemas con regresión lineal o logística. En descenso estocástico, el cálculo del gradiente se hace sobre una submuestra relativamente chica de la muestra de entrenamiento. En este contexto, a esta submuestra se le llama un minilote. En cada iteración, nos movemos en la dirección de descenso de ese minilote. La muestra de entrenamiento se divide entonces (al azar) en minilotes, y recorremos todos los minilotes haciendo una actualización de nuestros parámetros en cada minilote. Un recorrido sobre todos los minilotes se llama una época (las iteraciones se entienden sobre los minilotes). Antes de escribir el algoritmo mostramos una implementación para regresión logística. Usamos las mismas funciones para calcular devianza y gradiente. library(dplyr) library(tidyr) library(ggplot2) h &lt;- function(x){1/(1+exp(-x))} # la devianza es la misma devianza_calc &lt;- function(x, y){ dev_fun &lt;- function(beta){ p_beta &lt;- h(as.matrix(cbind(1, x)) %*% beta) -2*mean(y*log(p_beta) + (1-y)*log(1-p_beta)) } dev_fun } # el cálculo del gradiente es el mismo, pero x_ent y y_ent serán diferentes grad_calc &lt;- function(x_ent, y_ent){ salida_grad &lt;- function(beta){ p_beta &lt;- h(as.matrix(cbind(1, x_ent)) %*% beta) e &lt;- y_ent - p_beta grad_out &lt;- -2*as.numeric(t(cbind(1,x_ent)) %*% e)/nrow(x_ent) names(grad_out) &lt;- c(&#39;Intercept&#39;, colnames(x_ent)) grad_out } salida_grad } Y comparamos los dos algoritmos: descenso &lt;- function(n, z_0, eta, h_deriv){ z &lt;- matrix(0,n, length(z_0)) z[1, ] &lt;- z_0 for(i in 1:(n-1)){ z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ]) } z } # esta implementación es solo para este ejemplo: descenso_estocástico &lt;- function(n_epocas, z_0, eta, minilotes){ #minilotes es una lista m &lt;- length(minilotes) z &lt;- matrix(0, m*n_epocas, length(z_0)) z[1, ] &lt;- z_0 for(i in 1:(m*n_epocas-1)){ k &lt;- i %% m + 1 if(i %% m == 0){ #comenzar nueva época y reordenar minilotes al azar minilotes &lt;- minilotes[sample(1:m, m)] } h_deriv &lt;- grad_calc(minilotes[[k]]$x, minilotes[[k]]$y) z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ]) } z } Usaremos el ejemplo simulado de regresión para hacer algunos experimentos: p_1 &lt;- function(x){ ifelse(x &lt; 30, 0.9, 0.9 - 0.007 * (x - 15)) } set.seed(143) sim_datos &lt;- function(n){ x &lt;- pmin(rexp(n, 1/30), 100) probs &lt;- p_1(x) g &lt;- rbinom(length(x), 1, probs) # con dos variables de ruido: dat &lt;- data_frame(x_1 = (x - mean(x))/sd(x), x_2 = rnorm(length(x),0,1), x_3 = rnorm(length(x),0,1), p_1 = probs, g ) dat %&gt;% select(x_1, x_2, x_3, g) } dat_ent &lt;- sim_datos(100) dat_valid &lt;- sim_datos(1000) glm(g ~ x_1 + x_2+ x_3 , data = dat_ent, family = &#39;binomial&#39;) %&gt;% coef ## (Intercept) x_1 x_2 x_3 ## 1.8082362 -0.7439627 0.2172971 0.3711973 Hacemos descenso en gradiente: iteraciones_descenso &lt;- descenso(300, rep(0,4), 0.8, h_deriv = grad_calc(x_ent = as.matrix(dat_ent[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y_ent=dat_ent$g)) %&gt;% data.frame %&gt;% rename(beta_1 = X2, beta_2 = X3) ggplot(iteraciones_descenso, aes(x=beta_1, y=beta_2)) + geom_point() Y ahora hacemos descenso estocástico. Vamos a hacer minilotes de tamaño 5: dat_ent$minilote &lt;- rep(1:10, each=5) split_ml &lt;- split(dat_ent %&gt;% sample_n(nrow(dat_ent)), dat_ent$minilote) minilotes &lt;- lapply(split_ml, function(dat_ml){ list(x = as.matrix(dat_ml[, c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop=FALSE]), y = dat_ml$g) }) length(minilotes) ## [1] 10 Ahora iteramos. Nótese cómo descenso en gradiente tiene un patrón aleatorio de avance hacia el mínimo, y una vez que llega a una región oscila alrededor de este mínimo. iter_estocastico &lt;- descenso_estocástico(30, rep(0, 4), 0.1, minilotes) %&gt;% data.frame %&gt;% rename(beta_0 = X1, beta_1 = X2, beta_2 = X3) ggplot(iteraciones_descenso, aes(x=beta_1, y=beta_2)) + geom_path() + geom_point() + geom_path(data = iter_estocastico, colour =&#39;red&#39;, alpha=0.5) + geom_point(data = iter_estocastico, colour =&#39;red&#39;, alpha=0.5) Podemos ver cómo se ve la devianza de entrenamiento: dev_ent &lt;- devianza_calc(x = as.matrix(dat_ent[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_ent$g) dev_valid &lt;- devianza_calc(x = as.matrix(dat_valid[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_valid$g) dat_dev &lt;- data_frame(iteracion = 1:nrow(iteraciones_descenso)) %&gt;% mutate(descenso = apply(iteraciones_descenso, 1, dev_ent), descenso_estocastico = apply(iter_estocastico, 1, dev_ent)) %&gt;% gather(algoritmo, dev_ent, -iteracion) %&gt;% mutate(tipo =&#39;entrenamiento&#39;) dat_dev_valid &lt;- data_frame(iteracion = 1:nrow(iteraciones_descenso)) %&gt;% mutate(descenso = apply(iteraciones_descenso, 1, dev_valid), descenso_estocastico = apply(iter_estocastico, 1, dev_valid)) %&gt;% gather(algoritmo, dev_ent, -iteracion) %&gt;% mutate(tipo =&#39;validación&#39;) dat_dev &lt;- bind_rows(dat_dev, dat_dev_valid) ggplot(filter(dat_dev, tipo==&#39;entrenamiento&#39;), aes(x=iteracion, y=dev_ent, colour=algoritmo)) + geom_line() + geom_point() + facet_wrap(~tipo) y vemos que descenso estocástico también converge a una buena solución. 8.2 Algoritmo de descenso estocástico Descenso estocástico. Separamos al azar los datos de entrenamiento en \\(n\\) minilotes de tamaño \\(m\\). Para épocas \\(e =1,2,\\ldots, n_e\\) Calcular el gradiente sobre el minilote y hacer actualización, sucesivamente para cada uno de los minilotes \\(k=1,2,\\ldots, n/m\\): \\[\\beta_{i+1} = \\beta_{i} - \\eta\\sum_{j=1}^m \\nabla D^{(k)}_j (\\beta_i)\\] donde \\(D^{(k)}_j (\\beta_i)\\) es la devianza para el \\(j\\)-ésimo caso del minilote \\(k\\). Repetir para la siguiente época (opcional: reordenar antes al azar los minibatches, para evitar ciclos). 8.3 ¿Por qué usar descenso estocástico por minilotes? Las propiedades importantes de descenso estocástico son: Muchas veces no es necesario usar todos los datos para encontrar una buena dirección de descenso. Podemos ver la dirección de descenso en gradiente como un valor esperado sobre la muestra de entrenamiento (pues la pérdida es un promedio sobre el conjunto de entrenamiento). Una submuestra (minilote) puede ser suficiente para estimar ese valor esperado, con costo menor de cómputo. Adicionalmente, quizá no es tan buena idea intentar estimar el gradiente con la mejor precisión pues es solamente una dirección de descenso local (así que quizá no da la mejor decisión de a dónde moverse en cada punto). Es mejor hacer iteraciones más rápidas con direcciones estimadas. Desde este punto de vista, calcular el gradiente completo para descenso en gradiente es computacionalmente ineficiente. Si el conjunto de entrenamiento es masivo, descenso en gradiente no es factible. ¿Cuál es el mejor tamaño de minilote? Por un lado, minilotes más grandes nos dan mejores eficiencias en paralelización (multiplicación de matrices), especialmente en GPUs. Por otro lado, con minilotes más grandes puede ser que hagamos trabajo de más, por las razones expuestas en los incisos anteriores, y tengamos menos iteraciones en el mismo tiempo. El mejor punto está entre minilotes demasiado chicos (no aprovechamos paralelismo) o demasiado grande (hacemos demasiado trabajo por iteración). 4.La propiedad más importante de descenso estocástico en minilotes es que su convergencia no depende del tamaño del conjunto de entrenamiento, es decir, el tiempo de iteración para descenso estocástico no crece con el número de casos totales. Podemos tener obtener buenos ajustes incluso con tamaños muy grandes de conjuntos de entrenamiento (por ejemplo, antes de procesar todos los datos de entrenamiento). Descenso estocástico escala bien en este sentido: el factor limitante es el tamaño de minilote y el número de iteraciones. Es importante permutar al azar los datos antes de hacer los minibatches, pues órdenes naturales en los datos pueden afectar la convergencia. Se ha observado también que permutar los minibatches en cada iteración típicamente acelera la convergencia (si se pueden tener los datos en memoria). Ejemplo En el ejemplo anterior nota que las direcciones de descenso de descenso estocástico son muy razonables (punto 1). Nota también que obtenemos una buena aproximación a la solución con menos cómputo (punto 2 - mismo número de iteraciones, pero cada iteración con un minilote). ggplot(filter(dat_dev, iteracion &gt;= 1), aes(x=iteracion, y=dev_ent, colour=algoritmo)) + geom_line() + geom_point(size=0.5)+ facet_wrap(~tipo, ncol=1) 8.4 Escogiendo la tasa de aprendizaje Para escoger la tasa, monitoreamos las curvas de error de entrenamiento y de validación. Si la tasa es muy grande, habrá oscilaciones grandes y muchas veces incrementos grandes en la función objectivo (error de entrenamiento). Algunas oscilaciones suaves no tienen problema -es la naturaleza estocástica del algoritmo. Si la tasa es muy baja, el aprendizaje es lento y podemos quedarnos en un valor demasiado alto. Conviene monitorear las primeras iteraciones y escoger una tasa más alta que la mejor que tengamos acutalmente, pero no tan alta que cause inestabilidad. Una gráfica como la siguiente es útil. En este ejemplo, incluso podríamos detenernos antes para evitar el sobreajuste de la última parte de las iteraciones: ggplot(filter(dat_dev, algoritmo==&#39;descenso_estocastico&#39;), aes(x=iteracion, y=dev_ent, colour=tipo)) + geom_line() + geom_point() Por ejemplo: tasa demasiado alta: iter_estocastico &lt;- descenso_estocástico(20, rep(0,4), 0.95, minilotes) %&gt;% data.frame %&gt;% rename(beta_0 = X1, beta_1 = X2) dev_ent &lt;- devianza_calc(x = as.matrix(dat_ent[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_ent$g) dev_valid &lt;- devianza_calc(x = as.matrix(dat_valid[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_valid$g) dat_dev &lt;- data_frame(iteracion = 1:nrow(iter_estocastico)) %&gt;% mutate(entrena = apply(iter_estocastico, 1, dev_ent), validacion = apply(iter_estocastico, 1, dev_valid)) %&gt;% gather(tipo, devianza, entrena:validacion) ggplot(dat_dev, aes(x=iteracion, y=devianza, colour=tipo)) + geom_line() + geom_point() Tasa demasiado chica ( o hacer más iteraciones): iter_estocastico &lt;- descenso_estocástico(20, rep(0,4), 0.01, minilotes) %&gt;% data.frame %&gt;% rename(beta_0 = X1, beta_1 = X2) dev_ent &lt;- devianza_calc(x = as.matrix(dat_ent[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_ent$g) dev_valid &lt;- devianza_calc(x = as.matrix(dat_valid[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_valid$g) dat_dev &lt;- data_frame(iteracion = 1:nrow(iter_estocastico)) %&gt;% mutate(entrena = apply(iter_estocastico, 1, dev_ent), validacion = apply(iter_estocastico, 1, dev_valid)) %&gt;% gather(tipo, devianza, entrena:validacion) ggplot(dat_dev, aes(x=iteracion, y=devianza, colour=tipo)) + geom_line() Para redes neuronales, es importante explorar distintas tasas de aprendizaje, aún cuando no parezca haber oscilaciones grandes o convergencia muy lenta. En algunos casos, si la tasa es demasiado grande, puede ser que el algoritmo llegue a lugares con gradientes cercanos a cero (por ejemplo, por activaciones demasiado grandes) y tenga dificultad para moverse. 8.5 Mejoras al algoritmo de descenso estocástico. 8.5.1 Decaimiento de tasa de aprendizaje Hay muchos algoritmos derivados de descenso estocástico. La primera mejora consiste en reducir gradualmente la tasa de aprendizaje para aprender rápido al principio, pero filtrar el ruido de la estimación de minilotes más adelante en las iteraciones y permitir que el algoritmo se asiente en un mínimo. descenso_estocástico &lt;- function(n_epocas, z_0, eta, minilotes, decaimiento = 0.0){ #minilotes es una lista m &lt;- length(minilotes) z &lt;- matrix(0, m*n_epocas, length(z_0)) z[1, ] &lt;- z_0 for(i in 1:(m*n_epocas-1)){ k &lt;- i %% m + 1 if(i %% m == 0){ #comenzar nueva época y reordenar minilotes al azar minilotes &lt;- minilotes[sample(1:m, m)] } h_deriv &lt;- grad_calc(minilotes[[k]]$x, minilotes[[k]]$y) z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ]) eta &lt;- eta*(1/(1+decaimiento*i)) } z } Y ahora vemos qué pasa con decaimiento: iter_estocastico &lt;- descenso_estocástico(20, c(0,0, 0, 0), 0.3, minilotes, decaimiento = 0.0002) %&gt;% data.frame %&gt;% rename(beta_0 = X1, beta_1 = X2, beta_2 = X3, beta_3 = X4) dev_ent &lt;- devianza_calc(x = as.matrix(dat_ent[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_ent$g) dev_valid &lt;- devianza_calc(x = as.matrix(dat_valid[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_valid$g) dat_dev &lt;- data_frame(iteracion = 1:nrow(iter_estocastico)) %&gt;% mutate(entrena = apply(iter_estocastico, 1, dev_ent), validacion = apply(iter_estocastico, 1, dev_valid)) %&gt;% gather(tipo, devianza, entrena:validacion) ggplot(filter(dat_dev, iteracion&gt;1), aes(x=iteracion, y=devianza, colour=tipo)) + geom_line() + geom_point() ggplot(iteraciones_descenso, aes(x=beta_1, y=beta_2)) + geom_path() + geom_point() + geom_path(data = iter_estocastico, colour =&#39;red&#39;, alpha=0.5) + geom_point(data = iter_estocastico, colour =&#39;red&#39;, alpha=0.5) La tasa de aprendizaje es uno de los parámetros en redes neuronales más importantes de afinar. Generalmente se empieza con una tasa de aprendizaje con un valor bajo (0.01, o 0.1), pero es necesario experimentar. Un valor muy alto puede provocar oscilaciones muy fuertes en la pérdida Un valor alto también puede provocar que el algoritmo se detenga en lugar con función pérdida alta (sobreajusta rápidamente). Un valor demasiado bajo produce convergencia lenta. 8.5.2 Momento También es posible utilizar una idea adicional que acelera la convergencia. La idea es que muchas veces la aleatoriedad del algoritmo puede producir iteraciones en direcciones que no son tan buenas (pues la estimación del gradiente es mala). Esto es parte del algoritmo. Sin embargo, si en varias iteraciones hemos observado movimientos en direcciones consistentes, quizá deberíamos movernos en esas direcciones consistentes, y reducir el peso de la dirección del minilote (que nos puede llevar en una dirección mala). El resultado es un suavizamiento de las curvas de aprendizaje. Esto es similar al movimiento de una canica en una superficie: la dirección de su movimiento está dada en parte por la dirección de descenso (el gradiente) y en parte la velocidad actual de la canica. La canica se mueve en un promedio de estas dos direcciones Descenso estocástico con momento Separamos al azar los datos de entrenamiento en \\(n\\) minilotes de tamaño \\(m\\). Para épocas \\(e =1,2,\\ldots, n_e\\) Calcular el gradiente sobre el minilote y hacer actualización, sucesivamente para cada uno de los minilotes \\(k=1,2,\\ldots, n/m\\): \\[\\beta_{i+1} = \\beta_{i} + v,\\] \\[v= \\alpha v - \\eta\\sum_{j=1}^m \\nabla D^{(k)}_j\\] donde \\(D^{(k)}_j (\\beta_i)\\) es la devianza para el \\(j\\)-ésimo caso del minilote \\(k\\). A \\(v\\) se llama la velocidad Repetir para la siguiente época descenso_estocástico &lt;- function(n_epocas, z_0, eta, minilotes, momento = 0.0, decaimiento = 0.0){ #minilotes es una lista m &lt;- length(minilotes) z &lt;- matrix(0, m*n_epocas, length(z_0)) z[1, ] &lt;- z_0 v &lt;- 0 for(i in 1:(m*n_epocas-1)){ k &lt;- i %% m + 1 if(i %% m == 0){ #comenzar nueva época y reordenar minilotes al azar minilotes &lt;- minilotes[sample(1:m, m)] v &lt;- 0 } h_deriv &lt;- grad_calc(minilotes[[k]]$x, minilotes[[k]]$y) z[i+1, ] &lt;- z[i, ] + v v &lt;- momento*v - eta * h_deriv(z[i, ]) eta &lt;- eta*(1/(1+decaimiento*i)) } z } Y ahora vemos que usando momento el algoritmo es más parecido a descenso en gradiente usual (pues tenemos cierta memoria de direcciones anteriores de descenso): set.seed(231) iter_estocastico &lt;- descenso_estocástico(20, c(0,0, 0, 0), 0.2, minilotes, momento = 0.7, decaimiento = 0.001) %&gt;% data.frame %&gt;% rename(beta_0 = X1, beta_1 = X2, beta_2=X3, beta_3=X4) dev_ent &lt;- devianza_calc(x = as.matrix(dat_ent[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_ent$g) dev_valid &lt;- devianza_calc(x = as.matrix(dat_valid[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_valid$g) dat_dev &lt;- data_frame(iteracion = 1:nrow(iter_estocastico)) %&gt;% mutate(entrena = apply(iter_estocastico, 1, dev_ent), validacion = apply(iter_estocastico, 1, dev_valid)) %&gt;% gather(tipo, devianza, entrena:validacion) ggplot(filter(dat_dev, iteracion &gt; 1), aes(x=iteracion, y=devianza, colour=tipo)) + geom_line() + geom_point() ggplot(iteraciones_descenso, aes(x=beta_1, y=beta_2)) + geom_path() + geom_point() + geom_path(data = iter_estocastico, colour =&#39;red&#39;, alpha=0.5) + geom_point(data = iter_estocastico, colour =&#39;red&#39;, alpha=0.5) Nótese cómo llegamos más rápido a una buena solución (comparado con el ejemplo sin momento). Adicionalmente, error de entrenamiento y validación lucen más suaves, producto de promediar velocidades a lo largo de iteraciones. Valores típicos para momento son 0,0.5,0.9 o 0.99. 8.5.3 Otras variaciones Otras variaciones incluyen usar una tasa adaptativa de aprendizaje por cada parámetro (algoritmos adagrad, rmsprop, adam y adamax), o actualizaciones un poco diferentes (nesterov). Los más comunes son descenso estocástico, descenso estocástico con momento, rmsprop y adam (Capítulo 8 del Deep Learning Book, (Goodfellow, Bengio, and Courville 2016)). 8.6 Ajuste de redes con descenso estocástico library(keras) set.seed(21321) x_ent &lt;- as.matrix(dat_ent[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;)]) x_valid &lt;- as.matrix(dat_valid[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;)]) y_ent &lt;- dat_ent$g y_valid &lt;- dat_valid$g Empezamos con regresión logística (sin capas ocultas), que se escribe y ajusta como sigue: modelo &lt;- keras_model_sequential() modelo %&gt;% layer_dense(units = 1, activation = &#39;sigmoid&#39;, input_shape = c(3)) modelo %&gt;% compile(loss = &#39;binary_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.2, momentum = 0, decay = 0), metrics = c(&#39;accuracy&#39;) ) history &lt;- modelo %&gt;% fit(x_ent, y_ent, epochs = 50, batch_size = 64, verbose = 0, validation_data = list(x_valid, y_valid)) Podemos ver el progreso del algoritmo por época aprendizaje &lt;- as.data.frame(history) ggplot(aprendizaje, aes(x=epoch, y=value, colour=data, group=data)) + facet_wrap(~metric, ncol = 1) + geom_line() + geom_point(size = 0.5) Ver los pesos: get_weights(modelo) ## [[1]] ## [,1] ## [1,] -0.6497720 ## [2,] 0.2021007 ## [3,] 0.2829611 ## ## [[2]] ## [1] 1.673745 Y verificamos que concuerda con la salida de glm: mod_logistico &lt;- glm(g ~ x_1 + x_2+ x_3, data = dat_ent, family = &#39;binomial&#39;) coef(mod_logistico) ## (Intercept) x_1 x_2 x_3 ## 1.8082362 -0.7439627 0.2172971 0.3711973 0.5*mod_logistico$deviance/nrow(dat_ent) ## [1] 0.3925183 Ejemplo Ahora hacemos algunos ejemplos para redes totalmente conexas. Usaremos los datos de reconocimiento de dígitos. library(readr) digitos_entrena &lt;- read_csv(&#39;./datos/zip-train.csv&#39;) digitos_prueba &lt;- read_csv(&#39;./datos/zip-test.csv&#39;) names(digitos_entrena)[1] &lt;- &#39;digito&#39; names(digitos_entrena)[2:257] &lt;- paste0(&#39;pixel_&#39;, 1:256) names(digitos_prueba)[1] &lt;- &#39;digito&#39; names(digitos_prueba)[2:257] &lt;- paste0(&#39;pixel_&#39;, 1:256) dim(digitos_entrena) ## [1] 7291 257 table(digitos_entrena$digito) ## ## 0 1 2 3 4 5 6 7 8 9 ## 1194 1005 731 658 652 556 664 645 542 644 Ponemos el rango entre [0,2] (pixeles positivos) x_train &lt;- digitos_entrena %&gt;% select(contains(&#39;pixel&#39;)) %&gt;% as.matrix + 1 x_train &lt;- x_train x_test &lt;- digitos_prueba %&gt;% select(contains(&#39;pixel&#39;)) %&gt;% as.matrix + 1 x_test &lt;- x_test Usamos codificación dummy: #dim(x_train) &lt;- c(nrow(x_train), 16, 16, 1) #dim(x_test) &lt;- c(nrow(x_test), 16, 16, 1) y_train &lt;- to_categorical(digitos_entrena$digito) y_test &lt;- to_categorical(digitos_prueba$digito) head(y_train) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 0 0 0 0 0 0 1 0 0 0 ## [2,] 0 0 0 0 0 1 0 0 0 0 ## [3,] 0 0 0 0 1 0 0 0 0 0 ## [4,] 0 0 0 0 0 0 0 1 0 0 ## [5,] 0 0 0 1 0 0 0 0 0 0 ## [6,] 0 0 0 0 0 0 1 0 0 0 Y definimos un modelo con 2 capas de 200 unidades cada una y regularización L2. Nótese que usamos softmax en la última capa, que es la función (ver parte de regresión multinomial) cuya salida \\(k\\) está dada por \\[p_k = \\frac{exp(z_k)}{\\sum_j exp(z_j)}\\] donde \\(z=(z_1,\\ldots, z_K)\\) (estas son las combinaciones lineales de las unidades de la capa anterior). modelo_tc &lt;- keras_model_sequential() modelo_tc %&gt;% layer_dense(units = 200, activation = &#39;sigmoid&#39;, kernel_regularizer = regularizer_l2(l = 1e-6), input_shape=256) %&gt;% layer_dense(units = 200, activation = &#39;sigmoid&#39;, kernel_regularizer = regularizer_l2(l = 1e-6)) %&gt;% layer_dense(units = 10, activation = &#39;softmax&#39;, kernel_regularizer = regularizer_l2(l = 1e-6)) modelo_tc %&gt;% compile( loss = &#39;categorical_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.5, momentum = 0.0, decay = 1e-6), metrics = c(&#39;accuracy&#39; ,&#39;categorical_crossentropy&#39;) ) history &lt;- modelo_tc %&gt;% fit( x_train, y_train, epochs = 100, batch_size = 256, verbose = 0, validation_data = list(x_test, y_test) ) score &lt;- modelo_tc %&gt;% evaluate(x_test, y_test) score ## $loss ## [1] 0.2874141 ## ## $acc ## [1] 0.9327354 ## ## $categorical_crossentropy ## [1] 0.2863621 Podemos también intentar con el ejemplo de spam: library(readr) library(tidyr) library(dplyr) spam_entrena &lt;- read_csv(&#39;./datos/spam-entrena.csv&#39;) #%&gt;% sample_n(2000) spam_prueba &lt;- read_csv(&#39;./datos/spam-prueba.csv&#39;) set.seed(293) x_ent &lt;- spam_entrena %&gt;% select(-X1, -spam) %&gt;% as.matrix x_ent_s &lt;- scale(x_ent) x_valid &lt;- spam_prueba %&gt;% select(-X1, -spam) %&gt;% as.matrix x_valid_s &lt;- x_valid %&gt;% scale(center = attr(x_ent_s, &#39;scaled:center&#39;), scale = attr(x_ent_s, &#39;scaled:scale&#39;)) y_ent &lt;- spam_entrena$spam y_valid &lt;- spam_prueba$spam En este caso, intentemos una capa oculta: modelo_tc &lt;- keras_model_sequential() modelo_tc %&gt;% layer_dense(units = 200, activation = &#39;sigmoid&#39;, kernel_regularizer = regularizer_l2(l = 1e-5), input_shape=57) %&gt;% layer_dense(units = 1, activation = &#39;sigmoid&#39;) modelo_tc %&gt;% compile( loss = &#39;binary_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.5, momentum = 0.5), metrics = c(&#39;accuracy&#39;, &#39;binary_crossentropy&#39;) ) history &lt;- modelo_tc %&gt;% fit( x_ent_s, y_ent, epochs = 200, batch_size = 256, verbose = 0, validation_data = list(x_valid_s, y_valid) ) score &lt;- modelo_tc %&gt;% evaluate(x_valid_s, y_valid) tab_confusion &lt;- table(modelo_tc %&gt;% predict_classes(x_valid_s),y_valid) tab_confusion ## y_valid ## 0 1 ## 0 898 67 ## 1 29 540 prop.table(tab_confusion, 2) ## y_valid ## 0 1 ## 0 0.96871629 0.11037891 ## 1 0.03128371 0.88962109 8.7 Activaciones relu Recientemente se ha descubierto (en gran parte empíricamente) que hay una unidad más conveniente para las activaciones de las unidades, en lugar de la función sigmoide Activaciones lineales rectificadas (relu) La función relu es \\[\\begin{equation} h(z) = \\begin{cases} z &amp;\\, z&gt;0\\\\ 0 &amp;\\, z&lt;=0 \\end{cases} \\end{equation}\\] Estas generalmente sustituyen a las unidades sigmoidales en capas ocultas h_relu &lt;- function(z) ifelse(z &gt; 0, z, 0) h_logistica &lt;- function(z) 4/(1+exp(-z)) #mult por 4 para comparar más fácilmente curve(h_relu, -5,5) curve(h_logistica, add=T, col=&#39;red&#39;) La razón del exito de estas activaciones no está del todo clara, aunque generalmente se cita el hecho de que una unidad saturada (valores de entrada muy positivos o muy negativos) es problemática en optimización, y las unidades tienen menos ese problema pues no se saturan para valores positivos. Pregunta: ¿cómo cambiaría el algoritmo de feed-forward con estas unidades? ¿el de back-prop? Ejemplo Veamos el mismo modelo de dos capas de arriba, pero con activaciones relu: modelo_tc &lt;- keras_model_sequential() modelo_tc %&gt;% layer_dense(units = 200, activation = &#39;relu&#39;, kernel_regularizer = regularizer_l2(l = 1e-3), input_shape=256) %&gt;% layer_dense(units = 200, activation = &#39;relu&#39;, kernel_regularizer = regularizer_l2(l = 1e-3)) %&gt;% layer_dense(units = 10, activation = &#39;softmax&#39;, kernel_regularizer = regularizer_l2(l = 1e-3)) modelo_tc %&gt;% compile( loss = &#39;categorical_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.3, momentum = 0.0, decay = 0), metrics = c(&#39;accuracy&#39;, &#39;categorical_crossentropy&#39;) ) history &lt;- modelo_tc %&gt;% fit( x_train, y_train, epochs = 200, batch_size = 256, verbose = 0, validation_data = list(x_test, y_test) ) score &lt;- modelo_tc %&gt;% evaluate(x_test, y_test) score ## $loss ## [1] 0.2932282 ## ## $acc ## [1] 0.9387145 ## ## $categorical_crossentropy ## [1] 0.2114471 8.8 Dropout para regularización Un método más nuevo y exitoso para regularizar es el dropout. Consiste en perturbar la red en cada pasada de entrenamiento de minibatch (feed-forward y backprop), eliminando al azar algunas de las unidades de cada capa. El objeto es que al introducir ruido en el proceso de entrenamiento evitamos sobreajuste, pues en cada paso de la iteración estamos limitando el número de unidades que la red puede usar para ajustar las respuestas. Dropout entonces busca una reducción en el sobreajuste que sea más provechosa que el consecuente aumento en el sesgo. Dropout En cada iteración (minibatch), seleccionamos con cierta probablidad \\(p\\) eliminar cada una de las unidades (independientemente en cada capa, y posiblemente con distintas \\(p\\) en cada capa), es decir, hacemos su salida igual a 0. Hacemos forward-feed y back-propagation poniendo en 0 las unidades eliminadas. Escalar pesos: para predecir (prueba), usamos todas las unidades. Si una unidad tiene peso \\(\\theta\\) en una capa después de entrenar, y la probablidad de que esa capa no se haya hecho 0 es \\(1-p\\), entonces usamos \\((1-p)\\theta\\) como peso para hacer predicciones. Si hacemos dropout de la capa de entrada, generalmente se usan valores chicos alrededor de \\(0.2\\). En capas intermedias se usan generalmente valores más grandes alrededor de \\(0.5\\). Podemos hacer dropout de la capa de entrada. En este caso, estamos evitando que el modelo dependa fuertemente de variables individuales. Por ejemplo, en procesamiento de imágenes, no queremos que por sobreajuste algunas predicciones estén ligadas fuertemente a un solo pixel (aún cuando en entrenamiento puede ser que un pixel separe bien los casos que nos interesa clasificar). Ejemplo: dropout y regularización Consideremos el problema de separar 9 y 3 del resto de dígitos zip. Queremos comparar el desempeño de una red sin y con dropout (tanto de entradas como de capa oculta) y entender parcialmente cómo se comportan los pesos aprendidos: set.seed(29123) entrena_3 &lt;- digitos_entrena %&gt;% sample_n(nrow(digitos_entrena)) %&gt;% sample_n(3000) x_train_3 &lt;- entrena_3 %&gt;% select(-digito) %&gt;% as.matrix + 1 y_train_3 &lt;- (entrena_3$digito %in% c(3,8)) %&gt;% as.numeric set.seed(12) modelo_sin_reg &lt;- keras_model_sequential() modelo_sin_reg %&gt;% layer_dense(units = 30, activation = &#39;relu&#39;, input_shape = 256) %&gt;% layer_dense(units = 1, activation = &#39;sigmoid&#39;) set.seed(12) modelo_dropout &lt;- keras_model_sequential() modelo_dropout %&gt;% layer_reshape(input_shape=256, target_shape=256) %&gt;% layer_dropout(0.5) %&gt;% layer_dense(units = 30, activation = &#39;relu&#39;, input_shape = 256, name = &quot;dense_1&quot;) %&gt;% layer_dropout(0.5) %&gt;% layer_dense(units = 1, activation = &#39;sigmoid&#39;, name=&#39;output&#39;) El modelo sin regularización sobreajusta (nótese que el error de validación comienza a crecer considerablemente muy pronto, hay un margen grande entre entrenamiento y validación, y la pérdida de entrenamiento es cercana a 0): modelo_sin_reg %&gt;% compile(loss = &#39;binary_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.5), metrics = c(&#39;accuracy&#39;) ) history_1 &lt;- modelo_sin_reg %&gt;% fit(x_train_3/2, y_train_3, verbose=0, epochs = 800, batch_size = 256, validation_split = 0.2 ) hist_1 &lt;- as.data.frame(history_1) ggplot(hist_1, aes(x=epoch, y=value, colour=data)) + geom_line() + facet_wrap(~metric, scales = &#39;free&#39;, ncol=1) Y parecen ruidosas las unidades que aprendió en la capa oculta (algunas no aprendieron o aprendieron cosas irrelevantes). En la siguiente imagen, cada pixel es un peso. Cada imagen agrupa los pesos de una unidad, y ordenamos los pesos según la variable de entrada (pixel) al que se multiplican. graf_pesos &lt;- function(pesos, mostrar_facets=FALSE){ pesos_df &lt;- as_tibble(pesos) %&gt;% mutate(pixel = 1:256) %&gt;% mutate(x=(pixel -1) %% 16, y = (pixel-1)%/% 16) %&gt;% gather(unidad, valor, -pixel,-x,-y) %&gt;% mutate(unidad = as.integer(unidad)) %&gt;% mutate(x_grid = (unidad-1) %% 6 + 1, y_grid= (unidad-1) %/% 6 + 1) marco &lt;- expand.grid(x_grid = 1:6, y_grid=1:5) pesos_df &lt;- full_join(marco, pesos_df, by=c(&#39;x_grid&#39;,&#39;y_grid&#39;)) pesos_df$valor[is.na(pesos_df$valor)] &lt;- 0 gplot &lt;- ggplot(pesos_df, aes(x=x,y=-y, fill=valor)) + geom_tile() + facet_grid(x_grid~y_grid) + scale_fill_gradient2(low = &quot;black&quot;, mid=&#39;gray80&#39;, high = &quot;white&quot;) + coord_fixed() if(!mostrar_facets){ gplot &lt;- gplot + theme(strip.background = element_blank(), strip.text = element_blank()) } gplot } pesos &lt;- get_weights(modelo_sin_reg)[[1]] colnames(pesos) &lt;- 1:ncol(pesos) graf_pesos(pesos) Ahora ajustamos el modelo con dropout: modelo_dropout %&gt;% compile(loss = &#39;binary_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.5), metrics = c(&#39;accuracy&#39;) ) history_2 &lt;- modelo_dropout %&gt;% fit(x_train_3/2, y_train_3, verbose = 0, epochs = 800, batch_size = 256, validation_split = 0.2, callbacks = callback_tensorboard(&quot;logs/digits/run_2&quot;, write_images=TRUE), ) hist_2 &lt;- as.data.frame(history_2) ggplot(hist_2, aes(x=epoch, y=value, colour=data)) + geom_line() + facet_wrap(~metric, scales = &#39;free&#39;) El desempeño es mejor, y parecen ser más útiles los patrones que aprendió el capa oculta: pesos &lt;- get_weights(modelo_dropout)[[1]] colnames(pesos) &lt;- 1:ncol(pesos) graf_pesos(pesos) get_weights(modelo_dropout)[[3]] ## [,1] ## [1,] -0.7666185 ## [2,] -0.8057358 ## [3,] 0.4364979 ## [4,] -1.2287319 ## [5,] 0.6291505 ## [6,] -1.3679714 ## [7,] 0.9451178 ## [8,] -0.7824966 ## [9,] -0.6418459 ## [10,] -1.4190654 ## [11,] 0.8667771 ## [12,] 0.9389005 ## [13,] 0.9900619 ## [14,] 0.2021778 ## [15,] 0.4089233 ## [16,] 0.6988112 ## [17,] -1.0348938 ## [18,] -0.7507455 ## [19,] -0.6935381 ## [20,] 0.5527661 ## [21,] 0.4700725 ## [22,] -1.0086707 ## [23,] -0.6662267 ## [24,] 0.8343580 ## [25,] -0.8437275 ## [26,] -0.9349431 ## [27,] 0.9978455 ## [28,] -1.3275293 ## [29,] 0.7853740 ## [30,] 0.6147138 ¿Cuáles de estas unidades tienen peso positivo y negativo en la capa final? pesos_capa_f &lt;- get_weights(modelo_dropout)[[3]] graf_pesos(pesos[, pesos_capa_f &gt; 0], mostrar_facets = TRUE) ## Warning: Removed 15 rows containing missing values (geom_tile). graf_pesos(pesos[, pesos_capa_f &lt; -0], mostrar_facets = TRUE) ## Warning: Removed 15 rows containing missing values (geom_tile). Veamos cómo se activan distintas unidades con diferentes entradas: indices &lt;- c(10, 28, 3, 29, 16) entrena_3$digito[indices] ## [1] 3 5 7 1 6 dense_layer &lt;- keras_model(inputs = modelo_dropout$input, outputs = get_layer(modelo_dropout, &#39;dense_1&#39;)$output) dense_output &lt;- predict(dense_layer, x_train_3[indices, , drop=FALSE]) dense_t &lt;- t(dense_output) dense_t ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.000000 0.000000 0.8891668 0.000000 6.6341295 ## [2,] 0.000000 4.305577 0.0000000 2.811944 2.3169327 ## [3,] 4.108176 0.000000 0.0000000 0.000000 0.0000000 ## [4,] 0.000000 0.000000 3.3775349 0.000000 0.0000000 ## [5,] 1.659972 0.000000 0.0000000 0.000000 0.0000000 ## [6,] 0.000000 0.000000 4.0188308 2.528392 0.0000000 ## [7,] 0.000000 0.000000 0.0000000 0.000000 0.0000000 ## [8,] 0.000000 0.000000 2.0561087 0.000000 2.4333086 ## [9,] 0.000000 2.649631 0.0000000 0.000000 6.8841310 ## [10,] 0.000000 0.000000 4.8670650 2.356164 0.0000000 ## [11,] 0.000000 0.000000 0.0000000 0.000000 0.0000000 ## [12,] 2.173050 0.000000 0.0000000 0.000000 0.0000000 ## [13,] 0.000000 0.000000 0.0000000 0.000000 0.0000000 ## [14,] 0.000000 0.000000 0.0000000 0.000000 0.0000000 ## [15,] 0.000000 0.000000 0.0000000 0.000000 0.0000000 ## [16,] 0.000000 0.000000 0.0000000 0.000000 0.0000000 ## [17,] 0.000000 0.000000 0.0000000 2.458321 0.0000000 ## [18,] 0.000000 2.152258 0.0000000 2.250175 5.8062367 ## [19,] 0.000000 5.639275 0.0000000 3.325603 5.4370017 ## [20,] 1.626022 0.000000 0.0000000 0.000000 0.0000000 ## [21,] 2.837990 0.000000 0.0000000 0.000000 0.0000000 ## [22,] 0.000000 1.807391 0.0000000 1.804571 4.4407167 ## [23,] 0.000000 0.000000 0.0000000 0.000000 4.1352553 ## [24,] 2.403963 0.000000 0.0000000 0.000000 0.0000000 ## [25,] 0.000000 0.000000 3.9490790 0.000000 0.0000000 ## [26,] 0.000000 0.000000 1.9975677 0.000000 0.8100501 ## [27,] 0.000000 0.000000 0.0000000 0.000000 0.0000000 ## [28,] 0.000000 0.000000 4.0328112 1.715810 0.0000000 ## [29,] 0.000000 0.000000 0.0000000 0.000000 0.0000000 ## [30,] 2.188777 0.000000 0.0000000 0.000000 0.0000000 graf_pesos(pesos[, dense_t[ ,1] &gt; 0], mostrar_facets = TRUE) graf_pesos(pesos[, dense_t[ ,2] &gt; 0], mostrar_facets = TRUE) graf_pesos(pesos[, dense_t[ ,3] &gt; 0], mostrar_facets = TRUE) graf_pesos(pesos[, dense_t[ ,4] &gt; 0], mostrar_facets = TRUE) Comentarios adicionales Algunas maneras en que podemos pensar en la regularización de dropout: Dropout busca que cada unidad calcule algo importante por sí sola, y dependa menos de otras unidades para hacer algo útil. Algunas unidades y pesos pueden acoplarse fuertemente (y de manera compleja) para hacer las predicciones. Si estas unidades aprendieron ese acoplamento demasiado fuerte para el conjunto de entrenamiento, entonces puede ser nuevos datos, con perturbaciones, puedan producir predicciones malas (mala generalización). Con dropout buscamos que la unidades capturen información útil en general, no necesariamente en acoplamiento fuerte con otras unidades. Podemos pensar que en cada pasada de minibatch, escogemos una arquitectura diferente, y entrenamos. El resultado final será entonces es un tipo de promedio de todas esas arquitecturas que probamos. Este promedio reduce varianza de las salidas de las unidades. El paso de escalamiento es importante para el funcionamiento correcto del método. La idea intuitiva es que el peso de una unidad es 0 con probabilidad \\(p\\) y \\(\\theta\\) con probabilidad \\(1-p\\). Tomamos el valor esperado como peso para la red completa, que es \\(p0+(1-p)\\theta\\). Ver (Srivastava et al. 2014) Ejemplo Experimenta en este ejemplo con distintos valores de dropout, y verifica intuitivamente sus efectos de regularización (ve las curvas de aprendizaje). modelo_tc &lt;- keras_model_sequential() modelo_tc %&gt;% layer_reshape(input_shape=256, target_shape=256) %&gt;% layer_dropout(rate=0.2) %&gt;% layer_dense(units = 200, activation = &#39;relu&#39;) %&gt;% layer_dropout(rate = 0.5) %&gt;% layer_dense(units = 200, activation = &#39;relu&#39;) %&gt;% layer_dropout(rate = 0.5) %&gt;% layer_dense(units = 10, activation = &#39;softmax&#39;, kernel_regularizer = regularizer_l2(l = 1e-4)) modelo_tc %&gt;% compile( loss = &#39;categorical_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.3, momentum = 0.5, decay = 0.0001), metrics = c(&#39;accuracy&#39;, &#39;categorical_crossentropy&#39;) ) history &lt;- modelo_tc %&gt;% fit( x_train, y_train, epochs = 100, batch_size = 256, validation_data = list(x_test, y_test) ) score &lt;- modelo_tc %&gt;% evaluate(x_test, y_test) score ## $loss ## [1] 0.222676 ## ## $acc ## [1] 0.9546587 ## ## $categorical_crossentropy ## [1] 0.2186604 "]
]
