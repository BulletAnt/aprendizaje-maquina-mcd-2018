<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Aprendizaje de máquina</title>
  <meta name="description" content="Notas y material para el curso de aprendizaje de máquina (ITAM)">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Aprendizaje de máquina" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notas y material para el curso de aprendizaje de máquina (ITAM)" />
  <meta name="github-repo" content="felipegonzalez/aprendizaje-maquina-mcd" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Aprendizaje de máquina" />
  
  <meta name="twitter:description" content="Notas y material para el curso de aprendizaje de máquina (ITAM)" />
  

<meta name="author" content="Felipe González">


<meta name="date" content="2018-12-03">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="regresion.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
<link rel="stylesheet" href="css/font-awesome.min.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Aprendizaje Máquina</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Temario y referencias</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#evaluacion"><i class="fa fa-check"></i>Evaluación</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-r-y-rstudio"><i class="fa fa-check"></i>Software: R y Rstudio</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#referencias-principales"><i class="fa fa-check"></i>Referencias principales</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#otras-referencias"><i class="fa fa-check"></i>Otras referencias</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduccion.html"><a href="introduccion.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="introduccion.html"><a href="introduccion.html#que-es-aprendizaje-de-maquina-machine-learning"><i class="fa fa-check"></i><b>1.1</b> ¿Qué es aprendizaje de máquina (machine learning)?</a></li>
<li class="chapter" data-level="1.2" data-path="introduccion.html"><a href="introduccion.html#aprendizaje-supervisado-1"><i class="fa fa-check"></i><b>1.2</b> Aprendizaje Supervisado</a><ul>
<li class="chapter" data-level="" data-path="introduccion.html"><a href="introduccion.html#proceso-generador-de-datos-modelo-teorico"><i class="fa fa-check"></i>Proceso generador de datos (modelo teórico)</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduccion.html"><a href="introduccion.html#predicciones"><i class="fa fa-check"></i><b>1.3</b> Predicciones</a></li>
<li class="chapter" data-level="1.4" data-path="introduccion.html"><a href="introduccion.html#aprendizaje"><i class="fa fa-check"></i><b>1.4</b> Tarea de aprendizaje supervisado</a></li>
<li class="chapter" data-level="1.5" data-path="introduccion.html"><a href="introduccion.html#error"><i class="fa fa-check"></i><b>1.5</b> Balance de complejidad y rigidez</a></li>
<li class="chapter" data-level="1.6" data-path="introduccion.html"><a href="introduccion.html#como-estimar-f"><i class="fa fa-check"></i><b>1.6</b> ¿Cómo estimar f?</a></li>
<li class="chapter" data-level="1.7" data-path="introduccion.html"><a href="introduccion.html#resumen"><i class="fa fa-check"></i><b>1.7</b> Resumen</a></li>
<li class="chapter" data-level="1.8" data-path="introduccion.html"><a href="introduccion.html#tarea"><i class="fa fa-check"></i><b>1.8</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regresion.html"><a href="regresion.html"><i class="fa fa-check"></i><b>2</b> Regresión lineal</a><ul>
<li class="chapter" data-level="2.1" data-path="introduccion.html"><a href="introduccion.html#introduccion"><i class="fa fa-check"></i><b>2.1</b> Introducción</a></li>
<li class="chapter" data-level="2.2" data-path="regresion.html"><a href="regresion.html#aprendizaje-de-coeficientes-ajuste"><i class="fa fa-check"></i><b>2.2</b> Aprendizaje de coeficientes (ajuste)</a></li>
<li class="chapter" data-level="2.3" data-path="regresion.html"><a href="regresion.html#descenso-en-gradiente"><i class="fa fa-check"></i><b>2.3</b> Descenso en gradiente</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regresion.html"><a href="regresion.html#seleccion-de-tamano-de-paso-eta"><i class="fa fa-check"></i><b>2.3.1</b> Selección de tamaño de paso <span class="math inline">\(\eta\)</span></a></li>
<li class="chapter" data-level="2.3.2" data-path="regresion.html"><a href="regresion.html#funciones-de-varias-variables"><i class="fa fa-check"></i><b>2.3.2</b> Funciones de varias variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regresion.html"><a href="regresion.html#descenso-en-gradiente-para-regresion-lineal"><i class="fa fa-check"></i><b>2.4</b> Descenso en gradiente para regresión lineal</a></li>
<li class="chapter" data-level="2.5" data-path="regresion.html"><a href="regresion.html#normalizacion-de-entradas"><i class="fa fa-check"></i><b>2.5</b> Normalización de entradas</a></li>
<li class="chapter" data-level="2.6" data-path="regresion.html"><a href="regresion.html#interpretacion-de-modelos-lineales"><i class="fa fa-check"></i><b>2.6</b> Interpretación de modelos lineales</a></li>
<li class="chapter" data-level="2.7" data-path="regresion.html"><a href="regresion.html#solucion-analitica"><i class="fa fa-check"></i><b>2.7</b> Solución analítica</a></li>
<li class="chapter" data-level="2.8" data-path="regresion.html"><a href="regresion.html#por-que-el-modelo-lineal-funciona-bien-muchas-veces"><i class="fa fa-check"></i><b>2.8</b> ¿Por qué el modelo lineal funciona bien (muchas veces)?</a><ul>
<li class="chapter" data-level="2.8.1" data-path="regresion.html"><a href="regresion.html#k-vecinos-mas-cercanos"><i class="fa fa-check"></i><b>2.8.1</b> k vecinos más cercanos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regresion.html"><a href="regresion.html#tarea-1"><i class="fa fa-check"></i>Tarea</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="logistica.html"><a href="logistica.html"><i class="fa fa-check"></i><b>3</b> Regresión logística</a><ul>
<li class="chapter" data-level="3.1" data-path="logistica.html"><a href="logistica.html#el-problema-de-clasificacion"><i class="fa fa-check"></i><b>3.1</b> El problema de clasificación</a><ul>
<li class="chapter" data-level="" data-path="logistica.html"><a href="logistica.html#que-estimar-en-problemas-de-clasificacion"><i class="fa fa-check"></i>¿Qué estimar en problemas de clasificación?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="logistica.html"><a href="logistica.html#estimacion-de-probabilidades-de-clase"><i class="fa fa-check"></i><b>3.2</b> Estimación de probabilidades de clase</a><ul>
<li class="chapter" data-level="" data-path="logistica.html"><a href="logistica.html#ejemplo-9"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="3.2.1" data-path="logistica.html"><a href="logistica.html#k-vecinos-mas-cercanos-1"><i class="fa fa-check"></i><b>3.2.1</b> k-vecinos más cercanos</a></li>
<li class="chapter" data-level="" data-path="logistica.html"><a href="logistica.html#ejemplo-11"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="logistica.html"><a href="logistica.html#error-para-modelos-de-clasificacion"><i class="fa fa-check"></i><b>3.3</b> Error para modelos de clasificación</a><ul>
<li class="chapter" data-level="3.3.1" data-path="logistica.html"><a href="logistica.html#ejercicio-1"><i class="fa fa-check"></i><b>3.3.1</b> Ejercicio</a></li>
<li class="chapter" data-level="3.3.2" data-path="logistica.html"><a href="logistica.html#error-de-clasificacion-y-funcion-de-perdida-0-1"><i class="fa fa-check"></i><b>3.3.2</b> Error de clasificación y función de pérdida 0-1</a></li>
<li class="chapter" data-level="3.3.3" data-path="logistica.html"><a href="logistica.html#discusion-relacion-entre-devianza-y-error-de-clasificacion"><i class="fa fa-check"></i><b>3.3.3</b> Discusión: relación entre devianza y error de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="logistica.html"><a href="logistica.html#regresion-logistica"><i class="fa fa-check"></i><b>3.4</b> Regresión logística</a><ul>
<li class="chapter" data-level="3.4.1" data-path="logistica.html"><a href="logistica.html#regresion-logistica-simple"><i class="fa fa-check"></i><b>3.4.1</b> Regresión logística simple</a></li>
<li class="chapter" data-level="3.4.2" data-path="logistica.html"><a href="logistica.html#funcion-logistica"><i class="fa fa-check"></i><b>3.4.2</b> Función logística</a></li>
<li class="chapter" data-level="3.4.3" data-path="logistica.html"><a href="logistica.html#regresion-logistica-1"><i class="fa fa-check"></i><b>3.4.3</b> Regresión logística</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="logistica.html"><a href="logistica.html#aprendizaje-de-coeficientes-para-regresion-logistica-binomial."><i class="fa fa-check"></i><b>3.5</b> Aprendizaje de coeficientes para regresión logística (binomial).</a></li>
<li class="chapter" data-level="3.6" data-path="logistica.html"><a href="logistica.html#ejercicio-datos-de-diabetes"><i class="fa fa-check"></i><b>3.6</b> Ejercicio: datos de diabetes</a><ul>
<li class="chapter" data-level="" data-path="logistica.html"><a href="logistica.html#tarea-2"><i class="fa fa-check"></i>Tarea</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html"><i class="fa fa-check"></i><b>4</b> Más sobre problemas de clasificación</a><ul>
<li class="chapter" data-level="4.1" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#analisis-de-error-para-clasificadores-binarios"><i class="fa fa-check"></i><b>4.1</b> Análisis de error para clasificadores binarios</a><ul>
<li class="chapter" data-level="" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#medidas-resumen-de-desempeno"><i class="fa fa-check"></i>Medidas resumen de desempeño</a></li>
<li class="chapter" data-level="" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#interpetacion-de-resumenes-de-desempeno-y-tasas-base"><i class="fa fa-check"></i>Interpetación de resúmenes de desempeño y tasas base</a></li>
<li class="chapter" data-level="4.1.1" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#puntos-de-corte-para-un-clasificador-binario"><i class="fa fa-check"></i><b>4.1.1</b> Puntos de corte para un clasificador binario</a></li>
<li class="chapter" data-level="4.1.2" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#espacio-roc-de-clasificadores"><i class="fa fa-check"></i><b>4.1.2</b> Espacio ROC de clasificadores</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#perfil-de-un-clasificador-binario-y-curvas-roc"><i class="fa fa-check"></i><b>4.2</b> Perfil de un clasificador binario y curvas ROC</a></li>
<li class="chapter" data-level="4.3" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#regresion-logistica-para-problemas-de-mas-de-2-clases"><i class="fa fa-check"></i><b>4.3</b> Regresión logística para problemas de más de 2 clases</a><ul>
<li class="chapter" data-level="4.3.1" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#regresion-logistica-multinomial"><i class="fa fa-check"></i><b>4.3.1</b> Regresión logística multinomial</a></li>
<li class="chapter" data-level="4.3.2" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#interpretacion-de-coeficientes"><i class="fa fa-check"></i><b>4.3.2</b> Interpretación de coeficientes</a></li>
<li class="chapter" data-level="4.3.3" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#ejemplo-clasificacion-de-digitos-con-regresion-multinomial"><i class="fa fa-check"></i><b>4.3.3</b> Ejemplo: Clasificación de dígitos con regresión multinomial</a></li>
<li class="chapter" data-level="" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#discusion"><i class="fa fa-check"></i>Discusión</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#descenso-en-gradiente-para-regresion-multinomial-logistica"><i class="fa fa-check"></i><b>4.4</b> Descenso en gradiente para regresión multinomial logística</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regularizacion.html"><a href="regularizacion.html"><i class="fa fa-check"></i><b>5</b> Regularización</a><ul>
<li class="chapter" data-level="5.1" data-path="regularizacion.html"><a href="regularizacion.html#sesgo-y-varianza-de-predictores"><i class="fa fa-check"></i><b>5.1</b> Sesgo y varianza de predictores</a><ul>
<li class="chapter" data-level="5.1.1" data-path="regularizacion.html"><a href="regularizacion.html#sesgo-y-varianza-en-modelos-lineales"><i class="fa fa-check"></i><b>5.1.1</b> Sesgo y varianza en modelos lineales</a></li>
<li class="chapter" data-level="5.1.2" data-path="regularizacion.html"><a href="regularizacion.html#reduciendo-varianza-de-los-coeficientes"><i class="fa fa-check"></i><b>5.1.2</b> Reduciendo varianza de los coeficientes</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="regularizacion.html"><a href="regularizacion.html#regularizacion-ridge"><i class="fa fa-check"></i><b>5.2</b> Regularización ridge</a><ul>
<li class="chapter" data-level="5.2.1" data-path="regularizacion.html"><a href="regularizacion.html#seleccion-de-coeficiente-de-regularizacion"><i class="fa fa-check"></i><b>5.2.1</b> Selección de coeficiente de regularización</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="regularizacion.html"><a href="regularizacion.html#entrenamiento-validacion-y-prueba"><i class="fa fa-check"></i><b>5.3</b> Entrenamiento, Validación y Prueba</a><ul>
<li class="chapter" data-level="5.3.1" data-path="regularizacion.html"><a href="regularizacion.html#validacion-cruzada"><i class="fa fa-check"></i><b>5.3.1</b> Validación cruzada</a></li>
<li class="chapter" data-level="5.3.2" data-path="regularizacion.html"><a href="regularizacion.html#como-se-desempena-validacion-cruzada-como-estimacion-del-error"><i class="fa fa-check"></i><b>5.3.2</b> ¿Cómo se desempeña validación cruzada como estimación del error?</a></li>
<li class="chapter" data-level="" data-path="regularizacion.html"><a href="regularizacion.html#ejercicio-4"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="regularizacion.html"><a href="regularizacion.html#regularizacion-lasso"><i class="fa fa-check"></i><b>5.4</b> Regularización lasso</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html"><i class="fa fa-check"></i><b>6</b> Extensiones para regresión lineal y logística</a><ul>
<li class="chapter" data-level="6.1" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#como-hacer-mas-flexible-el-modelo-lineal"><i class="fa fa-check"></i><b>6.1</b> Cómo hacer más flexible el modelo lineal</a></li>
<li class="chapter" data-level="6.2" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#transformacion-de-entradas"><i class="fa fa-check"></i><b>6.2</b> Transformación de entradas</a></li>
<li class="chapter" data-level="6.3" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#variables-cualitativas"><i class="fa fa-check"></i><b>6.3</b> Variables cualitativas</a></li>
<li class="chapter" data-level="6.4" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#interacciones"><i class="fa fa-check"></i><b>6.4</b> Interacciones</a></li>
<li class="chapter" data-level="6.5" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#categorizacion-de-variables"><i class="fa fa-check"></i><b>6.5</b> Categorización de variables</a></li>
<li class="chapter" data-level="6.6" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#splines-opcional"><i class="fa fa-check"></i><b>6.6</b> Splines (opcional)</a></li>
<li class="chapter" data-level="6.7" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#modelando-en-escala-logaritmica"><i class="fa fa-check"></i><b>6.7</b> Modelando en escala logarítmica</a><ul>
<li class="chapter" data-level="6.7.1" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#cuando-usar-estas-tecnicas"><i class="fa fa-check"></i><b>6.7.1</b> ¿Cuándo usar estas técnicas?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html"><i class="fa fa-check"></i><b>7</b> Redes neuronales (parte 1)</a><ul>
<li class="chapter" data-level="7.1" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#introduccion-a-redes-neuronales"><i class="fa fa-check"></i><b>7.1</b> Introducción a redes neuronales</a><ul>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#como-construyen-entradas-las-redes-neuronales"><i class="fa fa-check"></i>¿Cómo construyen entradas las redes neuronales?</a></li>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#como-ajustar-los-parametros"><i class="fa fa-check"></i>¿Cómo ajustar los parámetros?</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#interacciones-en-redes-neuronales"><i class="fa fa-check"></i><b>7.2</b> Interacciones en redes neuronales</a></li>
<li class="chapter" data-level="7.3" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#calculo-en-redes-feed-forward"><i class="fa fa-check"></i><b>7.3</b> Cálculo en redes: feed-forward</a></li>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#notacion"><i class="fa fa-check"></i>Notación</a></li>
<li class="chapter" data-level="7.4" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#feed-forward"><i class="fa fa-check"></i><b>7.4</b> Feed forward</a></li>
<li class="chapter" data-level="7.5" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#backpropagation-calculo-del-gradiente-clasificacion-binaria"><i class="fa fa-check"></i><b>7.5</b> Backpropagation: cálculo del gradiente (clasificación binaria)</a><ul>
<li class="chapter" data-level="7.5.1" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#calculo-para-un-caso-de-entrenamiento"><i class="fa fa-check"></i><b>7.5.1</b> Cálculo para un caso de entrenamiento</a></li>
<li class="chapter" data-level="7.5.2" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#algoritmo-de-backpropagation"><i class="fa fa-check"></i><b>7.5.2</b> Algoritmo de backpropagation</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#ajuste-de-parametros-introduccion"><i class="fa fa-check"></i><b>7.6</b> Ajuste de parámetros (introducción)</a><ul>
<li class="chapter" data-level="7.6.1" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#ejemplo-35"><i class="fa fa-check"></i><b>7.6.1</b> Ejemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html"><i class="fa fa-check"></i><b>8</b> Redes neuronales (parte 2)</a><ul>
<li class="chapter" data-level="8.1" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#descenso-estocastico"><i class="fa fa-check"></i><b>8.1</b> Descenso estocástico</a></li>
<li class="chapter" data-level="8.2" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#algoritmo-de-descenso-estocastico"><i class="fa fa-check"></i><b>8.2</b> Algoritmo de descenso estocástico</a></li>
<li class="chapter" data-level="8.3" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#por-que-usar-descenso-estocastico-por-minilotes"><i class="fa fa-check"></i><b>8.3</b> ¿Por qué usar descenso estocástico por minilotes?</a></li>
<li class="chapter" data-level="8.4" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#escogiendo-la-tasa-de-aprendizaje"><i class="fa fa-check"></i><b>8.4</b> Escogiendo la tasa de aprendizaje</a></li>
<li class="chapter" data-level="8.5" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#mejoras-al-algoritmo-de-descenso-estocastico."><i class="fa fa-check"></i><b>8.5</b> Mejoras al algoritmo de descenso estocástico.</a><ul>
<li class="chapter" data-level="8.5.1" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#decaimiento-de-tasa-de-aprendizaje"><i class="fa fa-check"></i><b>8.5.1</b> Decaimiento de tasa de aprendizaje</a></li>
<li class="chapter" data-level="8.5.2" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#momento"><i class="fa fa-check"></i><b>8.5.2</b> Momento</a></li>
<li class="chapter" data-level="8.5.3" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#otras-variaciones"><i class="fa fa-check"></i><b>8.5.3</b> Otras variaciones</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#ajuste-de-redes-con-descenso-estocastico"><i class="fa fa-check"></i><b>8.6</b> Ajuste de redes con descenso estocástico</a></li>
<li class="chapter" data-level="8.7" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#activaciones-relu"><i class="fa fa-check"></i><b>8.7</b> Activaciones relu</a></li>
<li class="chapter" data-level="8.8" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#dropout-para-regularizacion"><i class="fa fa-check"></i><b>8.8</b> Dropout para regularización</a><ul>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#ejemplo-39"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#ajuste-de-hiperparametros"><i class="fa fa-check"></i><b>8.9</b> Ajuste de hiperparámetros</a><ul>
<li class="chapter" data-level="8.9.1" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#ajuste-manual-de-hiperparametros"><i class="fa fa-check"></i><b>8.9.1</b> Ajuste Manual de Hiperparámetros</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#ajuste-automatico"><i class="fa fa-check"></i><b>8.10</b> Ajuste automático</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html"><i class="fa fa-check"></i><b>9</b> Redes convolucionales</a><ul>
<li class="chapter" data-level="9.1" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#filtros-convolucionales"><i class="fa fa-check"></i><b>9.1</b> Filtros convolucionales</a><ul>
<li class="chapter" data-level="" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#filtros-en-una-dimension"><i class="fa fa-check"></i>Filtros en una dimensión</a></li>
<li class="chapter" data-level="" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#filtros-convolucionales-en-dos-dimensiones"><i class="fa fa-check"></i>Filtros convolucionales en dos dimensiones</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#filtros-convolucionales-para-redes-neuronales"><i class="fa fa-check"></i><b>9.2</b> Filtros convolucionales para redes neuronales</a></li>
<li class="chapter" data-level="9.3" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#capas-de-agregacion-pooling"><i class="fa fa-check"></i><b>9.3</b> Capas de agregación (pooling)</a></li>
<li class="chapter" data-level="9.4" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#ejemplo-arquitectura-lenet"><i class="fa fa-check"></i><b>9.4</b> Ejemplo (arquitectura LeNet):</a><ul>
<li class="chapter" data-level="" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#conteo-de-parametros"><i class="fa fa-check"></i>Conteo de parámetros</a></li>
<li class="chapter" data-level="" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#pesos-y-activaciones"><i class="fa fa-check"></i>Pesos y activaciones</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html"><i class="fa fa-check"></i><b>10</b> Diagnóstico y mejora de modelos</a><ul>
<li class="chapter" data-level="10.1" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#aspectos-generales"><i class="fa fa-check"></i><b>10.1</b> Aspectos generales</a></li>
<li class="chapter" data-level="10.2" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#que-hacer-cuando-el-desempeno-no-es-satisfactorio"><i class="fa fa-check"></i><b>10.2</b> ¿Qué hacer cuando el desempeño no es satisfactorio?</a></li>
<li class="chapter" data-level="10.3" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#pipeline-de-procesamiento"><i class="fa fa-check"></i><b>10.3</b> Pipeline de procesamiento</a></li>
<li class="chapter" data-level="10.4" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#diagnosticos-sesgo-y-varianza"><i class="fa fa-check"></i><b>10.4</b> Diagnósticos: sesgo y varianza</a></li>
<li class="chapter" data-level="10.5" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#refinando-el-pipeline"><i class="fa fa-check"></i><b>10.5</b> Refinando el pipeline</a></li>
<li class="chapter" data-level="10.6" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#consiguiendo-mas-datos"><i class="fa fa-check"></i><b>10.6</b> Consiguiendo más datos</a></li>
<li class="chapter" data-level="10.7" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#usar-datos-adicionales"><i class="fa fa-check"></i><b>10.7</b> Usar datos adicionales</a></li>
<li class="chapter" data-level="10.8" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#examen-de-modelo-y-analisis-de-errores"><i class="fa fa-check"></i><b>10.8</b> Examen de modelo y Análisis de errores</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html"><i class="fa fa-check"></i><b>11</b> Validación de modelos: problemas comunes</a><ul>
<li class="chapter" data-level="11.1" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#filtracion-de-datos"><i class="fa fa-check"></i><b>11.1</b> Filtración de datos</a></li>
<li class="chapter" data-level="11.2" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#series-de-tiempo"><i class="fa fa-check"></i><b>11.2</b> Series de tiempo</a></li>
<li class="chapter" data-level="11.3" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#filtracion-en-el-preprocesamiento"><i class="fa fa-check"></i><b>11.3</b> Filtración en el preprocesamiento</a></li>
<li class="chapter" data-level="11.4" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#uso-de-variables-fuera-de-rango-temporal"><i class="fa fa-check"></i><b>11.4</b> Uso de variables fuera de rango temporal</a></li>
<li class="chapter" data-level="11.5" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#datos-en-conglomerados-y-muestreo-complejo"><i class="fa fa-check"></i><b>11.5</b> Datos en conglomerados y muestreo complejo</a><ul>
<li class="chapter" data-level="" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejemplo-41"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="11.5.1" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#censura-y-evaluacion-incompleta"><i class="fa fa-check"></i><b>11.5.1</b> Censura y evaluación incompleta</a></li>
<li class="chapter" data-level="11.5.2" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejemplo-tiendas-cerradas"><i class="fa fa-check"></i><b>11.5.2</b> Ejemplo: tiendas cerradas</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#muestras-de-validacion-chicas"><i class="fa fa-check"></i><b>11.6</b> Muestras de validación chicas</a><ul>
<li class="chapter" data-level="" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejercicio-7"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#otros-ejemplos"><i class="fa fa-check"></i><b>11.7</b> Otros ejemplos</a></li>
<li class="chapter" data-level="11.8" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#resumen-1"><i class="fa fa-check"></i><b>11.8</b> Resumen</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html"><i class="fa fa-check"></i><b>12</b> Métodos basados en árboles</a><ul>
<li class="chapter" data-level="12.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-regresion-y-clasificacion."><i class="fa fa-check"></i><b>12.1</b> Árboles para regresión y clasificación.</a><ul>
<li class="chapter" data-level="12.1.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-clasificacion"><i class="fa fa-check"></i><b>12.1.1</b> Árboles para clasificación</a></li>
<li class="chapter" data-level="12.1.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#tipos-de-particion"><i class="fa fa-check"></i><b>12.1.2</b> Tipos de partición</a></li>
<li class="chapter" data-level="12.1.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#medidas-de-impureza"><i class="fa fa-check"></i><b>12.1.3</b> Medidas de impureza</a></li>
<li class="chapter" data-level="12.1.4" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#reglas-de-particion-y-tamano-del-arobl"><i class="fa fa-check"></i><b>12.1.4</b> Reglas de partición y tamaño del árobl</a></li>
<li class="chapter" data-level="12.1.5" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#costo---complejidad-breiman"><i class="fa fa-check"></i><b>12.1.5</b> Costo - Complejidad (Breiman)</a></li>
<li class="chapter" data-level="12.1.6" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#opcional-predicciones-con-cart"><i class="fa fa-check"></i><b>12.1.6</b> (Opcional) Predicciones con CART</a></li>
<li class="chapter" data-level="12.1.7" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-regresion"><i class="fa fa-check"></i><b>12.1.7</b> Árboles para regresión</a></li>
<li class="chapter" data-level="12.1.8" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#variabilidad-en-el-proceso-de-construccion"><i class="fa fa-check"></i><b>12.1.8</b> Variabilidad en el proceso de construcción</a></li>
<li class="chapter" data-level="12.1.9" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#relaciones-lineales"><i class="fa fa-check"></i><b>12.1.9</b> Relaciones lineales</a></li>
<li class="chapter" data-level="12.1.10" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ventajas-y-desventajas-de-arboles"><i class="fa fa-check"></i><b>12.1.10</b> Ventajas y desventajas de árboles</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#bagging-de-arboles"><i class="fa fa-check"></i><b>12.2</b> Bagging de árboles</a><ul>
<li class="chapter" data-level="12.2.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ejemplo-48"><i class="fa fa-check"></i><b>12.2.1</b> Ejemplo</a></li>
<li class="chapter" data-level="12.2.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#mejorando-bagging"><i class="fa fa-check"></i><b>12.2.2</b> Mejorando bagging</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#bosques-aleatorios"><i class="fa fa-check"></i><b>12.3</b> Bosques aleatorios</a><ul>
<li class="chapter" data-level="12.3.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#sabiduria-de-las-masas"><i class="fa fa-check"></i><b>12.3.1</b> Sabiduría de las masas</a></li>
<li class="chapter" data-level="12.3.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ejemplo-49"><i class="fa fa-check"></i><b>12.3.2</b> Ejemplo</a></li>
<li class="chapter" data-level="12.3.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#mas-detalles-de-bosques-aleatorios."><i class="fa fa-check"></i><b>12.3.3</b> Más detalles de bosques aleatorios.</a></li>
<li class="chapter" data-level="12.3.4" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#importancia-de-variables"><i class="fa fa-check"></i><b>12.3.4</b> Importancia de variables</a></li>
<li class="chapter" data-level="12.3.5" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ajustando-arboles-aleatorios."><i class="fa fa-check"></i><b>12.3.5</b> Ajustando árboles aleatorios.</a></li>
<li class="chapter" data-level="12.3.6" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ventajas-y-desventajas-de-bosques-aleatorios"><i class="fa fa-check"></i><b>12.3.6</b> Ventajas y desventajas de bosques aleatorios</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html"><i class="fa fa-check"></i><b>13</b> Métodos basados en árboles: boosting</a><ul>
<li class="chapter" data-level="13.1" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#forward-stagewise-additive-modeling-fsam"><i class="fa fa-check"></i><b>13.1</b> Forward stagewise additive modeling (FSAM)</a></li>
<li class="chapter" data-level="13.2" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#discusion-1"><i class="fa fa-check"></i><b>13.2</b> Discusión</a></li>
<li class="chapter" data-level="13.3" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#algoritmo-fsam"><i class="fa fa-check"></i><b>13.3</b> Algoritmo FSAM</a></li>
<li class="chapter" data-level="13.4" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#fsam-para-clasificacion-binaria."><i class="fa fa-check"></i><b>13.4</b> FSAM para clasificación binaria.</a></li>
<li class="chapter" data-level="13.5" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#gradient-boosting"><i class="fa fa-check"></i><b>13.5</b> Gradient boosting</a></li>
<li class="chapter" data-level="13.6" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#algoritmo-de-gradient-boosting"><i class="fa fa-check"></i><b>13.6</b> Algoritmo de gradient boosting</a></li>
<li class="chapter" data-level="13.7" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#funciones-de-perdida"><i class="fa fa-check"></i><b>13.7</b> Funciones de pérdida</a><ul>
<li class="chapter" data-level="13.7.1" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#discusion-adaboost-opcional"><i class="fa fa-check"></i><b>13.7.1</b> Discusión: adaboost (opcional)</a></li>
<li class="chapter" data-level="" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#ejemplo-52"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#modificaciones-de-gradient-boosting"><i class="fa fa-check"></i><b>13.8</b> Modificaciones de Gradient Boosting</a><ul>
<li class="chapter" data-level="13.8.1" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#tasa-de-aprendizaje-shrinkage"><i class="fa fa-check"></i><b>13.8.1</b> Tasa de aprendizaje (shrinkage)</a></li>
<li class="chapter" data-level="13.8.2" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#submuestreo-bag.fraction"><i class="fa fa-check"></i><b>13.8.2</b> Submuestreo (bag.fraction)</a></li>
<li class="chapter" data-level="13.8.3" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#numero-de-arboles-m"><i class="fa fa-check"></i><b>13.8.3</b> Número de árboles M</a></li>
<li class="chapter" data-level="13.8.4" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#tamano-de-arboles"><i class="fa fa-check"></i><b>13.8.4</b> Tamaño de árboles</a></li>
<li class="chapter" data-level="13.8.5" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#controlar-numero-de-casos-para-cortes"><i class="fa fa-check"></i><b>13.8.5</b> Controlar número de casos para cortes</a></li>
<li class="chapter" data-level="" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#ejemplo-53"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="13.8.6" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#evaluacion-con-validacion-cruzada."><i class="fa fa-check"></i><b>13.8.6</b> Evaluación con validación cruzada.</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#graficas-de-dependencia-parcial"><i class="fa fa-check"></i><b>13.9</b> Gráficas de dependencia parcial</a><ul>
<li class="chapter" data-level="13.9.1" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#dependencia-parcial"><i class="fa fa-check"></i><b>13.9.1</b> Dependencia parcial</a></li>
<li class="chapter" data-level="" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#ejemplo-54"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#discusion-2"><i class="fa fa-check"></i>Discusión</a></li>
<li class="chapter" data-level="13.9.2" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#graficas-de-dependencia-parcial-para-otros-modelos"><i class="fa fa-check"></i><b>13.9.2</b> Gráficas de dependencia parcial para otros modelos</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#xgboost-y-gbm"><i class="fa fa-check"></i><b>13.10</b> xgboost y gbm</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html"><i class="fa fa-check"></i><b>14</b> Reducción de dimensionalidad</a><ul>
<li class="chapter" data-level="14.1" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#descomposicion-aditiva-en-matrices-de-rango-1"><i class="fa fa-check"></i><b>14.1</b> Descomposición aditiva en matrices de rango 1</a><ul>
<li class="chapter" data-level="14.1.1" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#matrices-de-rango-1"><i class="fa fa-check"></i><b>14.1.1</b> Matrices de rango 1</a></li>
<li class="chapter" data-level="" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#ejemplo-una-matriz-de-rango-1-de-preferencias"><i class="fa fa-check"></i>Ejemplo: una matriz de rango 1 de preferencias</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#aproximacion-con-matrices-de-rango-1."><i class="fa fa-check"></i><b>14.2</b> Aproximación con matrices de rango 1.</a><ul>
<li class="chapter" data-level="" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#ejemplo-55"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="14.2.1" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#suma-de-matrices-de-rango-1."><i class="fa fa-check"></i><b>14.2.1</b> Suma de matrices de rango 1.</a></li>
<li class="chapter" data-level="" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#ejemplo-peliculas"><i class="fa fa-check"></i>Ejemplo: películas</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#aproximacion-con-matrices-de-rango-bajo"><i class="fa fa-check"></i><b>14.3</b> Aproximación con matrices de rango bajo</a><ul>
<li class="chapter" data-level="14.3.1" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#discusion-aproximacion-de-rango-1."><i class="fa fa-check"></i><b>14.3.1</b> Discusión: aproximación de rango 1.</a></li>
<li class="chapter" data-level="14.3.2" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#discusion-aproximaciones-de-rango-mas-alto"><i class="fa fa-check"></i><b>14.3.2</b> Discusión: aproximaciones de rango más alto</a></li>
<li class="chapter" data-level="" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#ejemplo-57"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#descomposicion-en-valores-singulares-svd-o-dvs"><i class="fa fa-check"></i><b>14.4</b> Descomposición en valores singulares (SVD o DVS)</a></li>
<li class="chapter" data-level="14.5" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#interpretacion-geometrica"><i class="fa fa-check"></i><b>14.5</b> Interpretación geométrica</a></li>
<li class="chapter" data-level="14.6" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#svd-para-peliculas-de-netflix"><i class="fa fa-check"></i><b>14.6</b> SVD para películas de netflix</a><ul>
<li class="chapter" data-level="14.6.1" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#calidad-de-representacion-de-svd."><i class="fa fa-check"></i><b>14.6.1</b> Calidad de representación de SVD.</a></li>
<li class="chapter" data-level="" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#ejemplo-58"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#componentes-principales"><i class="fa fa-check"></i><b>14.7</b> Componentes principales</a><ul>
<li class="chapter" data-level="" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#ejemplo-60"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="14.7.1" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#varianza-en-componentes-principales."><i class="fa fa-check"></i><b>14.7.1</b> Varianza en componentes principales.</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#centrar-o-no-centrar-por-columna"><i class="fa fa-check"></i><b>14.8</b> ¿Centrar o no centrar por columna?</a><ul>
<li class="chapter" data-level="" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#ejemplo-resultados-similares"><i class="fa fa-check"></i>Ejemplo: resultados similares</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#ejemplos-donde-es-buena-idea-centrar"><i class="fa fa-check"></i>Ejemplos: donde es buena idea centrar</a></li>
<li class="chapter" data-level="" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#ejemplo-donde-no-centrar-funciona-bien"><i class="fa fa-check"></i>Ejemplo: donde no centrar funciona bien</a><ul>
<li class="chapter" data-level="14.8.1" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#otros-tipos-de-centrado"><i class="fa fa-check"></i><b>14.8.1</b> Otros tipos de centrado</a></li>
<li class="chapter" data-level="14.8.2" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#reescalando-variables"><i class="fa fa-check"></i><b>14.8.2</b> Reescalando variables</a></li>
<li class="chapter" data-level="" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#ejemplo-61"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="14.9" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#otros-metodos-t-sne"><i class="fa fa-check"></i><b>14.9</b> Otros métodos: t-SNE</a><ul>
<li class="chapter" data-level="" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#ejemplo-62"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="14.9.1" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#sne"><i class="fa fa-check"></i><b>14.9.1</b> SNE</a></li>
<li class="chapter" data-level="14.9.2" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#minimizacion-para-sne"><i class="fa fa-check"></i><b>14.9.2</b> Minimización para SNE</a></li>
<li class="chapter" data-level="" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#ejemplo-63"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="14.9.3" data-path="reduccion-de-dimensionalidad.html"><a href="reduccion-de-dimensionalidad.html#perplexity"><i class="fa fa-check"></i><b>14.9.3</b> Perplexity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="analisis-de-conglomerados-clustering.html"><a href="analisis-de-conglomerados-clustering.html"><i class="fa fa-check"></i><b>15</b> Análisis de conglomerados (clustering)</a><ul>
<li class="chapter" data-level="15.1" data-path="analisis-de-conglomerados-clustering.html"><a href="analisis-de-conglomerados-clustering.html#introduccion-1"><i class="fa fa-check"></i><b>15.1</b> Introducción</a><ul>
<li class="chapter" data-level="" data-path="analisis-de-conglomerados-clustering.html"><a href="analisis-de-conglomerados-clustering.html#ejemplo-64"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="analisis-de-conglomerados-clustering.html"><a href="analisis-de-conglomerados-clustering.html#enfoques-combinatorio-y-basado-en-modelos."><i class="fa fa-check"></i><b>15.2</b> Enfoques: combinatorio y basado en modelos.</a></li>
<li class="chapter" data-level="15.3" data-path="analisis-de-conglomerados-clustering.html"><a href="analisis-de-conglomerados-clustering.html#k-medias"><i class="fa fa-check"></i><b>15.3</b> K-medias</a><ul>
<li class="chapter" data-level="" data-path="analisis-de-conglomerados-clustering.html"><a href="analisis-de-conglomerados-clustering.html#algoritmo-de-k-medias"><i class="fa fa-check"></i>Algoritmo de k-medias</a></li>
<li class="chapter" data-level="" data-path="analisis-de-conglomerados-clustering.html"><a href="analisis-de-conglomerados-clustering.html#ejemplo-65"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="" data-path="analisis-de-conglomerados-clustering.html"><a href="analisis-de-conglomerados-clustering.html#ejercicio-8"><i class="fa fa-check"></i>Ejercicio</a></li>
<li class="chapter" data-level="" data-path="analisis-de-conglomerados-clustering.html"><a href="analisis-de-conglomerados-clustering.html#usando-la-funcion-k-means"><i class="fa fa-check"></i>Usando la funcion k-means</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="analisis-de-conglomerados-clustering.html"><a href="analisis-de-conglomerados-clustering.html#seleccion-de-numero-de-clusters."><i class="fa fa-check"></i><b>15.4</b> Selección de número de clusters.</a><ul>
<li class="chapter" data-level="" data-path="analisis-de-conglomerados-clustering.html"><a href="analisis-de-conglomerados-clustering.html#variacion-dentro-de-clusters-para-distintas-soluciones"><i class="fa fa-check"></i>Variación dentro de clusters para distintas soluciones</a></li>
<li class="chapter" data-level="15.4.1" data-path="analisis-de-conglomerados-clustering.html"><a href="analisis-de-conglomerados-clustering.html#criterios-especificos"><i class="fa fa-check"></i><b>15.4.1</b> Criterios específicos</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="analisis-de-conglomerados-clustering.html"><a href="analisis-de-conglomerados-clustering.html#dificultades-en-segmentacionclustering."><i class="fa fa-check"></i><b>15.5</b> Dificultades en segmentación/clustering.</a><ul>
<li class="chapter" data-level="15.5.1" data-path="analisis-de-conglomerados-clustering.html"><a href="analisis-de-conglomerados-clustering.html#estructuras-no-compactas"><i class="fa fa-check"></i><b>15.5.1</b> Estructuras no compactas</a></li>
<li class="chapter" data-level="15.5.2" data-path="analisis-de-conglomerados-clustering.html"><a href="analisis-de-conglomerados-clustering.html#existencia-o-no-de-grupos-naturales"><i class="fa fa-check"></i><b>15.5.2</b> Existencia o no de grupos “naturales”</a></li>
<li class="chapter" data-level="" data-path="analisis-de-conglomerados-clustering.html"><a href="analisis-de-conglomerados-clustering.html#grupos-en-dimension-alta"><i class="fa fa-check"></i>Grupos en dimensión alta</a></li>
<li class="chapter" data-level="" data-path="analisis-de-conglomerados-clustering.html"><a href="analisis-de-conglomerados-clustering.html#dificultades-en-la-seleccion-de-metrica"><i class="fa fa-check"></i>Dificultades en la selección de métrica</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Aprendizaje de máquina</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduccion" class="section level1">
<h1><span class="header-section-number">Clase 1</span> Introducción</h1>
<div id="que-es-aprendizaje-de-maquina-machine-learning" class="section level2">
<h2><span class="header-section-number">1.1</span> ¿Qué es aprendizaje de máquina (machine learning)?</h2>
<p>Métodos <strong>computacionales</strong> para <strong>aprender de datos</strong> con el fin
de producir reglas para
mejorar el <strong>desempeño</strong> en alguna tarea o toma de decisión.</p>
<p>En este curso nos enfocamos en las tareas de aprendizaje supervisado
(predecir o estimar una variable respuesta a partir de datos de entrada) y
aprendizaje no supervisado (describir estructuras interesantes en datos,
donde no necesariamente hay una respuesta que predecir).</p>
<div id="ejemplos-de-tareas-de-aprendizaje" class="section level4 unnumbered">
<h4>Ejemplos de tareas de aprendizaje:</h4>
<ul>
<li>Predecir si un cliente de tarjeta de crédito va a caer en impago en los próximos
tres meses.</li>
<li>Reconocer palabras escritas a mano (OCR).</li>
<li>Detectar llamados de ballenas en grabaciones de boyas.</li>
<li>Estimar el ingreso mensual de un hogar a partir de las características
de la vivienda, posesiones y equipamiento y localización geográfica.</li>
<li>Dividir a los clientes de Netflix según sus gustos.</li>
<li>Recomendar artículos a clientes de un programa de lealtad o servicio online.</li>
</ul>
<p>Las razones usuales para intentar resolver estos problemas <strong>computacionalmente</strong>
son diversas:</p>
<ul>
<li>Quisiéramos obtener una respuesta barata, rápida, <strong>automatizada</strong>, y
con suficiente precisión.
Por ejemplo, reconocer caracteres en una placa de coche de una fotografía se puede hacer por
personas, pero eso es lento y costoso. Igual oír cada segundo de grabación
de las boyas para saber si hay ballenas o no. Hacer mediciones directas
del ingreso de un hogar requiere mucho tiempo y esfuerzo.</li>
<li>Quisiéramos <strong>superar el desempeño actual</strong> de los expertos o de reglas simples utilizando
datos: por ejemplo, en la decisión de dar o no un préstamo a un solicitante,
puede ser posible tomar mejores decisiones con algoritmos que con evaluaciones personales
o con reglas simples que toman en cuenta el ingreso mensual, por ejemplo.</li>
<li>Queremos <strong>entender de manera más completa y sistemática</strong> el comportamiento de un fenómeno,
identificando variables o patrones importantes.</li>
</ul>
<p>Es posible aproximarse a todos estos problemas usando reglas (por ejemplo,
si los pixeles del centro de la imagen están vacíos, entonces es un cero,
si el crédito total es mayor al 50% del ingreso anual, declinar el préstamo, etc)
Las razones para intentar usar <strong>aprendizaje</strong> para producir reglas en lugar
de intentar construir estas reglas directamente son, por ejemplo:</p>
<ul>
<li>Cuando conjuntos de reglas creadas a mano se desempeñan mal (por ejemplo, para
otorgar créditos, reconocer caracteres, etc.)</li>
<li>Reglas creadas a mano pueden ser difíciles de mantener (por ejemplo, un corrector
ortográfico.)</li>
</ul>
</div>
<div id="ejemplo-reconocimiento-de-digitos-escritos-a-mano" class="section level4 unnumbered">
<h4>Ejemplo: reconocimiento de dígitos escritos a mano</h4>
<p>¿Cómo reconocer los siguientes dígitos de manera automática?</p>
<p>En los datos tenemos los valores de cada pixel (los caracteres son
imagenes de 16x16 pixeles), y una <em>etiqueta</em> asociada, que es el número
que la imagen representa. Podemos ver las imágenes y las etiquetas:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
zip_train &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="dt">file =</span> <span class="st">&#39;datos/zip-train.csv&#39;</span>)
muestra_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">sample_n</span>(zip_train, <span class="dv">10</span>)
<span class="kw">graficar_digitos</span>(muestra_<span class="dv">1</span>)</code></pre>
<p><img src="01-introduccion_files/figure-html/grafdigitos-1.png" width="400px" /></p>
<pre class="sourceCode r"><code class="sourceCode r">muestra_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">sample_n</span>(zip_train, <span class="dv">10</span>) 
<span class="kw">graficar_digitos</span>(muestra_<span class="dv">2</span>)</code></pre>
<p><img src="01-introduccion_files/figure-html/unnamed-chunk-2-1.png" width="400px" /></p>
<p>Los 16x16=256 están escritos acomodando las filas de la imagen en
vector de 256 valores (cada renglón de <code>zip_train</code>). Un dígito entonces
se representa como sigue:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(zip_train)</code></pre>
<pre><code>## [1] 7291  257</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">as.numeric</span>(zip_train[<span class="dv">1</span>,])</code></pre>
<pre><code>##   [1]  6.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.631  0.862
##  [11] -0.167 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000
##  [21] -1.000 -1.000 -1.000 -0.992  0.297  1.000  0.307 -1.000 -1.000 -1.000
##  [31] -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.410
##  [41]  1.000  0.986 -0.565 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000
##  [51] -1.000 -1.000 -1.000 -1.000 -0.683  0.825  1.000  0.562 -1.000 -1.000
##  [61] -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.938
##  [71]  0.540  1.000  0.778 -0.715 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000
##  [81] -1.000 -1.000 -1.000 -1.000 -1.000  0.100  1.000  0.922 -0.439 -1.000
##  [91] -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000
## [101] -0.257  0.950  1.000 -0.162 -1.000 -1.000 -1.000 -0.987 -0.714 -0.832
## [111] -1.000 -1.000 -1.000 -1.000 -1.000 -0.797  0.909  1.000  0.300 -0.961
## [121] -1.000 -1.000 -0.550  0.485  0.996  0.867  0.092 -1.000 -1.000 -1.000
## [131] -1.000  0.278  1.000  0.877 -0.824 -1.000 -0.905  0.145  0.977  1.000
## [141]  1.000  1.000  0.990 -0.745 -1.000 -1.000 -0.950  0.847  1.000  0.327
## [151] -1.000 -1.000  0.355  1.000  0.655 -0.109 -0.185  1.000  0.988 -0.723
## [161] -1.000 -1.000 -0.630  1.000  1.000  0.068 -0.925  0.113  0.960  0.308
## [171] -0.884 -1.000 -0.075  1.000  0.641 -0.995 -1.000 -1.000 -0.677  1.000
## [181]  1.000  0.753  0.341  1.000  0.707 -0.942 -1.000 -1.000  0.545  1.000
## [191]  0.027 -1.000 -1.000 -1.000 -0.903  0.792  1.000  1.000  1.000  1.000
## [201]  0.536  0.184  0.812  0.837  0.978  0.864 -0.630 -1.000 -1.000 -1.000
## [211] -1.000 -0.452  0.828  1.000  1.000  1.000  1.000  1.000  1.000  1.000
## [221]  1.000  0.135 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.483  0.813
## [231]  1.000  1.000  1.000  1.000  1.000  1.000  0.219 -0.943 -1.000 -1.000
## [241] -1.000 -1.000 -1.000 -1.000 -1.000 -0.974 -0.429  0.304  0.823  1.000
## [251]  0.482 -0.474 -0.991 -1.000 -1.000 -1.000 -1.000</code></pre>
<ul>
<li>Un enfoque más utilizado anteriormente para resolver este tipo de problemas
consistía en procesar estas imágenes con filtros hechos a mano (por ejemplo,
calcular cuántos pixeles están prendidos, si existen ciertas curvas o trazos)
para después construir reglas para determinar cada dígito. Actualmente,
el enfoque más exitoso es utilizar métodos de aprendizaje que aprendan
automáticamente esos filtros y esas reglas basadas en filtros (redes convolucionales).</li>
</ul>
</div>
<div id="ejemplo-predecir-ingreso-trimestral" class="section level4 unnumbered">
<h4>Ejemplo: predecir ingreso trimestral</h4>
<p>Consideramos la medición de ingreso total trimestral para una
muestra de hogares de la encuesta de ENIGH. Cada una de estas mediciones
es muy costosa en tiempo y dinero.</p>
<pre class="sourceCode r"><code class="sourceCode r">dat_ingreso &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="dt">file =</span> <span class="st">&#39;datos/enigh-ejemplo.csv&#39;</span>)
<span class="kw">head</span>(dat_ingreso) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">select</span>(TAM_HOG, INGCOR, NOM_ENT_<span class="dv">1</span>, FOCOS, 
           PISOS, marginación, tamaño_localidad)  <span class="op">%&gt;%</span>
<span class="st">    </span>knitr<span class="op">::</span><span class="kw">kable</span>()</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">TAM_HOG</th>
<th align="right">INGCOR</th>
<th align="left">NOM_ENT_1</th>
<th align="right">FOCOS</th>
<th align="right">PISOS</th>
<th align="left">marginación</th>
<th align="left">tamaño_localidad</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">4</td>
<td align="right">30238.13</td>
<td align="left">Jalisco</td>
<td align="right">11</td>
<td align="right">3</td>
<td align="left">Muy bajo</td>
<td align="left">De 15 mil a 100 mil</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">61147.41</td>
<td align="left">México</td>
<td align="right">10</td>
<td align="right">2</td>
<td align="left">Bajo</td>
<td align="left">De 15 mil a 100 mil</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">6170.21</td>
<td align="left">Puebla</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Alto</td>
<td align="left">De 2500 a 15 mil</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">14639.79</td>
<td align="left">Distrito Federal</td>
<td align="right">5</td>
<td align="right">2</td>
<td align="left">Muy bajo</td>
<td align="left">100 mil o más</td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">40638.35</td>
<td align="left">Chihuahua</td>
<td align="right">8</td>
<td align="right">3</td>
<td align="left">Muy bajo</td>
<td align="left">De 15 mil a 100 mil</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">21172.35</td>
<td align="left">Baja California</td>
<td align="right">4</td>
<td align="right">2</td>
<td align="left">Muy bajo</td>
<td align="left">100 mil o más</td>
</tr>
</tbody>
</table>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(dat_ingreso, <span class="kw">aes</span>(<span class="dt">x=</span>INGTOT<span class="op">/</span><span class="dv">1000</span>)) <span class="op">+</span><span class="st">  </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">100</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_x_log10</span>(<span class="dt">breaks =</span> <span class="kw">c</span>(<span class="fl">2.5</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">40</span>, <span class="dv">80</span>, <span class="dv">160</span>, <span class="dv">320</span>, <span class="dv">640</span>, <span class="dv">1280</span>)) <span class="op">+</span>
<span class="st">    </span><span class="kw">xlab</span>(<span class="st">&quot;Ingreso trimestral (miles de pesos)&quot;</span>)</code></pre>
<p><img src="01-introduccion_files/figure-html/unnamed-chunk-5-1.png" width="480" /></p>
<p>Pero quizá podemos usar otras variables más fácilmente medibles
para predecir el ingreso de un hogar. Por ejemplo, si consideramos el número
de focos en la vivienda:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(dat_ingreso, <span class="kw">aes</span>(<span class="dt">x =</span> FOCOS, <span class="dt">y =</span> INGTOT<span class="op">/</span><span class="dv">1000</span>)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_log10</span>(<span class="dt">breaks =</span> <span class="kw">c</span>(<span class="fl">2.5</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">40</span>, <span class="dv">80</span>, <span class="dv">160</span>, <span class="dv">320</span>, <span class="dv">640</span>, <span class="dv">1280</span>)) <span class="op">+</span>
<span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;Ingreso trimestral (miles de pesos)&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">xlim</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">50</span>))</code></pre>
<p><img src="01-introduccion_files/figure-html/unnamed-chunk-6-1.png" width="480" /></p>
<p>O el tamaño de la localidad:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(dat_ingreso, <span class="kw">aes</span>(<span class="dt">x =</span> tamaño_localidad, <span class="dt">y =</span> INGTOT<span class="op">/</span><span class="dv">1000</span>)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_boxplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_log10</span>(<span class="dt">breaks =</span> <span class="kw">c</span>(<span class="fl">2.5</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">40</span>, <span class="dv">80</span>, <span class="dv">160</span>, <span class="dv">320</span>, <span class="dv">640</span>, <span class="dv">1280</span>)) <span class="op">+</span>
<span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;Ingreso trimestral (miles de pesos)&quot;</span>)</code></pre>
<p><img src="01-introduccion_files/figure-html/unnamed-chunk-7-1.png" width="480" /></p>
<ul>
<li>En algunas encuestas se pregunta directamente el ingreso mensual del hogar. La
respuesta es generalmente una mala estimación del verdadero ingreso, por lo que
actualmente se prefiere utilizar aprendizaje para estimar a partir de otras
variables que son más fielmente reportadas por encuestados (años de estudio,
ocupación, número de focos en el hogar, etc.)</li>
</ul>
</div>
<div id="aprendizaje-supervisado" class="section level4 unnumbered">
<h4>Aprendizaje supervisado</h4>
<p>Las tareas de aprendizaje se divide en dos grandes partes: aprendizaje
supervisado y aprendizaje no supervisado.</p>
<ul>
<li><strong>Aprendizaje supervisado</strong> Construir un modelo o algoritmo para
predecir o estimar un <em>target</em> o una <em>variable de salida</em> a partir
de ciertas variables de entrada.</li>
</ul>
<p>Predecir y estimar, en este contexto, se refieren a cosas similares. Generalmente
se usa
<em>predecir</em> cuando se trata de variables que no son observables ahora, sino en el futuro,
y
<em>estimar</em> cuando nos interesan variables actuales que no podemos observar ahora
por costos o por la naturaleza del fenómeno.</p>
<p>Por ejemplo, para identificar a los clientes con alto riesgo de impago
de tarjeta de crédito, utilizamos datos históricos de clientes que han pagado
y no han pagado. Con estos datos entrenamos un algoritmo para detectar anticipadamente los
clientes con alto riesgo de impago.</p>
<p>Usualmente dividimos los problemas de aprendizaje supervisado en dos tipos,
dependiendo de la variables salida:</p>
<ul>
<li>Problemas de <strong>regresión</strong>: cuando la salida es una variable numérica. El ejemplo
de estimación de ingreso es un problema de regresión</li>
<li>Problemas de <strong>clasificación</strong>: cuando la salida es una variable categórica. El
ejemplo de detección de dígitos escritos a manos es un problema de clasificación.</li>
</ul>
</div>
<div id="ejemplo-predecir-el-rendimiento-de-un-coche." class="section level4 unnumbered">
<h4>Ejemplo: predecir el rendimiento de un coche.</h4>
<p>Estimar directamente el rendimiento (km por litro de combustible) de un
coche es costoso: hay que hacer varias pruebas en diversas condiciones, etc.
¿Podríamos estimar el rendimiento de un coche usando variables más accesibles,
peso del coche, año de producción, etc.?</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ISLR)
datos &lt;-<span class="st"> </span>Auto[, <span class="kw">c</span>(<span class="st">&#39;name&#39;</span>, <span class="st">&#39;weight&#39;</span>,<span class="st">&#39;year&#39;</span>, <span class="st">&#39;mpg&#39;</span>)]
datos<span class="op">$</span>peso_kg &lt;-<span class="st"> </span>datos<span class="op">$</span>weight<span class="op">*</span><span class="fl">0.45359237</span>
datos<span class="op">$</span>rendimiento_kpl &lt;-<span class="st"> </span>datos<span class="op">$</span>mpg<span class="op">*</span>(<span class="fl">1.609344</span><span class="op">/</span><span class="fl">3.78541178</span>)
<span class="kw">set.seed</span>(<span class="dv">213</span>)
datos_muestra &lt;-<span class="st"> </span><span class="kw">sample_n</span>(datos, <span class="dv">50</span>)
datos_muestra <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(name, peso_kg, rendimiento_kpl)</code></pre>
<pre><code>##                                  name   peso_kg rendimiento_kpl
## 9                    pontiac catalina 2007.1462        5.952012
## 139         dodge coronet custom (sw) 2021.6612        5.952012
## 248                    datsun b210 gx  938.9362       16.750662
## 229                      ford granada 1598.9131        7.865159
## 166               chevrolet monza 2+2 1461.0210        8.502874
## 321              datsun 510 hatchback 1104.0438       15.730317
## 5                         ford torino 1564.4401        7.227443
## 145                     toyota corona  747.9738       13.179455
## 282                  mercury zephyr 6 1356.2412        8.417845
## 297                     amc spirit dl 1211.0916       11.648938
## 19                       datsun pl510  966.1517       11.478880
## 320                         mazda 626 1153.0318       13.306998
## 218           buick opel isuzu deluxe  977.4916       12.754311
## 1           chevrolet chevelle malibu 1589.3877        7.652587
## 195                        amc hornet 1399.3325        9.565733
## 317                       dodge aspen 1533.5958        8.120245
## 35          plymouth satellite custom 1559.9042        6.802299
## 356                     honda prelude 1002.4391       14.327343
## 250 oldsmobile cutlass salon brougham 1526.3383        8.460360
## 373                   pontiac phoenix 1240.5751       11.478880
## 80                    renault 12 (sw)  992.9137       11.053736
## 201                 ford granada ghia 1621.1391        7.652587
## 202                pontiac ventura sj 1653.3442        7.865159
## 59                 dodge colt hardtop  964.3374       10.628593
## 277                        saab 99gle 1267.7907        9.183104
## 108                       amc gremlin 1265.0691        7.652587
## 329                mercedes-benz 240d 1474.1752       12.754311
## 220                 plymouth arrow gs 1043.2625       10.841165
## 209        plymouth volare premier v8 1787.1539        5.526868
## 263      chevrolet monte carlo landau 1553.5539        8.162759
## 178                        audi 100ls 1221.9778        9.778305
## 182                  honda civic cvcc  814.1983       14.029742
## 16                    plymouth duster 1285.0272        9.353162
## 191                  ford gran torino 1911.8918        6.164584
## 113                        ford pinto 1047.7984        8.077730
## 285                     dodge aspen 6 1524.0704        8.757960
## 49                       ford mustang 1423.8264        7.652587
## 243                          bmw 320i 1179.3402        9.140590
## 271         toyota celica gt liftback 1140.7848        8.970532
## 349                     toyota tercel  929.8644       16.027918
## 339                  plymouth reliant 1129.4450       11.563909
## 309                   pontiac phoenix 1159.3821       14.242314
## 345                    plymouth champ  850.4857       16.580605
## 91           mercury marquis brougham 2246.1894        5.101724
## 275                         audi 5000 1283.6664        8.630417
## 46         amc hornet sportabout (sw) 1343.5406        7.652587
## 255              ford fairmont (auto) 1344.9014        8.587903
## 7                    chevrolet impala 1974.9412        5.952012
## 378            plymouth horizon miser  963.8838       16.155461
## 6                    ford galaxie 500 1969.0445        6.377156</code></pre>
<p>Y podríamos comenzar graficando rendimiento contra
peso. Cada
punto representa un coche distinto. En esta
gráfica vemos que los valores de rendimiento varían según según
peso de una manera sistemática: cuanto más grande es el peso,
más bajo es el rendimiento:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(datos_muestra, 
  <span class="kw">aes</span>(<span class="dt">x=</span>peso_kg, <span class="dt">y=</span>rendimiento_kpl)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() </code></pre>
<p><img src="01-introduccion_files/figure-html/unnamed-chunk-9-1.png" width="480" /></p>
<p>Podemos entonces ajustar una curva, que para cada nivel de peso
da un valor de rendimiento que se ‘aleja lo menos posible’ de los
valores de rendimiento cercanos. Por ejemplo: según la curva roja,
¿cómo haríamos la predicción para un peso de 1500 kg?</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(datos_muestra, 
  <span class="kw">aes</span>(<span class="dt">x=</span>peso_kg, <span class="dt">y=</span>rendimiento_kpl)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">se =</span><span class="ot">FALSE</span>, <span class="dt">colour=</span><span class="st">&#39;red&#39;</span>, <span class="dt">size=</span><span class="fl">1.1</span>, 
    <span class="dt">span=</span><span class="fl">0.4</span>, <span class="dt">method=</span><span class="st">&#39;loess&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">se =</span><span class="ot">FALSE</span>, <span class="dt">colour=</span><span class="st">&#39;gray&#39;</span>, <span class="dt">size=</span><span class="fl">1.1</span>, 
    <span class="dt">span=</span><span class="dv">2</span>, <span class="dt">method=</span><span class="st">&#39;loess&#39;</span>)</code></pre>
<p><img src="01-introduccion_files/figure-html/unnamed-chunk-10-1.png" width="480" /></p>
</div>
<div id="aprendizaje-no-supervisado" class="section level4 unnumbered">
<h4>Aprendizaje no supervisado</h4>
<ul>
<li><strong>Aprendizaje no supervisado</strong> En este caso no hay <em>target</em>
o variable salida. Buscamos modelar y entender las relaciones entre variables
y entre observaciones, o patrones importantes o interesantes en los datos.</li>
</ul>
<p>Los problemas supervisados tienen un objetivo claro: hacer las mejores
predicciones posibles bajo ciertas restricciones. Los problemas no supervisados
tienden a tener objetivos más vagos, y por lo mismo pueden ser más difíciles.</p>
</div>
<div id="ejemplo-tipos-de-coches-en-el-mercado" class="section level4 unnumbered">
<h4>Ejemplo: tipos de coches en el mercado</h4>
<p>Quisieramos encontrar categorías de coches tales que: las categorías son diferentes
entre sí, y los coches en una misma categoría son similares entre sí.
Esta agrupación nos permite entender la estructura general de los datos, cómo
están organizados en términos de similitud de características.</p>
<p>En este ejemplo, encontramos un plano de máxima variabilidad donde proyectamos
los coches, y después formamos grupos de coches similares:</p>
<pre class="sourceCode r"><code class="sourceCode r">autos &lt;-<span class="st"> </span>Auto <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(mpg, displacement, horsepower, acceleration)
comps_autos &lt;-<span class="st"> </span><span class="kw">princomp</span>(autos, <span class="dt">cor =</span> <span class="ot">TRUE</span>)
clust &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(comps_autos<span class="op">$</span>scores[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]), <span class="dt">method =</span> <span class="st">&#39;ward.D&#39;</span>)
autos<span class="op">$</span>grupo &lt;-<span class="st"> </span><span class="kw">cutree</span>(clust, <span class="dt">k =</span> <span class="dv">4</span>)
autos<span class="op">$</span>Comp<span class="fl">.1</span> &lt;-<span class="st"> </span>comps_autos<span class="op">$</span>scores[,<span class="dv">1</span>]
autos<span class="op">$</span>Comp<span class="fl">.2</span> &lt;-<span class="st"> </span>comps_autos<span class="op">$</span>scores[,<span class="dv">2</span>]
autos<span class="op">$</span>nombre &lt;-<span class="st"> </span>Auto<span class="op">$</span>name
<span class="kw">ggplot</span>(autos, <span class="kw">aes</span>(<span class="dt">x=</span>Comp<span class="fl">.1</span>, <span class="dt">y=</span>Comp<span class="fl">.2</span>, <span class="dt">colour=</span><span class="kw">factor</span>(grupo), <span class="dt">label=</span>nombre)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() </code></pre>
<p><img src="01-introduccion_files/figure-html/unnamed-chunk-11-1.png" width="480" /></p>
<p>¿Cómo interpretamos los grupos?</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">filter</span>(autos, grupo<span class="op">==</span><span class="dv">1</span>))</code></pre>
<pre><code>##   mpg displacement horsepower acceleration grupo    Comp.1    Comp.2
## 1  18          307        130         12.0     1 -1.817719 0.5042535
## 2  15          350        165         11.5     1 -2.800712 0.3938195
## 3  18          318        150         11.0     1 -2.310357 0.7966085
## 4  16          304        150         12.0     1 -2.213807 0.3989781
## 5  17          302        140         10.5     1 -2.225309 0.9183779
## 6  15          429        198         10.0     1 -3.900596 0.6915313
##                      nombre
## 1 chevrolet chevelle malibu
## 2         buick skylark 320
## 3        plymouth satellite
## 4             amc rebel sst
## 5               ford torino
## 6          ford galaxie 500</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">filter</span>(autos, grupo<span class="op">==</span><span class="dv">3</span>))</code></pre>
<pre><code>##   mpg displacement horsepower acceleration grupo      Comp.1       Comp.2
## 1  22          198         95         15.5     3  0.01913364 -0.090471378
## 2  18          199         97         15.5     3 -0.26705470 -0.339015545
## 3  21          200         85         16.0     3  0.16412490 -0.315611651
## 4  21          199         90         15.0     3 -0.05362631 -0.004579963
## 5  19          232        100         13.0     3 -0.79359758  0.413938751
## 6  16          225        105         15.5     3 -0.63973365 -0.517394423
##                      nombre
## 1           plymouth duster
## 2                amc hornet
## 3             ford maverick
## 4               amc gremlin
## 5               amc gremlin
## 6 plymouth satellite custom</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">filter</span>(autos, grupo<span class="op">==</span><span class="dv">2</span>))</code></pre>
<pre><code>##   mpg displacement horsepower acceleration grupo      Comp.1    Comp.2
## 1  24          113         95         15.0     2  0.50234800 0.3800473
## 2  27           97         88         14.5     2  0.79722704 0.7509781
## 3  24          107         90         14.5     2  0.52837050 0.5437610
## 4  26          121        113         12.5     2 -0.04757934 1.2605758
## 5  27           97         88         14.5     2  0.79722704 0.7509781
## 6  28          140         90         15.5     2  0.76454526 0.4100595
##                  nombre
## 1 toyota corona mark ii
## 2          datsun pl510
## 3           audi 100 ls
## 4              bmw 2002
## 5          datsun pl510
## 6   chevrolet vega 2300</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">filter</span>(autos, grupo<span class="op">==</span><span class="dv">4</span>))</code></pre>
<pre><code>##   mpg displacement horsepower acceleration grupo    Comp.1     Comp.2
## 1  26           97         46         20.5     4 2.2421696 -1.1703377
## 2  25          110         87         17.5     4 1.0737328 -0.3205227
## 3  25          104         95         17.5     4 0.9902507 -0.3021997
## 4  22          140         72         19.0     4 1.1727317 -1.0419917
## 5  30           79         70         19.5     4 2.0927389 -0.5620939
## 6  31           71         65         19.0     4 2.1920905 -0.3319627
##                         nombre
## 1 volkswagen 1131 deluxe sedan
## 2                  peugeot 504
## 3                     saab 99e
## 4          chevrolet vega (sw)
## 5                  peugeot 304
## 6          toyota corolla 1200</code></pre>
</div>
</div>
<div id="aprendizaje-supervisado-1" class="section level2">
<h2><span class="header-section-number">1.2</span> Aprendizaje Supervisado</h2>
<p>Por el momento nos concentramos en problemas supervisados de regresión, es
decir predicción de variables numéricas.</p>
<p>¿Cómo entendemos el problema de predicción?</p>
<div id="proceso-generador-de-datos-modelo-teorico" class="section level3 unnumbered">
<h3>Proceso generador de datos (modelo teórico)</h3>
<p>Para entender lo que estamos intentando hacer, pensaremos en términos
de <strong>modelos probabilísticos que generan los datos</strong>. La idea es que
estos representan los procesos que generan los datos o las observaciones.</p>
<p>Si <span class="math inline">\(Y\)</span> es la respuesta
que queremos predecir, y <span class="math inline">\(X\)</span> es una entrada que queremos usar para predecir
<span class="math inline">\(Y\)</span>,<br />
consideramos que las variables aleatorias <span class="math inline">\(Y\)</span> y <span class="math inline">\(X\)</span> están relacionadas como sigue:
<span class="math display">\[Y=f(X)+\epsilon,\]</span>
donde <span class="math inline">\(\epsilon\)</span> es una término de error aleatorio que no depende de <span class="math inline">\(X\)</span>, y
que tiene valor esperado <span class="math inline">\(\textrm{E}(\epsilon)=0\)</span>.</p>
<ul>
<li><span class="math inline">\(f\)</span> expresa la relación sistemática que hay entre <span class="math inline">\(Y\)</span> y <span class="math inline">\(X\)</span>: para cada valor
posible de <span class="math inline">\(X\)</span>, la <code>contribución</code> de <span class="math inline">\(X\)</span> a <span class="math inline">\(Y\)</span> es <span class="math inline">\(f(X)\)</span>.</li>
<li>Pero <span class="math inline">\(X\)</span> <strong>no determina</strong> a <span class="math inline">\(Y\)</span>, como en el ejemplo anterior de rendimiento
de coches.
Entonces agregamos una error aleatorio <span class="math inline">\(\epsilon\)</span>, con media cero (si la media
no es cero podemos agregar una constante a <span class="math inline">\(f\)</span>), que no contiene información
acerca de <span class="math inline">\(X\)</span> (independiente de <span class="math inline">\(X\)</span>).</li>
<li><span class="math inline">\(\epsilon\)</span> representa, por ejemplo, el efecto de variables que no hemos
medido o procesos aleatorios que determinan la respuesta.</li>
</ul>
<div id="ejemplo" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>Vamos a usar simulación para entender estas ideas: supongamos que <span class="math inline">\(X\)</span>
es el número de años de estudio de una persona y <span class="math inline">\(Y\)</span> es su ingreso mensual.
En primer lugar, estas son el número de años de estudio de 8 personas:</p>
<pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">7</span>,<span class="dv">10</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">5</span>,<span class="dv">9</span>,<span class="dv">13</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">17</span>,<span class="dv">18</span>,<span class="dv">1</span>,<span class="dv">2</span>)</code></pre>
<p>Ahora <em>supondremos</em> que la dependencia de Y de X está dada por
<span class="math inline">\(Y=f(X)+\epsilon\)</span> por una función <span class="math inline">\(f\)</span> que no conocemos (esta función está
determinada por el fenómeno)</p>
<pre class="sourceCode r"><code class="sourceCode r">f &lt;-<span class="st"> </span><span class="cf">function</span>(x){
  <span class="kw">ifelse</span>(x <span class="op">&lt;</span><span class="st"> </span><span class="dv">10</span>, <span class="dv">1000</span><span class="op">*</span><span class="kw">sqrt</span>(x), <span class="dv">1000</span><span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">10</span>))
}</code></pre>
<p>El ingreso no se determina
únicamente por número de años de estudio. Suponemos entonces que hay algunas
variables
adicionales que perturban los niveles de <span class="math inline">\(f(X)\)</span> por una cantidad aleatoria. Los
valores que observamos de <span class="math inline">\(Y\)</span> están dados entonces por <span class="math inline">\(Y=f(X)+\epsilon\)</span>.</p>
<p>Entonces podríamos obtener, por ejemplo:</p>
<pre class="sourceCode r"><code class="sourceCode r">x_g &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">20</span>,<span class="fl">0.5</span>)
y_g &lt;-<span class="st"> </span><span class="kw">f</span>(x_g)
dat_g &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> x_g, <span class="dt">y =</span> y_g)
<span class="kw">set.seed</span>(<span class="dv">281</span>)
error &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(x), <span class="dv">0</span>, <span class="dv">500</span>)
y &lt;-<span class="st"> </span><span class="kw">f</span>(x) <span class="op">+</span><span class="st"> </span>error
datos &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)
datos<span class="op">$</span>y_media &lt;-<span class="st"> </span><span class="kw">f</span>(datos<span class="op">$</span>x)
<span class="kw">ggplot</span>(datos, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data=</span>dat_g, <span class="dt">colour =</span> <span class="st">&#39;blue&#39;</span>, <span class="dt">size =</span> <span class="fl">1.1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_segment</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">xend =</span> x, <span class="dt">y =</span> y, <span class="dt">yend =</span> y_media), <span class="dt">col=</span><span class="st">&#39;red&#39;</span>)</code></pre>
<p><img src="01-introduccion_files/figure-html/unnamed-chunk-15-1.png" width="480" /></p>
<p>En problemas de aprendizaje nunca conocemos esta <span class="math inline">\(f\)</span> verdadera,
aunque quizá sabemos algo acerca de sus propiedades (por ejemplo, continua, de
variación suave). Lo que tenemos son los datos, que también podrían haber resultado
en (para otra muestra de personas, por ejemplo):</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">28015</span>)
error &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(x), <span class="dv">0</span>, <span class="dv">500</span>)
y &lt;-<span class="st"> </span><span class="kw">f</span>(x) <span class="op">+</span><span class="st"> </span>error
datos &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)
<span class="kw">ggplot</span>(datos, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() </code></pre>
<p><img src="01-introduccion_files/figure-html/unnamed-chunk-16-1.png" width="480" /></p>
<p>La siguiente observación nos da una idea de lo que intentamos hacer, aunque
todavía es vaga y requiere refinamiento:</p>
<div class="comentario">
<p>
Bajo los supuestos del modelo <span class="math inline"><span class="math inline">\(Y=f(X)+\epsilon\)</span></span>, <strong>aprender de los datos</strong> significa intentar recuperar o estimar la forma de la función <span class="math inline"><span class="math inline">\(f\)</span></span> que no conocemos. <span class="math inline"><span class="math inline">\(f\)</span></span> representa la relación sistemática entre <span class="math inline"><span class="math inline">\(Y\)</span></span> y <span class="math inline"><span class="math inline">\(X\)</span></span>.
</p>
</div>
<p>¿Qué tan bien podemos estimar esa <span class="math inline">\(f\)</span> que no conocemos, con
los datos disponibles? ¿Qué significa <em>estimar bien</em>?
Incluso este ejemplo tan simple muestra las dificultades que vamos a
enfrentar, y la importancia de determinar con cuidado qué tanta información
tenemos, y qué tan buenas pueden ser nuestras predicciones.</p>
</div>
</div>
</div>
<div id="predicciones" class="section level2">
<h2><span class="header-section-number">1.3</span> Predicciones</h2>
<p>La idea es entonces producir una estimación de f que nos permita hacer predicciones.</p>
<p>Si denotamos por <span class="math inline">\(\hat{f}\)</span> a una estimación de <span class="math inline">\(f\)</span> construida a partir de los datos,
podemos hacer predicciones aplicando <span class="math inline">\(\hat{f}\)</span> a valores de <span class="math inline">\(X\)</span>.
La predicción de Y la denotamos por <span class="math inline">\(\hat{Y}\)</span>, y <span class="math display">\[\hat{Y}=\hat{f}(X).\]</span>
El error de predicción (residual) está dado por el valor observado menos la predicción:
<span class="math display">\[Y-\hat{Y}.\]</span></p>
<p>En nuestro ejemplo anterior, podríamos construir, por ejemplo, una recta
ajustada por mínimos cuadrados:</p>
<pre class="sourceCode r"><code class="sourceCode r">curva_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">data=</span>datos,
  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se=</span><span class="ot">FALSE</span>, <span class="dt">color=</span><span class="st">&quot;red&quot;</span>, <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span>x, <span class="dt">size =</span> <span class="fl">1.1</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(datos, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>curva_<span class="dv">1</span></code></pre>
<p><img src="01-introduccion_files/figure-html/unnamed-chunk-19-1.png" width="480" /></p>
<p>En este caso <span class="math inline">\(\hat{f}\)</span> es una recta, y la podemos usar para hacer predicciones.
Por ejemplo, si tenemos una observación con
<span class="math inline">\(x_0=8\)</span> años de estudio, nuestra predicción del ingreso
<span class="math inline">\(\hat{y}=\hat{f}(8)\)</span> sería</p>
<pre class="sourceCode r"><code class="sourceCode r">lineal &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x,<span class="dt">data =</span> datos)
pred_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">predict</span>(lineal, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x=</span><span class="dv">8</span>))
pred_<span class="dv">1</span></code></pre>
<pre><code>##        1 
## 2193.561</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(datos, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>curva_<span class="dv">1</span> <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_segment</span>(<span class="dt">x =</span> <span class="dv">0</span>, <span class="dt">xend =</span> <span class="dv">8</span>, <span class="dt">y =</span> pred_<span class="dv">1</span>, <span class="dt">yend =</span> pred_<span class="dv">1</span>, <span class="dt">colour =</span> <span class="st">&#39;salmon&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_segment</span>(<span class="dt">x =</span> <span class="dv">8</span>, <span class="dt">xend =</span> <span class="dv">8</span>, <span class="dt">y =</span> <span class="dv">0</span>, <span class="dt">yend =</span> pred_<span class="dv">1</span>, <span class="dt">colour =</span> <span class="st">&#39;salmon&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&#39;text&#39;</span>, <span class="dt">x =</span> <span class="fl">0.5</span>, <span class="dt">y =</span> pred_<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">100</span>, <span class="dt">label =</span> <span class="kw">round</span>(pred_<span class="dv">1</span>, <span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>( <span class="dt">x=</span> <span class="dv">8</span>, <span class="dt">y =</span><span class="dv">3200</span>, <span class="dt">col=</span><span class="st">&#39;green&#39;</span>, <span class="dt">size =</span> <span class="dv">4</span>)</code></pre>
<p><img src="01-introduccion_files/figure-html/unnamed-chunk-21-1.png" width="480" /></p>
<p>Si observamos que para esta observación con <span class="math inline">\(x_0=8\)</span>, resulta que
el correspondiente ingreso es <span class="math inline">\(y_0=3200\)</span>, entonces el error sería</p>
<pre class="sourceCode r"><code class="sourceCode r">y_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="dv">3200</span>
y_<span class="dv">0</span> <span class="op">-</span><span class="st"> </span>pred_<span class="dv">1</span></code></pre>
<pre><code>##        1 
## 1006.439</code></pre>
<div class="comentario">
<p>
En aprendizaje buscamos que estos errores sean lo más cercano a cero que sea posible.
</p>
</div>
</div>
<div id="aprendizaje" class="section level2">
<h2><span class="header-section-number">1.4</span> Tarea de aprendizaje supervisado</h2>
<p>El elemento faltante para definir la tarea de aprendizaje supervisado es
cuantificar qué significa aproximar <em>bien</em> a <span class="math inline">\(f\)</span>, o tener predicciones precisas. Para
esto definimos una <strong>función de pérdida</strong>:</p>
<p><span class="math display">\[L(Y, \hat{f}(X)),\]</span></p>
<p>que nos dice cuánto nos cuesta hacer la predicción <span class="math inline">\(\hat{f}(X)\)</span> cuando el
verdadero valor es <span class="math inline">\(Y\)</span> y las variables de entrada son <span class="math inline">\(X\)</span>.
Una opción conveniente para problemas de regresión
es la <strong>pérdida cuadrática</strong>:</p>
<p><span class="math display">\[L(Y, \hat{f}(X)) =  (Y - \hat{f}(X))^2\]</span>
Esta es una cantidad aleatoria, de modo que en algunos casos este error
puede ser más grande o más chico. Usualmente buscamos una <span class="math inline">\(\hat{f}\)</span> de
modo que el error promedio sea chico:</p>
<p><span class="math display">\[Err = E (Y - \hat{f}(X))^2 \]</span></p>
<p><strong>Notas</strong>:</p>
<ul>
<li>Este valor esperado es sobre la población para la que queremos hacer
predicciones. Es una cantidad teórica, no podemos calcularla con ningún
conjunto de datos</li>
<li>Intenta demostrar que bajo error cuadrático medio y suponiendo
el modelo aditivo <span class="math inline">\(Y=f(X)+\epsilon\)</span>, el mejor predictor
de <span class="math inline">\(Y\)</span> es <span class="math inline">\(f(x)= E[Y|X=x]\)</span>. Es decir: lo que nos interesa es aproximar
lo mejor que se pueda la esperanza condicional</li>
</ul>
<p>Ahora tenemos los elementos para definir con precisión el problema
de aprendizaje supervisado.</p>
<p>Consideramos un proceso generador de datos <span class="math inline">\((X,Y)\)</span>.</p>
<p>En primer lugar,
tenemos datos de los que vamos a aprender. Supongamos entonces que tenemos un conjunto de datos <em>etiquetados</em> (generados según <span class="math inline">\((X,Y)\)</span>)</p>
<p><span class="math display">\[{\mathcal L}=\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \ldots, (x^{(N)}, y^{(N)}) \}\]</span>
que llamamos <strong>conjunto de entrenamiento</strong>. Nótese que usamos minúsculas
para denotar observaciones particulares de <span class="math inline">\((X,Y)\)</span>.</p>
<p>Un <strong>algoritmo de aprendizaje</strong> (<strong>aprender</strong> de los datos)
es una regla que asigna a cada conjunto de
entrenamiento <span class="math inline">\({\mathcal L}\)</span> una función <span class="math inline">\(\hat{f}\)</span>:</p>
<p><span class="math display">\[{\mathcal L} \to \hat{f}.\]</span></p>
<p>Una vez que construimos la función <span class="math inline">\(\hat{f}\)</span>, podemos hacer predicciones.
El desempeño del predictor particular <span class="math inline">\(\hat{f}\)</span> se mide como sigue: si
en el futuro observamos otra muestra <span class="math inline">\({\mathcal T}\)</span>, que llamamos <strong>muestra de prueba</strong>,</p>
<p><span class="math display">\[{\mathcal T}=\{ (x_0^{(1)},y_0^{(1)}),(x_0^{(2)},y_0^{(2)}), \ldots, (x_0^{(m)}, y_0^{(m)}) \}\]</span></p>
<p>entonces decimos que el <strong>error de predicción</strong> (cuadrático) de <span class="math inline">\(\hat{f}\)</span> para el
ejemplo <span class="math inline">\((x_0^{(j)},y_0^{(j)})\)</span> está dado por
<span class="math display">\[(y_0^{(j)} - \hat{f}(x_0^{(j)}))^2\]</span></p>
<p>y el error promedio sobre la muestra <span class="math inline">\({\mathcal T}\)</span> es</p>
<p><span class="math display">\[\hat{Err} =   \frac{1}{m}\sum_{j=1}^m (y_0^{(j)} - \hat{f}(x_0^{(j)}))^2\]</span></p>
<p>que es una estimación del error de predicción
<span class="math display">\[Err = E (Y - \hat{f}(X))^2 \]</span></p>
<p>Adicionalmente, definimos otra cantidad de menor interés,
el <strong>error de entrenamiento</strong>, como</p>
<p><span class="math display">\[\overline{err} = \frac{1}{N}\sum_{i=1}^N (y^{(i)} - \hat{f}(x^{(i)}))^2.\]</span></p>
<p>El punto más importante que discutiremos ahora es el siguiente:</p>
<ul>
<li>Nótese que el error de entrenamiento se calcula sobre la muestra <span class="math inline">\({\mathcal L}\)</span>
que se usó
para construir <span class="math inline">\(\hat{f}\)</span>, mientras que el error de prueba se calcula usando
una muestra independiente <span class="math inline">\({\mathcal T}\)</span>.</li>
<li><span class="math inline">\(\hat{Err}\)</span> es una estimación razonable de el error de predicción <span class="math inline">\(Err\)</span>
(por ejemplo, <span class="math inline">\(\hat{Err} \to Err\)</span> cuando el tamaño de la muestra de prueba
crece), pero <span class="math inline">\(\overline{err}\)</span> típicamente es una estimación mala del error de
predicción.</li>
</ul>
<div id="ejemplo-1" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>En el ejemplo que hemos estado usando, ¿que curva preferirías para
predecir, la gris, la roja o la azul? ¿Cuál tiene menor error de entrenamiento?</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">280572</span>)
error &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(x), <span class="dv">0</span>, <span class="dv">500</span>)
y &lt;-<span class="st"> </span><span class="kw">f</span>(x) <span class="op">+</span><span class="st"> </span>error
datos_entrena &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y)
<span class="kw">head</span>(datos_entrena)</code></pre>
<pre><code>##    x          y
## 1  1   86.22033
## 2  7 2353.75863
## 3 10 3078.71029
## 4  0 -397.80229
## 5  0  424.73363
## 6  5 3075.92998</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">curva_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">data=</span>datos_entrena,
  <span class="dt">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="dt">se=</span><span class="ot">FALSE</span>, <span class="dt">color=</span><span class="st">&quot;gray&quot;</span>, <span class="dt">span=</span><span class="dv">1</span>, <span class="dt">size=</span><span class="fl">1.1</span>)
curva_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">data=</span>datos_entrena,
  <span class="dt">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="dt">se=</span><span class="ot">FALSE</span>, <span class="dt">color=</span><span class="st">&quot;red&quot;</span>, <span class="dt">span=</span><span class="fl">0.5</span>, <span class="dt">size=</span><span class="fl">1.1</span>)
curva_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">data=</span>datos_entrena,
  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se=</span><span class="ot">FALSE</span>, <span class="dt">color=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">size=</span><span class="fl">1.1</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(datos_entrena, <span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">    </span>curva_<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>curva_<span class="dv">2</span> <span class="op">+</span><span class="st"> </span>curva_<span class="dv">3</span></code></pre>
<p><img src="01-introduccion_files/figure-html/unnamed-chunk-25-1.png" width="480" /></p>
<p>Calculamos los errores de entrenamiento de cada curva:</p>
<pre class="sourceCode r"><code class="sourceCode r">mod_rojo &lt;-<span class="st"> </span><span class="kw">loess</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> datos_entrena, <span class="dt">span=</span><span class="fl">0.3</span>)
mod_gris &lt;-<span class="st"> </span><span class="kw">loess</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> datos_entrena, <span class="dt">span=</span><span class="dv">1</span>)
mod_recta &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> datos_entrena)
df_mods &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">nombre =</span> <span class="kw">c</span>(<span class="st">&#39;recta&#39;</span>, <span class="st">&#39;rojo&#39;</span>,<span class="st">&#39;gris&#39;</span>))
df_mods<span class="op">$</span>modelo &lt;-<span class="st"> </span><span class="kw">list</span>(mod_recta, mod_rojo, mod_gris)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">error_f &lt;-<span class="st"> </span><span class="cf">function</span>(df, mod){
  <span class="cf">function</span>(mod){
    preds &lt;-<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">newdata =</span> df)
    <span class="kw">round</span>(<span class="kw">sqrt</span>(<span class="kw">mean</span>((preds <span class="op">-</span><span class="st"> </span>df<span class="op">$</span>y) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)))
  }
}
error_ent &lt;-<span class="st"> </span><span class="kw">error_f</span>(datos_entrena)

df_mods &lt;-<span class="st"> </span>df_mods <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">error_entrena =</span> <span class="kw">map_dbl</span>(modelo, error_ent))
df_mods</code></pre>
<pre><code>## # A tibble: 3 x 3
##   nombre modelo      error_entrena
##   &lt;chr&gt;  &lt;list&gt;              &lt;dbl&gt;
## 1 recta  &lt;S3: lm&gt;              782
## 2 rojo   &lt;S3: loess&gt;           189
## 3 gris   &lt;S3: loess&gt;           389</code></pre>
<p>El error de entrenamiento es considerablemente menor para la curva
roja, y es más grande para la recta.</p>
<p>Sin embargo, consideremos que tenemos una nueva muestra (de prueba).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">218052272</span>)
x_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">13</span>, <span class="dv">100</span>, <span class="dt">replace =</span> T)
error &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(x_<span class="dv">0</span>), <span class="dv">0</span>, <span class="dv">500</span>)
y_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">f</span>(x_<span class="dv">0</span>) <span class="op">+</span><span class="st"> </span>error
datos_prueba &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x =</span> x_<span class="dv">0</span>, <span class="dt">y =</span> y_<span class="dv">0</span>)
datos_prueba</code></pre>
<pre><code>## # A tibble: 100 x 2
##        x      y
##    &lt;int&gt;  &lt;dbl&gt;
##  1     9  2156.
##  2    11  3227.
##  3     3  2382.
##  4    10  3482.
##  5     7  2733.
##  6     7  2326.
##  7    12  3464.
##  8     0  -564.
##  9    10  3296.
## 10     0   366.
## # ... with 90 more rows</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">error_p &lt;-<span class="st"> </span><span class="kw">error_f</span>(datos_prueba)
df_mods &lt;-<span class="st"> </span>df_mods <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">error_prueba =</span> <span class="kw">map_dbl</span>(modelo, error_p))
df_mods</code></pre>
<pre><code>## # A tibble: 3 x 4
##   nombre modelo      error_entrena error_prueba
##   &lt;chr&gt;  &lt;list&gt;              &lt;dbl&gt;        &lt;dbl&gt;
## 1 recta  &lt;S3: lm&gt;              782          801
## 2 rojo   &lt;S3: loess&gt;           189          628
## 3 gris   &lt;S3: loess&gt;           389          520</code></pre>
<p><strong>Observaciones</strong></p>
<ul>
<li><p>El “mejor”&quot; modelo en entrenamiento es uno que <em>sobreajusta</em> a los datos, pero es
el peor con una muestra de prueba. La curva roja aprende de la componente
de ruido del modelo - lo cual realmente no es aprendizaje.</p></li>
<li><p>El modelo de la recta no es bueno en entrenamiento ni en prueba. Este modelo
no tiene la capacidad para aprender de la señal en los datos.</p></li>
<li><p>El mejor modelo en la muestra de prueba es uno que está entre la recta y
la curva roja en términos de flexibilidad.</p></li>
<li><p>Nuestra intuición para escoger el modelo gris desde el principio se refleja
en que <em>generaliza</em> mejor que los otros, y eso a su vez se refleja en
un error de prueba más bajo.</p></li>
<li>¿De dónde
provienen los errores en la predicción? ¿Podemos hacer el error igual a cero?
Si establemos que el error es una función
creciente de <span class="math inline">\(Y-\hat{Y}\)</span>, vemos que
<span class="math display">\[ Y-\hat{Y} = f(X) + \epsilon - \hat{f}(X)= (f(X) - \hat{f}(X)) + \epsilon,\]</span>
donde vemos que hay dos componentes que pueden hacer grande a <span class="math inline">\(Y-\hat{Y}\)</span>:
<ul>
<li>La diferencia <span class="math inline">\(f(X) - \hat{f}(X)\)</span> está asociada a <strong>error reducible</strong>, pues
depende de qué tan bien estimemos <span class="math inline">\(f(X)\)</span> con <span class="math inline">\(\hat{f}(X)\)</span></li>
<li>El error aleatorio <span class="math inline">\(\epsilon\)</span>, asociado a <strong>error irreducible</strong>.</li>
</ul></li>
<li>Cualquiera de estas dos cantidades pueden hacer que nuestras predicciones no sean
precisas. No podemos hacer mucho acerca del error irreducible (sin cambiar las variables
que usamos, la definición del problema, etc.) En nuestro ejemplo anterior, el error reducible:
<ul>
<li>Es grande para el modelo rojo, pues responde demasiado fuerte a ruido en los datos (tiene varianza alta).</li>
<li>Es grande para el modelo de la recta, pues no tiene capacidad para acercarse a
la verdadera curva (está sesgado).</li>
</ul></li>
</ul>
</div>
</div>
<div id="error" class="section level2">
<h2><span class="header-section-number">1.5</span> Balance de complejidad y rigidez</h2>
<p>Como vimos en el ejemplo de arriba, el error de entrenamiento no es
un buen indicador del desempeño futuro de nuestras predicciones. Para evaluar
este desempeño, necesitamos una muestra de prueba independiente de la
muestra que usamos para aprender o para entrenar el modelo.</p>
<p>Intuitivamente esto tiene sentido: en el proceso de aprendizaje tenemos
disponibles las etiquetas (sabemos las respuestas), de modo que puede suceder
que el algoritmo <strong>memorice</strong> la asociación de qué etiquetas <span class="math inline">\(y^{(i)}\)</span> van con cada
conjunto de entradas <span class="math inline">\(x^{(i)}\)</span>. Esto se dice de varias maneras, por ejemplo:</p>
<ul>
<li><p>El modelo <em>sobreajusta</em> a los datos: esto quiere decir que por ajustar aspectos
de los datos de entrenamiento demasiado fuertemente, el algoritmo parece replicar
de cerca los datos de entrenamiento pero se desempeña mal en la predicción.</p></li>
<li><p>El modelo <em>aprende del ruido</em>: nuestro proceso de aprendizaje captura aspectos
irrelevantes de los datos, que nuevos datos no van a compartir.</p></li>
<li><p>El modelo <em>no tiene capacidad de generalización</em>, porque captura aspectos
que solo están presentes en nuestra muestra de entrenamiento.</p></li>
<li><p>El modelo <em>tiene varianza alta</em>, porque cambia mucho dependiendo de la muestra
de entrenamiento.</p></li>
<li><p>El modelo es <em>demasiado complejo o flexible</em> y fácilmente se adapta a cualquier
conjunto de datos, tanto señal como ruido</p></li>
</ul>
<p>En el ejemplo de arriba, también vimos que algunos modelos pueden tener desempeño
malo porque no tienen la capacidad de aprender de patrones reales y generales
en los datos (la recta en el ejemplo anterior). Podemos decir esto de varias maneras:</p>
<ul>
<li><p>El modelo <em>subajusta</em> a los datos: no tienen la capacidad de ajustar
aspectos de los datos de entrenamiento que son relaciones reales entre las
variables.</p></li>
<li><p>El modelo <em>ignora señal en los datos</em>: el algoritmo no captura aspectos relevantes de los
datos, que comparten con nuevos datos y pueden utilizarse para hacer predicciones.</p></li>
<li><p>El modelo <em>no tiene capacidad de aprendizaje</em>, pues no puede capturar
aspectos que son generales para el fenómeno de interés.</p></li>
<li><p>El modelo tiene <em>sesgo alto</em>, porque no puede ajustar patrones generalizables
en los datos.</p></li>
<li><p>El modelo es <em>demasiado rígido</em>, y no puede adaptarse ni siquiera a patrones
fuertes y claros en los datos.</p></li>
</ul>
<p>Logramos buenas predicciones cuando refinamos nuestros modelos o algoritmos
para lograr <em>aprender de la señal</em> e
<em>ignorar el ruido, que no ayuda en la predicción</em>, y lograr reducir el
error de predicción lo más posible con los datos disponibles. Esto requiere
buscar el nivel adecuado de complejidad en los modelos o algoritmos para los
datos que tenemos.</p>
<div class="comentario">
<p>
Para construir buenos predictores, requerimos que:
</p>
<ul>
<li>
El algoritmo tenga la <strong>flexibilidad</strong> necesaria para capturar patrones generales y fuertes en los datos
</li>
<li>
El algoritmo tenga la <strong>rigidez</strong> necesaria para tener robustez a patrones de ruido o particularidades no repetibles de nuestra muestra de entrenamiento.
</li>
<li>
Saber intuitivamente cuál es el grado adecuado de complejidad para un problema dado es difícil. Para decidirlo, evaluamos el desempeño de nuestros métodos usando una <strong>muestra de prueba</strong>. El nivel adecuado de complejidad se traduce en menos errores de predicción.
</li>
</ul>
</div>
<div id="discusion-error-de-entrenamiento-y-prueba" class="section level4">
<h4><span class="header-section-number">1.5.0.1</span> Discusión (error de entrenamiento y prueba)</h4>
<p>En términos teóricos, podemos ver cuál es el problema de intentar evaluar el
error de predicción utilizando la muestra de entrenamiento.</p>
<p>En primer lugar consideremos evaluar el error de predicción para
un ejemplo <span class="math display">\[(y_0- \hat{f}(x_0))^2\]</span></p>
<p>donde <span class="math inline">\((x_0, y_0)\)</span> es independiente de la muestra de entrenamiento. En este
caso, la <span class="math inline">\(\hat{f}\)</span> está fija, y el valor esperado (error de predicción) nos
da el error de predicción.</p>
<p>Sin embargo, si <span class="math inline">\((x,y)\)</span> es un caso de entrenamiento, el valor esperado
de
<span class="math display">\[(y- \hat{f}(x))^2\]</span>
requiere un cálculo más complicado, pues ¡ <span class="math inline">\(\hat{f}\)</span> también depende de <span class="math inline">\((x,y)\)</span>,
pues se construye con la muestra de entrenamiento !
Esta cantidad podría ser igual a cero para cualquier <span class="math inline">\((x,y)\)</span>
(si nuestro algoritmo “interpola” como el en la curva roja del ejemplo anterior),
y no necesariamente tiene qué ver con el error de predicción.</p>
<ul>
<li>En general, el error de entrenamiento es una cantidad secundaria, que
utilizaremos más como medida de diagnóstico de nuestro proceso de ajuste. La
cantidad que realmente queremos hacer chica es el error de predicción, que evaluamos
con una muestra de prueba independiente de la muestra de entrenamiento.</li>
<li>Para modelos muy simples, el error de entrenamiento puedes ser similar al de prueba. Sin
embargo, conforme aumentamos complejidad (necesario para capturar patrones reales
en los datos), estos dos errores típicamente divergen.</li>
</ul>
</div>
</div>
<div id="como-estimar-f" class="section level2">
<h2><span class="header-section-number">1.6</span> ¿Cómo estimar f?</h2>
<p>Ahora mostramos otro aspecto característico del aprendizaje supervisado. En primer
lugar, el método general más usual para encontrar <span class="math inline">\(\hat{f}\)</span> es hacer lo siguiente:</p>
<ul>
<li>Consideramos una familia de funciones <span class="math inline">\(h\)</span> candidatas para aproximar <span class="math inline">\(f\)</span></li>
<li>Calculamos el error de entrenamiento de cada posible <span class="math inline">\(h\)</span>, y encontramos
la <span class="math inline">\(h\)</span> que minimiza el error de entrenamiento (la que más se ajusta a los datos
de entrenamiento). Tomamos <span class="math inline">\(\hat{f} = h\)</span>.
<span class="math display">\[\hat{f} = \min_h \frac{1}{N}\sum_{i=1}^N (y^{(i)} - h(x^{(i)}))^2.\]</span></li>
<li>Evaluar el error de predicción del modelo que seleccionamos (queremos que sea bajo):</li>
</ul>
<p><span class="math display">\[\hat{Err} =   \frac{1}{m}\sum_{j=1}^m (y_0^{(j)} - \hat{f}(x_0^{(j)}))^2\]</span></p>
<p>De modo que el proceso es un problema de minimización. Lo que hace
interesante nuestro caso es que realmente <strong>no queremos minimizar el error de entrenamiento</strong>. Queremos <strong>minimizar el error de prueba</strong>. O sea que minimizamos una cantidad
que realmente no nos interesa (error de entrenamiento) con la esperanza de minimizar
la cantidad
que nos interesa (error de predicción).</p>
<p>Como es de esperarse, este esquema simple no funciona muy bien en general.
Para que la solución anterior sea razonable o buena, entonces:</p>
<ul>
<li>Tenemos que ser cuidadosos y poder regular la elección de la familia inicial de funciones
(rectas? curvas muy flexibles? etc.), y/o</li>
<li>A veces tenemos que modificar el objetivo del problema de minimización para
que nos obligue encontrar un balance adecuado de complejidad y error de
predicción bajo. Por ejemplo, penalizar el objetivo
de modelos que son poco creíbles o demasiado complicados.</li>
<li>Perturbar la muestra de entrenamiento de distintas maneras para evitar que
un algoritmo aprenda información irrelevante</li>
</ul>
<p>La mayor parte del curso se concentra en considerar qué familias podemos
utilizar, qué modificaciones de la función objetivo pueden hacerse,
y qué perturbaciones pueden considerarse mejorar el desempeño predictivo de
nuestros modelos.</p>
</div>
<div id="resumen" class="section level2">
<h2><span class="header-section-number">1.7</span> Resumen</h2>
<ul>
<li><p>Aprendizaje de máquina: algoritmos que aprenden de los datos para predecir cantidades
numéricas, o clasificar (aprendizaje supervisado), o para encontrar estructura en los
datos (aprendizaje no supervisado).</p></li>
<li>En aprendizaje supervisado, el esquema general es:
<ul>
<li>Un algoritmo aprende de una
muestra de entrenamiento <span class="math inline">\({\mathcal L}\)</span>, que es generada por el proceso generador de datos que nos interesa. Eso quiere decir que produce una función <span class="math inline">\(\hat{f}\)</span> (a partir de <span class="math inline">\({\mathcal L}\)</span>) que nos sirve para hacer predicciones <span class="math inline">\(x \to \hat{f}(x)\)</span> de <span class="math inline">\(y\)</span></li>
<li>El error de predicción del algoritmo es <span class="math inline">\(Err\)</span>, que mide en promedio qué tan lejos están las predicciones de valores reales.</li>
<li>Para estimar esta cantidad usamos una muestra de prueba <span class="math inline">\({\mathcal T}\)</span>, que
es independiente de <span class="math inline">\({\mathcal L}\)</span>.</li>
<li>Esta es porque nos interesa el desempeño futuro de <span class="math inline">\(\hat{f}\)</span> para nuevos casos
que el algoritmo no ha visto (esto es aprender).</li>
</ul></li>
<li><p>El error en la muestra de entrenamiento no necesariamente es buen indicador
del desempeño futuro de nuestro algoritmo.</p></li>
<li><p>Para obtener las mejores predicciones posibles, es necesario que el algoritmo
sea capaz de capturar patrones en los datos, pero no tanto que tienda a absorber ruido
en la estimación - es un balance de complejidad y rigidez. En términos estadísticos,
se trata de un balance de varianza y sesgo.</p></li>
</ul>
</div>
<div id="tarea" class="section level2">
<h2><span class="header-section-number">1.8</span> Tarea</h2>
<p>En el ejemplo simple que vimos en la sección <a href="introduccion.html#aprendizaje">1.4</a>, utilizamos
una sola muestra de entrenamiento para evaluar el algoritmo. ¿Será posible
que escogimos una muestra atípica?</p>
<ul>
<li>Corre el ejemplo con otra muestra y reporta tus resultados de error de entrenamiento y error de prueba para los tres métodos.</li>
<li>Opcional (difícil): evalúa los tres métodos comparando estos valores para
un número grande de distintas simulaciones de los datos de entrenamiento.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regresion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/felipegonzalez/aprendizaje-maquina-mcd/edit/master/01-introduccion.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
