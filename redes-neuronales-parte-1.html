<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Aprendizaje de máquina</title>
  <meta name="description" content="Notas y material para el curso de aprendizaje de máquina (ITAM)">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Aprendizaje de máquina" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notas y material para el curso de aprendizaje de máquina (ITAM)" />
  <meta name="github-repo" content="felipegonzalez/aprendizaje-maquina-mcd" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Aprendizaje de máquina" />
  
  <meta name="twitter:description" content="Notas y material para el curso de aprendizaje de máquina (ITAM)" />
  

<meta name="author" content="Felipe González">


<meta name="date" content="2018-11-12">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="extensiones-para-regresion-lineal-y-logistica.html">
<link rel="next" href="redes-neuronales-parte-2.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
<link rel="stylesheet" href="css/font-awesome.min.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Aprendizaje Máquina</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Temario y referencias</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#evaluacion"><i class="fa fa-check"></i>Evaluación</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-r-y-rstudio"><i class="fa fa-check"></i>Software: R y Rstudio</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#referencias-principales"><i class="fa fa-check"></i>Referencias principales</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#otras-referencias"><i class="fa fa-check"></i>Otras referencias</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduccion.html"><a href="introduccion.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="introduccion.html"><a href="introduccion.html#que-es-aprendizaje-de-maquina-machine-learning"><i class="fa fa-check"></i><b>1.1</b> ¿Qué es aprendizaje de máquina (machine learning)?</a></li>
<li class="chapter" data-level="1.2" data-path="introduccion.html"><a href="introduccion.html#aprendizaje-supervisado-1"><i class="fa fa-check"></i><b>1.2</b> Aprendizaje Supervisado</a><ul>
<li class="chapter" data-level="" data-path="introduccion.html"><a href="introduccion.html#proceso-generador-de-datos-modelo-teorico"><i class="fa fa-check"></i>Proceso generador de datos (modelo teórico)</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduccion.html"><a href="introduccion.html#predicciones"><i class="fa fa-check"></i><b>1.3</b> Predicciones</a></li>
<li class="chapter" data-level="1.4" data-path="introduccion.html"><a href="introduccion.html#aprendizaje"><i class="fa fa-check"></i><b>1.4</b> Tarea de aprendizaje supervisado</a></li>
<li class="chapter" data-level="1.5" data-path="introduccion.html"><a href="introduccion.html#error"><i class="fa fa-check"></i><b>1.5</b> Balance de complejidad y rigidez</a></li>
<li class="chapter" data-level="1.6" data-path="introduccion.html"><a href="introduccion.html#como-estimar-f"><i class="fa fa-check"></i><b>1.6</b> ¿Cómo estimar f?</a></li>
<li class="chapter" data-level="1.7" data-path="introduccion.html"><a href="introduccion.html#resumen"><i class="fa fa-check"></i><b>1.7</b> Resumen</a></li>
<li class="chapter" data-level="1.8" data-path="introduccion.html"><a href="introduccion.html#tarea"><i class="fa fa-check"></i><b>1.8</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regresion.html"><a href="regresion.html"><i class="fa fa-check"></i><b>2</b> Regresión lineal</a><ul>
<li class="chapter" data-level="2.1" data-path="introduccion.html"><a href="introduccion.html#introduccion"><i class="fa fa-check"></i><b>2.1</b> Introducción</a></li>
<li class="chapter" data-level="2.2" data-path="regresion.html"><a href="regresion.html#aprendizaje-de-coeficientes-ajuste"><i class="fa fa-check"></i><b>2.2</b> Aprendizaje de coeficientes (ajuste)</a></li>
<li class="chapter" data-level="2.3" data-path="regresion.html"><a href="regresion.html#descenso-en-gradiente"><i class="fa fa-check"></i><b>2.3</b> Descenso en gradiente</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regresion.html"><a href="regresion.html#seleccion-de-tamano-de-paso-eta"><i class="fa fa-check"></i><b>2.3.1</b> Selección de tamaño de paso <span class="math inline">\(\eta\)</span></a></li>
<li class="chapter" data-level="2.3.2" data-path="regresion.html"><a href="regresion.html#funciones-de-varias-variables"><i class="fa fa-check"></i><b>2.3.2</b> Funciones de varias variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regresion.html"><a href="regresion.html#descenso-en-gradiente-para-regresion-lineal"><i class="fa fa-check"></i><b>2.4</b> Descenso en gradiente para regresión lineal</a></li>
<li class="chapter" data-level="2.5" data-path="regresion.html"><a href="regresion.html#normalizacion-de-entradas"><i class="fa fa-check"></i><b>2.5</b> Normalización de entradas</a></li>
<li class="chapter" data-level="2.6" data-path="regresion.html"><a href="regresion.html#interpretacion-de-modelos-lineales"><i class="fa fa-check"></i><b>2.6</b> Interpretación de modelos lineales</a></li>
<li class="chapter" data-level="2.7" data-path="regresion.html"><a href="regresion.html#solucion-analitica"><i class="fa fa-check"></i><b>2.7</b> Solución analítica</a></li>
<li class="chapter" data-level="2.8" data-path="regresion.html"><a href="regresion.html#por-que-el-modelo-lineal-funciona-bien-muchas-veces"><i class="fa fa-check"></i><b>2.8</b> ¿Por qué el modelo lineal funciona bien (muchas veces)?</a><ul>
<li class="chapter" data-level="2.8.1" data-path="regresion.html"><a href="regresion.html#k-vecinos-mas-cercanos"><i class="fa fa-check"></i><b>2.8.1</b> k vecinos más cercanos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regresion.html"><a href="regresion.html#tarea-1"><i class="fa fa-check"></i>Tarea</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="logistica.html"><a href="logistica.html"><i class="fa fa-check"></i><b>3</b> Regresión logística</a><ul>
<li class="chapter" data-level="3.1" data-path="logistica.html"><a href="logistica.html#el-problema-de-clasificacion"><i class="fa fa-check"></i><b>3.1</b> El problema de clasificación</a><ul>
<li class="chapter" data-level="" data-path="logistica.html"><a href="logistica.html#que-estimar-en-problemas-de-clasificacion"><i class="fa fa-check"></i>¿Qué estimar en problemas de clasificación?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="logistica.html"><a href="logistica.html#estimacion-de-probabilidades-de-clase"><i class="fa fa-check"></i><b>3.2</b> Estimación de probabilidades de clase</a><ul>
<li class="chapter" data-level="" data-path="logistica.html"><a href="logistica.html#ejemplo-9"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="3.2.1" data-path="logistica.html"><a href="logistica.html#k-vecinos-mas-cercanos-1"><i class="fa fa-check"></i><b>3.2.1</b> k-vecinos más cercanos</a></li>
<li class="chapter" data-level="" data-path="logistica.html"><a href="logistica.html#ejemplo-11"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="logistica.html"><a href="logistica.html#error-para-modelos-de-clasificacion"><i class="fa fa-check"></i><b>3.3</b> Error para modelos de clasificación</a><ul>
<li class="chapter" data-level="3.3.1" data-path="logistica.html"><a href="logistica.html#ejercicio-1"><i class="fa fa-check"></i><b>3.3.1</b> Ejercicio</a></li>
<li class="chapter" data-level="3.3.2" data-path="logistica.html"><a href="logistica.html#error-de-clasificacion-y-funcion-de-perdida-0-1"><i class="fa fa-check"></i><b>3.3.2</b> Error de clasificación y función de pérdida 0-1</a></li>
<li class="chapter" data-level="3.3.3" data-path="logistica.html"><a href="logistica.html#discusion-relacion-entre-devianza-y-error-de-clasificacion"><i class="fa fa-check"></i><b>3.3.3</b> Discusión: relación entre devianza y error de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="logistica.html"><a href="logistica.html#regresion-logistica"><i class="fa fa-check"></i><b>3.4</b> Regresión logística</a><ul>
<li class="chapter" data-level="3.4.1" data-path="logistica.html"><a href="logistica.html#regresion-logistica-simple"><i class="fa fa-check"></i><b>3.4.1</b> Regresión logística simple</a></li>
<li class="chapter" data-level="3.4.2" data-path="logistica.html"><a href="logistica.html#funcion-logistica"><i class="fa fa-check"></i><b>3.4.2</b> Función logística</a></li>
<li class="chapter" data-level="3.4.3" data-path="logistica.html"><a href="logistica.html#regresion-logistica-1"><i class="fa fa-check"></i><b>3.4.3</b> Regresión logística</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="logistica.html"><a href="logistica.html#aprendizaje-de-coeficientes-para-regresion-logistica-binomial."><i class="fa fa-check"></i><b>3.5</b> Aprendizaje de coeficientes para regresión logística (binomial).</a></li>
<li class="chapter" data-level="3.6" data-path="logistica.html"><a href="logistica.html#ejercicio-datos-de-diabetes"><i class="fa fa-check"></i><b>3.6</b> Ejercicio: datos de diabetes</a><ul>
<li class="chapter" data-level="" data-path="logistica.html"><a href="logistica.html#tarea-2"><i class="fa fa-check"></i>Tarea</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html"><i class="fa fa-check"></i><b>4</b> Más sobre problemas de clasificación</a><ul>
<li class="chapter" data-level="4.1" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#analisis-de-error-para-clasificadores-binarios"><i class="fa fa-check"></i><b>4.1</b> Análisis de error para clasificadores binarios</a><ul>
<li class="chapter" data-level="" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#medidas-resumen-de-desempeno"><i class="fa fa-check"></i>Medidas resumen de desempeño</a></li>
<li class="chapter" data-level="" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#interpetacion-de-resumenes-de-desempeno-y-tasas-base"><i class="fa fa-check"></i>Interpetación de resúmenes de desempeño y tasas base</a></li>
<li class="chapter" data-level="4.1.1" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#puntos-de-corte-para-un-clasificador-binario"><i class="fa fa-check"></i><b>4.1.1</b> Puntos de corte para un clasificador binario</a></li>
<li class="chapter" data-level="4.1.2" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#espacio-roc-de-clasificadores"><i class="fa fa-check"></i><b>4.1.2</b> Espacio ROC de clasificadores</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#perfil-de-un-clasificador-binario-y-curvas-roc"><i class="fa fa-check"></i><b>4.2</b> Perfil de un clasificador binario y curvas ROC</a></li>
<li class="chapter" data-level="4.3" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#regresion-logistica-para-problemas-de-mas-de-2-clases"><i class="fa fa-check"></i><b>4.3</b> Regresión logística para problemas de más de 2 clases</a><ul>
<li class="chapter" data-level="4.3.1" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#regresion-logistica-multinomial"><i class="fa fa-check"></i><b>4.3.1</b> Regresión logística multinomial</a></li>
<li class="chapter" data-level="4.3.2" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#interpretacion-de-coeficientes"><i class="fa fa-check"></i><b>4.3.2</b> Interpretación de coeficientes</a></li>
<li class="chapter" data-level="4.3.3" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#ejemplo-clasificacion-de-digitos-con-regresion-multinomial"><i class="fa fa-check"></i><b>4.3.3</b> Ejemplo: Clasificación de dígitos con regresión multinomial</a></li>
<li class="chapter" data-level="" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#discusion"><i class="fa fa-check"></i>Discusión</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#descenso-en-gradiente-para-regresion-multinomial-logistica"><i class="fa fa-check"></i><b>4.4</b> Descenso en gradiente para regresión multinomial logística</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regularizacion.html"><a href="regularizacion.html"><i class="fa fa-check"></i><b>5</b> Regularización</a><ul>
<li class="chapter" data-level="5.1" data-path="regularizacion.html"><a href="regularizacion.html#sesgo-y-varianza-de-predictores"><i class="fa fa-check"></i><b>5.1</b> Sesgo y varianza de predictores</a><ul>
<li class="chapter" data-level="5.1.1" data-path="regularizacion.html"><a href="regularizacion.html#sesgo-y-varianza-en-modelos-lineales"><i class="fa fa-check"></i><b>5.1.1</b> Sesgo y varianza en modelos lineales</a></li>
<li class="chapter" data-level="5.1.2" data-path="regularizacion.html"><a href="regularizacion.html#reduciendo-varianza-de-los-coeficientes"><i class="fa fa-check"></i><b>5.1.2</b> Reduciendo varianza de los coeficientes</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="regularizacion.html"><a href="regularizacion.html#regularizacion-ridge"><i class="fa fa-check"></i><b>5.2</b> Regularización ridge</a><ul>
<li class="chapter" data-level="5.2.1" data-path="regularizacion.html"><a href="regularizacion.html#seleccion-de-coeficiente-de-regularizacion"><i class="fa fa-check"></i><b>5.2.1</b> Selección de coeficiente de regularización</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="regularizacion.html"><a href="regularizacion.html#entrenamiento-validacion-y-prueba"><i class="fa fa-check"></i><b>5.3</b> Entrenamiento, Validación y Prueba</a><ul>
<li class="chapter" data-level="5.3.1" data-path="regularizacion.html"><a href="regularizacion.html#validacion-cruzada"><i class="fa fa-check"></i><b>5.3.1</b> Validación cruzada</a></li>
<li class="chapter" data-level="5.3.2" data-path="regularizacion.html"><a href="regularizacion.html#como-se-desempena-validacion-cruzada-como-estimacion-del-error"><i class="fa fa-check"></i><b>5.3.2</b> ¿Cómo se desempeña validación cruzada como estimación del error?</a></li>
<li class="chapter" data-level="" data-path="regularizacion.html"><a href="regularizacion.html#ejercicio-4"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="regularizacion.html"><a href="regularizacion.html#regularizacion-lasso"><i class="fa fa-check"></i><b>5.4</b> Regularización lasso</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html"><i class="fa fa-check"></i><b>6</b> Extensiones para regresión lineal y logística</a><ul>
<li class="chapter" data-level="6.1" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#como-hacer-mas-flexible-el-modelo-lineal"><i class="fa fa-check"></i><b>6.1</b> Cómo hacer más flexible el modelo lineal</a></li>
<li class="chapter" data-level="6.2" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#transformacion-de-entradas"><i class="fa fa-check"></i><b>6.2</b> Transformación de entradas</a></li>
<li class="chapter" data-level="6.3" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#variables-cualitativas"><i class="fa fa-check"></i><b>6.3</b> Variables cualitativas</a></li>
<li class="chapter" data-level="6.4" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#interacciones"><i class="fa fa-check"></i><b>6.4</b> Interacciones</a></li>
<li class="chapter" data-level="6.5" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#categorizacion-de-variables"><i class="fa fa-check"></i><b>6.5</b> Categorización de variables</a></li>
<li class="chapter" data-level="6.6" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#splines-opcional"><i class="fa fa-check"></i><b>6.6</b> Splines (opcional)</a></li>
<li class="chapter" data-level="6.7" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#modelando-en-escala-logaritmica"><i class="fa fa-check"></i><b>6.7</b> Modelando en escala logarítmica</a><ul>
<li class="chapter" data-level="6.7.1" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#cuando-usar-estas-tecnicas"><i class="fa fa-check"></i><b>6.7.1</b> ¿Cuándo usar estas técnicas?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html"><i class="fa fa-check"></i><b>7</b> Redes neuronales (parte 1)</a><ul>
<li class="chapter" data-level="7.1" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#introduccion-a-redes-neuronales"><i class="fa fa-check"></i><b>7.1</b> Introducción a redes neuronales</a><ul>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#como-construyen-entradas-las-redes-neuronales"><i class="fa fa-check"></i>¿Cómo construyen entradas las redes neuronales?</a></li>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#como-ajustar-los-parametros"><i class="fa fa-check"></i>¿Cómo ajustar los parámetros?</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#interacciones-en-redes-neuronales"><i class="fa fa-check"></i><b>7.2</b> Interacciones en redes neuronales</a></li>
<li class="chapter" data-level="7.3" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#calculo-en-redes-feed-forward"><i class="fa fa-check"></i><b>7.3</b> Cálculo en redes: feed-forward</a></li>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#notacion"><i class="fa fa-check"></i>Notación</a></li>
<li class="chapter" data-level="7.4" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#feed-forward"><i class="fa fa-check"></i><b>7.4</b> Feed forward</a></li>
<li class="chapter" data-level="7.5" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#backpropagation-calculo-del-gradiente-clasificacion-binaria"><i class="fa fa-check"></i><b>7.5</b> Backpropagation: cálculo del gradiente (clasificación binaria)</a><ul>
<li class="chapter" data-level="7.5.1" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#calculo-para-un-caso-de-entrenamiento"><i class="fa fa-check"></i><b>7.5.1</b> Cálculo para un caso de entrenamiento</a></li>
<li class="chapter" data-level="7.5.2" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#algoritmo-de-backpropagation"><i class="fa fa-check"></i><b>7.5.2</b> Algoritmo de backpropagation</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#ajuste-de-parametros-introduccion"><i class="fa fa-check"></i><b>7.6</b> Ajuste de parámetros (introducción)</a><ul>
<li class="chapter" data-level="7.6.1" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#ejemplo-35"><i class="fa fa-check"></i><b>7.6.1</b> Ejemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html"><i class="fa fa-check"></i><b>8</b> Redes neuronales (parte 2)</a><ul>
<li class="chapter" data-level="8.1" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#descenso-estocastico"><i class="fa fa-check"></i><b>8.1</b> Descenso estocástico</a></li>
<li class="chapter" data-level="8.2" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#algoritmo-de-descenso-estocastico"><i class="fa fa-check"></i><b>8.2</b> Algoritmo de descenso estocástico</a></li>
<li class="chapter" data-level="8.3" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#por-que-usar-descenso-estocastico-por-minilotes"><i class="fa fa-check"></i><b>8.3</b> ¿Por qué usar descenso estocástico por minilotes?</a></li>
<li class="chapter" data-level="8.4" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#escogiendo-la-tasa-de-aprendizaje"><i class="fa fa-check"></i><b>8.4</b> Escogiendo la tasa de aprendizaje</a></li>
<li class="chapter" data-level="8.5" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#mejoras-al-algoritmo-de-descenso-estocastico."><i class="fa fa-check"></i><b>8.5</b> Mejoras al algoritmo de descenso estocástico.</a><ul>
<li class="chapter" data-level="8.5.1" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#decaimiento-de-tasa-de-aprendizaje"><i class="fa fa-check"></i><b>8.5.1</b> Decaimiento de tasa de aprendizaje</a></li>
<li class="chapter" data-level="8.5.2" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#momento"><i class="fa fa-check"></i><b>8.5.2</b> Momento</a></li>
<li class="chapter" data-level="8.5.3" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#otras-variaciones"><i class="fa fa-check"></i><b>8.5.3</b> Otras variaciones</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#ajuste-de-redes-con-descenso-estocastico"><i class="fa fa-check"></i><b>8.6</b> Ajuste de redes con descenso estocástico</a></li>
<li class="chapter" data-level="8.7" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#activaciones-relu"><i class="fa fa-check"></i><b>8.7</b> Activaciones relu</a></li>
<li class="chapter" data-level="8.8" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#dropout-para-regularizacion"><i class="fa fa-check"></i><b>8.8</b> Dropout para regularización</a><ul>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#ejemplo-39"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#ajuste-de-hiperparametros"><i class="fa fa-check"></i><b>8.9</b> Ajuste de hiperparámetros</a><ul>
<li class="chapter" data-level="8.9.1" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#ajuste-manual-de-hiperparametros"><i class="fa fa-check"></i><b>8.9.1</b> Ajuste Manual de Hiperparámetros</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#ajuste-automatico"><i class="fa fa-check"></i><b>8.10</b> Ajuste automático</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html"><i class="fa fa-check"></i><b>9</b> Redes convolucionales</a><ul>
<li class="chapter" data-level="9.1" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#filtros-convolucionales"><i class="fa fa-check"></i><b>9.1</b> Filtros convolucionales</a><ul>
<li class="chapter" data-level="" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#filtros-en-una-dimension"><i class="fa fa-check"></i>Filtros en una dimensión</a></li>
<li class="chapter" data-level="" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#filtros-convolucionales-en-dos-dimensiones"><i class="fa fa-check"></i>Filtros convolucionales en dos dimensiones</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#filtros-convolucionales-para-redes-neuronales"><i class="fa fa-check"></i><b>9.2</b> Filtros convolucionales para redes neuronales</a></li>
<li class="chapter" data-level="9.3" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#capas-de-agregacion-pooling"><i class="fa fa-check"></i><b>9.3</b> Capas de agregación (pooling)</a></li>
<li class="chapter" data-level="9.4" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#ejemplo-arquitectura-lenet"><i class="fa fa-check"></i><b>9.4</b> Ejemplo (arquitectura LeNet):</a><ul>
<li class="chapter" data-level="" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#conteo-de-parametros"><i class="fa fa-check"></i>Conteo de parámetros</a></li>
<li class="chapter" data-level="" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#pesos-y-activaciones"><i class="fa fa-check"></i>Pesos y activaciones</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html"><i class="fa fa-check"></i><b>10</b> Diagnóstico y mejora de modelos</a><ul>
<li class="chapter" data-level="10.1" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#aspectos-generales"><i class="fa fa-check"></i><b>10.1</b> Aspectos generales</a></li>
<li class="chapter" data-level="10.2" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#que-hacer-cuando-el-desempeno-no-es-satisfactorio"><i class="fa fa-check"></i><b>10.2</b> ¿Qué hacer cuando el desempeño no es satisfactorio?</a></li>
<li class="chapter" data-level="10.3" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#pipeline-de-procesamiento"><i class="fa fa-check"></i><b>10.3</b> Pipeline de procesamiento</a></li>
<li class="chapter" data-level="10.4" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#diagnosticos-sesgo-y-varianza"><i class="fa fa-check"></i><b>10.4</b> Diagnósticos: sesgo y varianza</a></li>
<li class="chapter" data-level="10.5" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#refinando-el-pipeline"><i class="fa fa-check"></i><b>10.5</b> Refinando el pipeline</a></li>
<li class="chapter" data-level="10.6" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#consiguiendo-mas-datos"><i class="fa fa-check"></i><b>10.6</b> Consiguiendo más datos</a></li>
<li class="chapter" data-level="10.7" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#usar-datos-adicionales"><i class="fa fa-check"></i><b>10.7</b> Usar datos adicionales</a></li>
<li class="chapter" data-level="10.8" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#examen-de-modelo-y-analisis-de-errores"><i class="fa fa-check"></i><b>10.8</b> Examen de modelo y Análisis de errores</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html"><i class="fa fa-check"></i><b>11</b> Validación de modelos: problemas comunes</a><ul>
<li class="chapter" data-level="11.1" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#filtracion-de-datos"><i class="fa fa-check"></i><b>11.1</b> Filtración de datos</a></li>
<li class="chapter" data-level="11.2" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#series-de-tiempo"><i class="fa fa-check"></i><b>11.2</b> Series de tiempo</a></li>
<li class="chapter" data-level="11.3" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#filtracion-en-el-preprocesamiento"><i class="fa fa-check"></i><b>11.3</b> Filtración en el preprocesamiento</a></li>
<li class="chapter" data-level="11.4" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#uso-de-variables-fuera-de-rango-temporal"><i class="fa fa-check"></i><b>11.4</b> Uso de variables fuera de rango temporal</a></li>
<li class="chapter" data-level="11.5" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#datos-en-conglomerados-y-muestreo-complejo"><i class="fa fa-check"></i><b>11.5</b> Datos en conglomerados y muestreo complejo</a><ul>
<li class="chapter" data-level="" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejemplo-41"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="11.5.1" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#censura-y-evaluacion-incompleta"><i class="fa fa-check"></i><b>11.5.1</b> Censura y evaluación incompleta</a></li>
<li class="chapter" data-level="11.5.2" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejemplo-tiendas-cerradas"><i class="fa fa-check"></i><b>11.5.2</b> Ejemplo: tiendas cerradas</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#muestras-de-validacion-chicas"><i class="fa fa-check"></i><b>11.6</b> Muestras de validación chicas</a><ul>
<li class="chapter" data-level="" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejercicio-7"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#otros-ejemplos"><i class="fa fa-check"></i><b>11.7</b> Otros ejemplos</a></li>
<li class="chapter" data-level="11.8" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#resumen-1"><i class="fa fa-check"></i><b>11.8</b> Resumen</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html"><i class="fa fa-check"></i><b>12</b> Métodos basados en árboles</a><ul>
<li class="chapter" data-level="12.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-regresion-y-clasificacion."><i class="fa fa-check"></i><b>12.1</b> Árboles para regresión y clasificación.</a><ul>
<li class="chapter" data-level="12.1.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-clasificacion"><i class="fa fa-check"></i><b>12.1.1</b> Árboles para clasificación</a></li>
<li class="chapter" data-level="12.1.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#tipos-de-particion"><i class="fa fa-check"></i><b>12.1.2</b> Tipos de partición</a></li>
<li class="chapter" data-level="12.1.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#medidas-de-impureza"><i class="fa fa-check"></i><b>12.1.3</b> Medidas de impureza</a></li>
<li class="chapter" data-level="12.1.4" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#reglas-de-particion-y-tamano-del-arobl"><i class="fa fa-check"></i><b>12.1.4</b> Reglas de partición y tamaño del árobl</a></li>
<li class="chapter" data-level="12.1.5" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#costo---complejidad-breiman"><i class="fa fa-check"></i><b>12.1.5</b> Costo - Complejidad (Breiman)</a></li>
<li class="chapter" data-level="12.1.6" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#opcional-predicciones-con-cart"><i class="fa fa-check"></i><b>12.1.6</b> (Opcional) Predicciones con CART</a></li>
<li class="chapter" data-level="12.1.7" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-regresion"><i class="fa fa-check"></i><b>12.1.7</b> Árboles para regresión</a></li>
<li class="chapter" data-level="12.1.8" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#variabilidad-en-el-proceso-de-construccion"><i class="fa fa-check"></i><b>12.1.8</b> Variabilidad en el proceso de construcción</a></li>
<li class="chapter" data-level="12.1.9" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#relaciones-lineales"><i class="fa fa-check"></i><b>12.1.9</b> Relaciones lineales</a></li>
<li class="chapter" data-level="12.1.10" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ventajas-y-desventajas-de-arboles"><i class="fa fa-check"></i><b>12.1.10</b> Ventajas y desventajas de árboles</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#bagging-de-arboles"><i class="fa fa-check"></i><b>12.2</b> Bagging de árboles</a><ul>
<li class="chapter" data-level="12.2.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ejemplo-48"><i class="fa fa-check"></i><b>12.2.1</b> Ejemplo</a></li>
<li class="chapter" data-level="12.2.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#mejorando-bagging"><i class="fa fa-check"></i><b>12.2.2</b> Mejorando bagging</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#bosques-aleatorios"><i class="fa fa-check"></i><b>12.3</b> Bosques aleatorios</a><ul>
<li class="chapter" data-level="12.3.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#sabiduria-de-las-masas"><i class="fa fa-check"></i><b>12.3.1</b> Sabiduría de las masas</a></li>
<li class="chapter" data-level="12.3.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ejemplo-49"><i class="fa fa-check"></i><b>12.3.2</b> Ejemplo</a></li>
<li class="chapter" data-level="12.3.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#mas-detalles-de-bosques-aleatorios."><i class="fa fa-check"></i><b>12.3.3</b> Más detalles de bosques aleatorios.</a></li>
<li class="chapter" data-level="12.3.4" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#importancia-de-variables"><i class="fa fa-check"></i><b>12.3.4</b> Importancia de variables</a></li>
<li class="chapter" data-level="12.3.5" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ajustando-arboles-aleatorios."><i class="fa fa-check"></i><b>12.3.5</b> Ajustando árboles aleatorios.</a></li>
<li class="chapter" data-level="12.3.6" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ventajas-y-desventajas-de-bosques-aleatorios"><i class="fa fa-check"></i><b>12.3.6</b> Ventajas y desventajas de bosques aleatorios</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html"><i class="fa fa-check"></i><b>13</b> Métodos basados en árboles: boosting</a><ul>
<li class="chapter" data-level="13.1" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#forward-stagewise-additive-modeling-fsam"><i class="fa fa-check"></i><b>13.1</b> Forward stagewise additive modeling (FSAM)</a></li>
<li class="chapter" data-level="13.2" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#discusion-1"><i class="fa fa-check"></i><b>13.2</b> Discusión</a></li>
<li class="chapter" data-level="13.3" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#algoritmo-fsam"><i class="fa fa-check"></i><b>13.3</b> Algoritmo FSAM</a></li>
<li class="chapter" data-level="13.4" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#fsam-para-clasificacion-binaria."><i class="fa fa-check"></i><b>13.4</b> FSAM para clasificación binaria.</a></li>
<li class="chapter" data-level="13.5" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#gradient-boosting"><i class="fa fa-check"></i><b>13.5</b> Gradient boosting</a></li>
<li class="chapter" data-level="13.6" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#algoritmo-de-gradient-boosting"><i class="fa fa-check"></i><b>13.6</b> Algoritmo de gradient boosting</a></li>
<li class="chapter" data-level="13.7" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#funciones-de-perdida"><i class="fa fa-check"></i><b>13.7</b> Funciones de pérdida</a><ul>
<li class="chapter" data-level="13.7.1" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#discusion-adaboost-opcional"><i class="fa fa-check"></i><b>13.7.1</b> Discusión: adaboost (opcional)</a></li>
<li class="chapter" data-level="" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#ejemplo-52"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#modificaciones-de-gradient-boosting"><i class="fa fa-check"></i><b>13.8</b> Modificaciones de Gradient Boosting</a><ul>
<li class="chapter" data-level="13.8.1" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#tasa-de-aprendizaje-shrinkage"><i class="fa fa-check"></i><b>13.8.1</b> Tasa de aprendizaje (shrinkage)</a></li>
<li class="chapter" data-level="13.8.2" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#submuestreo-bag.fraction"><i class="fa fa-check"></i><b>13.8.2</b> Submuestreo (bag.fraction)</a></li>
<li class="chapter" data-level="13.8.3" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#numero-de-arboles-m"><i class="fa fa-check"></i><b>13.8.3</b> Número de árboles M</a></li>
<li class="chapter" data-level="13.8.4" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#tamano-de-arboles"><i class="fa fa-check"></i><b>13.8.4</b> Tamaño de árboles</a></li>
<li class="chapter" data-level="13.8.5" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#controlar-numero-de-casos-para-cortes"><i class="fa fa-check"></i><b>13.8.5</b> Controlar número de casos para cortes</a></li>
<li class="chapter" data-level="" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#ejemplo-53"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="13.8.6" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#evaluacion-con-validacion-cruzada."><i class="fa fa-check"></i><b>13.8.6</b> Evaluación con validación cruzada.</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#graficas-de-dependencia-parcial"><i class="fa fa-check"></i><b>13.9</b> Gráficas de dependencia parcial</a><ul>
<li class="chapter" data-level="13.9.1" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#dependencia-parcial"><i class="fa fa-check"></i><b>13.9.1</b> Dependencia parcial</a></li>
<li class="chapter" data-level="" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#ejemplo-54"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#discusion-2"><i class="fa fa-check"></i>Discusión</a></li>
<li class="chapter" data-level="13.9.2" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#graficas-de-dependencia-parcial-para-otros-modelos"><i class="fa fa-check"></i><b>13.9.2</b> Gráficas de dependencia parcial para otros modelos</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#xgboost-y-gbm"><i class="fa fa-check"></i><b>13.10</b> xgboost y gbm</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Aprendizaje de máquina</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="redes-neuronales-parte-1" class="section level1">
<h1><span class="header-section-number">Clase 7</span> Redes neuronales (parte 1)</h1>
<div id="introduccion-a-redes-neuronales" class="section level2">
<h2><span class="header-section-number">7.1</span> Introducción a redes neuronales</h2>
<p>En la parte anterior, vimos cómo hacer más flexibles los métodos de regresión:
la idea es construir entradas derivadas a partir de las variables originales, e incluirlas en el modelo de regresión.
Este enfoque es bueno cuando tenemos relativamente pocas variables originales de entrada, y tenemos una idea de qué variables derivadas es buena idea incluir (por ejemplo, splines para una variable como edad, interacciones para variables importantes, etc). Sin embargo, si hay una gran cantidad de entradas, esta técnica puede ser prohibitiva en términos de cálculo y
trabajo manual.</p>
<p>Por ejemplo, si tenemos unas 100 entradas numéricas, al crear todas las interacciones
<span class="math inline">\(x_i x_j\)</span> y los cuadrados <span class="math inline">\(x_i^2\)</span> terminamos con unas 5150 variables. Para el problema de dígitos (256 entradas o pixeles) terminaríamos con unas 32 mil entradas adicionales. Aún cuando es posible regularizar, en estos casos suena más conveniente construir entradas derivadas a partir de los datos.</p>
<p>Para hacer esto, consideramos entradas <span class="math inline">\(X_1, . . . , X_p\)</span>, y
supongamos que tenemos un problema de clasificación binaria,
con <span class="math inline">\(G = 1\)</span> o <span class="math inline">\(G = 0\)</span>. Aunque hay muchas
maneras de construir entradas derivadas, una
manera simple sería construir <span class="math inline">\(m\)</span> nuevas entradas mediante:</p>
<p><span class="math display">\[a_k = h \left ( \theta_{k,0} + \sum_{j=1}^p \theta_{k,j}x_j
\right)\]</span></p>
<p>para <span class="math inline">\(k=1,\ldots, m\)</span>, donde <span class="math inline">\(h\)</span> es la función logística, y las <span class="math inline">\(\theta\)</span> son parámetros
que seleccionaremos más tarde. La idea es hacer <strong>combinaciones lineales</strong> de
variables <strong>transformadas</strong>.</p>
<p>Modelamos ahora la probabilidad de clase 1 con regresión logística, pero en lugar de usar las entradas originales X usamos las entradas derivadas
<span class="math inline">\(a_1, . . . , a_m\)</span>:
<span class="math display">\[p_1(x) = h \left ( \beta_0 + \sum_{j=1}^m \beta_ja_j
\right)\]</span></p>
<p>Podemos representar este esquema con una red dirigida (<span class="math inline">\(m=3\)</span> variables derivadas):</p>
<p><img src="07-redes-neuronales_files/figure-html/unnamed-chunk-2-1.png" width="600" /></p>
<p><strong>Observaciones:</strong></p>
<ul>
<li>¿Por qué usar <span class="math inline">\(h\)</span> para las entradas derivadas <span class="math inline">\(a_k\)</span>? En primer lugar,
nótese que si no transformamos con alguna función no lineal <span class="math inline">\(h\)</span>,
el modelo final <span class="math inline">\(p_1\)</span> para la probabilidad
condicional es el mismo que el de regresión logística (combinaciones lineales
de combinaciones lineales son combinaciones lineales). Sin embargo, al
transformar con <span class="math inline">\(h\)</span>, las <span class="math inline">\(x_j\)</span> contribuyen de manera no lineal a las
entradas derivadas.</li>
<li>Las variables <span class="math inline">\(a_k\)</span> que se pueden obtener son similares (para una variable de entrada)
a los I-splines que vimos en la parte anterior.</li>
<li>Es posible demostrar que si se crean suficientes entradas derivadas
(<span class="math inline">\(m\)</span> es suficientemente grande), entonces la función <span class="math inline">\(p_1(x)\)</span> puede aproximar
cualquier función continua. La función <span class="math inline">\(h\)</span> (que se llama
<strong>función de activación</strong> no es especial: funciones
continuas con forma similar a la sigmoide (logística) pueden usarse también (por ejemplo,
arcotangente, o lineal rectificada). La idea es que cualquier función se puede aproximar
mediante superposición de funciones tipo sigmoide (ver por ejemplo
Cybenko 1989, Approximation by
Superpositions of a Sigmoidal Function).</li>
</ul>
<div id="como-construyen-entradas-las-redes-neuronales" class="section level3 unnumbered">
<h3>¿Cómo construyen entradas las redes neuronales?</h3>
<p>Comencemos por un ejemplo simple de clasificación binaria
con una sola entrada <span class="math inline">\(x\)</span>. Supondremos que el modelo verdadero
está dado por:</p>
<pre class="sourceCode r"><code class="sourceCode r">h &lt;-<span class="st"> </span><span class="cf">function</span>(x){
    <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x)) <span class="co"># es lo mismo que exp(x)/(1 + exp(x))</span>
}
x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="fl">0.1</span>)
p &lt;-<span class="st"> </span><span class="kw">h</span>(<span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span>) <span class="co">#probabilidad condicional de clase 1 (vs. 0)</span>
<span class="kw">set.seed</span>(<span class="dv">2805721</span>)
x_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">30</span>, <span class="dv">-2</span>, <span class="dv">2</span>)
g_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dv">30</span>, <span class="dv">1</span>, <span class="kw">h</span>(<span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>x_<span class="dv">1</span><span class="op">^</span><span class="dv">2</span>))
datos &lt;-<span class="st"> </span><span class="kw">data.frame</span>(x_<span class="dv">1</span>, g_<span class="dv">1</span>)
dat_p &lt;-<span class="st"> </span><span class="kw">data.frame</span>(x, p)
g &lt;-<span class="st"> </span><span class="kw">qplot</span>(x, p, <span class="dt">geom=</span><span class="st">&#39;line&#39;</span>, <span class="dt">colour=</span><span class="st">&quot;red&quot;</span>)
g <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="dt">data =</span> datos, <span class="kw">aes</span>(<span class="dt">x =</span> x_<span class="dv">1</span>, <span class="dt">y =</span> g_<span class="dv">1</span>), <span class="dt">colour =</span> <span class="st">&#39;red&#39;</span>)</code></pre>
<p><img src="07-redes-neuronales_files/figure-html/unnamed-chunk-3-1.png" width="480" /></p>
<p>donde adicionalmente graficamos 30 datos simulados. Recordamos que queremos
ajustar la curva roja, que da la probabilidad condicional de clase.
Podríamos ajustar
un modelo de regresión logística expandiendo manualmente el espacio de entradas
agregando <span class="math inline">\(x^2\)</span>, y obtendríamos un ajuste razonable. Pero la idea aquí es
que podemos crear entradas derivadas de forma automática.</p>
<p>Supongamos entonces que pensamos crear dos entradas <span class="math inline">\(a_1\)</span> y <span class="math inline">\(a_2\)</span>, funciones
de <span class="math inline">\(x_1\)</span>, y luego predecir <span class="math inline">\(g.1\)</span>, la clase, en función de estas dos entradas.
Por ejemplo, podríamos tomar:</p>
<p><img src="07-redes-neuronales_files/figure-html/unnamed-chunk-4-1.png" width="500" /></p>
<p>donde hacemos una regresión logística para predecir <span class="math inline">\(G\)</span> mediante
<span class="math display">\[p_1(a) = h(\beta_0 + \beta_1a_1+\beta_2 a_2),\]</span>
<span class="math inline">\(a_1\)</span> y <span class="math inline">\(a_2\)</span> están dadas por
<span class="math display">\[a_1(x)=h(\beta_{1,0} + \beta_{1,1} x_1),\]</span>
<span class="math display">\[a_2(x)=h(\beta_{2,0} + \beta_{2,1} x_1).\]</span></p>
<p>Por ejemplo, podríamos tomar</p>
<pre class="sourceCode r"><code class="sourceCode r">a_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">h</span>( <span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>x)  <span class="co"># 2(x+1/2)</span>
a_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">h</span>(<span class="op">-</span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>x)  <span class="co"># 2(x-1/2) # una es una versión desplazada de otra.</span></code></pre>
<p>Las funciones <span class="math inline">\(a_1\)</span> y <span class="math inline">\(a_2\)</span> dependen de <span class="math inline">\(x\)</span> de la siguiente forma:</p>
<pre class="sourceCode r"><code class="sourceCode r">dat_a &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x =</span> x, <span class="dt">a_1 =</span> a_<span class="dv">1</span>, <span class="dt">a_2 =</span> a_<span class="dv">2</span>)
dat_a_<span class="dv">2</span> &lt;-<span class="st"> </span>dat_a <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">gather</span>(variable, valor, a_<span class="dv">1</span><span class="op">:</span>a_<span class="dv">2</span>)
<span class="kw">ggplot</span>(dat_a_<span class="dv">2</span>, <span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>valor, <span class="dt">colour=</span>variable, <span class="dt">group=</span>variable)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()</code></pre>
<p><img src="07-redes-neuronales_files/figure-html/unnamed-chunk-6-1.png" width="480" /></p>
<p>Si las escalamos y sumamos, obtenemos</p>
<pre class="sourceCode r"><code class="sourceCode r">dat_a &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span>x, <span class="dt">a_1 =</span> <span class="dv">-4</span> <span class="op">+</span><span class="st"> </span><span class="dv">12</span> <span class="op">*</span><span class="st"> </span>a_<span class="dv">1</span>, <span class="dt">a_2 =</span> <span class="dv">-12</span> <span class="op">*</span><span class="st"> </span>a_<span class="dv">2</span>, <span class="dt">suma =</span> <span class="dv">-4</span> <span class="op">+</span><span class="st"> </span><span class="dv">12</span> <span class="op">*</span><span class="st"> </span>a_<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="dv">12</span> <span class="op">*</span><span class="st"> </span>a_<span class="dv">2</span>)
dat_a_<span class="dv">2</span> &lt;-<span class="st"> </span>dat_a <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">gather</span>(variable, valor, a_<span class="dv">1</span><span class="op">:</span>suma)
<span class="kw">ggplot</span>(dat_a_<span class="dv">2</span>, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> valor, <span class="dt">colour =</span> variable, <span class="dt">group =</span> variable)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()</code></pre>
<p><img src="07-redes-neuronales_files/figure-html/unnamed-chunk-7-1.png" width="480" /></p>
<p>y finalmente, aplicando <span class="math inline">\(h\)</span>:</p>
<pre class="sourceCode r"><code class="sourceCode r">dat_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">data.frame</span>(x, <span class="dt">p2 =</span> <span class="kw">h</span>(<span class="op">-</span><span class="dv">4</span> <span class="op">+</span><span class="st"> </span><span class="dv">12</span> <span class="op">*</span><span class="st"> </span>a_<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="dv">12</span> <span class="op">*</span><span class="st"> </span>a_<span class="dv">2</span>))
<span class="kw">ggplot</span>(dat_<span class="dv">2</span>, <span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>p2)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()<span class="op">+</span>
<span class="kw">geom_line</span>(<span class="dt">data=</span>dat_p, <span class="kw">aes</span>(<span class="dt">x=</span>x,<span class="dt">y=</span>p), <span class="dt">col=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span><span class="kw">ylim</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))<span class="op">+</span>
<span class="st">   </span><span class="kw">geom_point</span>(<span class="dt">data =</span> datos, <span class="kw">aes</span>(<span class="dt">x =</span> x_<span class="dv">1</span>, <span class="dt">y =</span> g_<span class="dv">1</span>))</code></pre>
<p><img src="07-redes-neuronales_files/figure-html/unnamed-chunk-8-1.png" width="480" /></p>
<p>que da un ajuste razonable. Este es un ejemplo de cómo
la mezcla de dos funciones logísticas puede
replicar esta función con forma de chipote.</p>
</div>
<div id="como-ajustar-los-parametros" class="section level3 unnumbered">
<h3>¿Cómo ajustar los parámetros?</h3>
<p>Para encontrar los mejores parámetros,
minimizamos la devianza sobre los
parámetros <span class="math inline">\(\beta_0,\beta_1,\beta_{1,0},\beta_{1,1}, \beta_{2,0},\beta_{2,1}\)</span>.</p>
<p>Veremos más adelante que conviene hacer esto usando descenso o en gradiente
o descenso en gradiente estocástico, pero por el momento
usamos la función <em>optim</em> de R para
minimizar la devianza. En primer lugar, creamos una
función que para todas las entradas calcula los valores
de salida. En esta función hacemos <strong>feed-forward</strong> de las entradas
a través de la red para calcular la salida.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">## esta función calcula los valores de cada nodo en toda la red,</span>
<span class="co">## para cada entrada</span>
feed_fow &lt;-<span class="st"> </span><span class="cf">function</span>(beta, x){
  a_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">h</span>(beta[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>beta[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>x) <span class="co"># calcula variable 1 de capa oculta</span>
  a_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">h</span>(beta[<span class="dv">3</span>] <span class="op">+</span><span class="st"> </span>beta[<span class="dv">4</span>] <span class="op">*</span><span class="st"> </span>x) <span class="co"># calcula variable 2 de capa oculta</span>
  p &lt;-<span class="st"> </span><span class="kw">h</span>(beta[<span class="dv">5</span>] <span class="op">+</span><span class="st"> </span>beta[<span class="dv">6</span>] <span class="op">*</span><span class="st"> </span>a_<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>beta[<span class="dv">7</span>] <span class="op">*</span><span class="st"> </span>a_<span class="dv">2</span>) <span class="co"># calcula capa de salida</span>
  p
}</code></pre>
<p>Nótese que simplemente seguimos el diagrama mostrado arriba
para hacer los cálculos, combinando linealmente las entradas en cada
capa.</p>
<p>Ahora definimos una función para calcular la devianza. Conviene
crear una función que crea funciones, para obtener una función
que <em>sólo se evalúa en los parámetros</em> para cada conjunto
de datos de entrenamiento fijos:</p>
<pre class="sourceCode r"><code class="sourceCode r">devianza_fun &lt;-<span class="st"> </span><span class="cf">function</span>(x, y){
    <span class="co"># esta función es una fábrica de funciones</span>
   devianza &lt;-<span class="st"> </span><span class="cf">function</span>(beta){
         p &lt;-<span class="st"> </span><span class="kw">feed_fow</span>(beta, x)
      <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(y<span class="op">*</span><span class="kw">log</span>(p) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span>p))
   }
  devianza
}</code></pre>
<p>Por ejemplo:</p>
<pre class="sourceCode r"><code class="sourceCode r">dev &lt;-<span class="st"> </span><span class="kw">devianza_fun</span>(x_<span class="dv">1</span>, g_<span class="dv">1</span>) <span class="co"># crea función dev</span>
<span class="co">## ahora dev toma solamente los 7 parámetros beta:</span>
<span class="kw">dev</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>))</code></pre>
<pre><code>## [1] 1.386294</code></pre>
<p>Finalmente, optimizamos la devianza. Para esto usaremos
la función <em>optim</em> de R:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">5</span>)
salida &lt;-<span class="st"> </span><span class="kw">optim</span>(<span class="kw">rnorm</span>(<span class="dv">7</span>), dev, <span class="dt">method =</span> <span class="st">&#39;BFGS&#39;</span>) <span class="co"># inicializar al azar punto inicial</span>
salida</code></pre>
<pre><code>## $par
## [1] -24.8192568  23.0201169  -8.4364869  -6.7633494   0.9849461 -14.0157655
## [7] -14.3394673
## 
## $value
## [1] 0.654347
## 
## $counts
## function gradient 
##      103      100 
## 
## $convergence
## [1] 1
## 
## $message
## NULL</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">beta &lt;-<span class="st"> </span>salida<span class="op">$</span>par</code></pre>
<p>Y ahora podemos graficar con el vector <span class="math inline">\(\beta\)</span> encontrado:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">## hacer feed forward con beta encontrados</span>
p_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">feed_fow</span>(beta, x)
dat_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">data.frame</span>(x, <span class="dt">p_2 =</span> p_<span class="dv">2</span>)
<span class="kw">ggplot</span>(dat_<span class="dv">2</span>, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> p_<span class="dv">2</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()<span class="op">+</span>
<span class="kw">geom_line</span>(<span class="dt">data =</span> dat_p, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> p), <span class="dt">col=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span><span class="kw">ylim</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))<span class="op">+</span>
<span class="st">   </span><span class="kw">geom_point</span>(<span class="dt">data =</span> datos, <span class="kw">aes</span>(<span class="dt">x =</span> x_<span class="dv">1</span>, <span class="dt">y =</span> g_<span class="dv">1</span>))</code></pre>
<p><img src="07-redes-neuronales_files/figure-html/unnamed-chunk-13-1.png" width="480" /></p>
<p>Los coeficientes estimados, que en este caso muchas veces se llaman
<em>pesos</em>, son:</p>
<pre class="sourceCode r"><code class="sourceCode r">beta</code></pre>
<pre><code>## [1] -24.8192568  23.0201169  -8.4364869  -6.7633494   0.9849461 -14.0157655
## [7] -14.3394673</code></pre>
<p>que parecen ser muy grandes. Igualmente, de la figura
vemos que el ajuste no parece ser muy estable (esto se puede
confirmar corriendo con distintos conjuntos de entrenamiento).
Podemos entonces regularizar ligeramente la devianza
para resolver este problema. En primer lugar, definimos la
devianza regularizada (ridge), donde penalizamos todos los coeficientes
que multiplican a una variable, pero no los intercepts:</p>
<pre class="sourceCode r"><code class="sourceCode r">devianza_reg &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, lambda){
    <span class="co"># esta función es una fábrica de funciones</span>
   devianza &lt;-<span class="st"> </span><span class="cf">function</span>(beta){
         p &lt;-<span class="st"> </span><span class="kw">feed_fow</span>(beta, x)
         <span class="co"># en esta regularizacion quitamos sesgos, pero puede hacerse también con sesgos.</span>
        <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(y<span class="op">*</span><span class="kw">log</span>(p) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span>p)) <span class="op">+</span><span class="st"> </span>lambda<span class="op">*</span><span class="kw">sum</span>(beta[<span class="op">-</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>)]<span class="op">^</span><span class="dv">2</span>) 
   }
  devianza
}</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">dev_r &lt;-<span class="st"> </span><span class="kw">devianza_reg</span>(x_<span class="dv">1</span>, g_<span class="dv">1</span>, <span class="fl">0.001</span>) <span class="co"># crea función dev</span>
<span class="kw">set.seed</span>(<span class="dv">5</span>)
salida &lt;-<span class="st"> </span><span class="kw">optim</span>(<span class="kw">rnorm</span>(<span class="dv">7</span>), dev_r, <span class="dt">method=</span><span class="st">&#39;BFGS&#39;</span>) <span class="co"># inicializar al azar punto inicial</span>
salida</code></pre>
<pre><code>## $par
## [1] -4.826652  4.107146 -4.845864 -4.561488  1.067216 -5.236453 -5.195981
## 
## $value
## [1] 0.8322745
## 
## $counts
## function gradient 
##      102      100 
## 
## $convergence
## [1] 1
## 
## $message
## NULL</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">beta &lt;-<span class="st"> </span>salida<span class="op">$</span>par
<span class="kw">dev</span>(beta)</code></pre>
<pre><code>## [1] 0.74018</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">p_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">feed_fow</span>(beta, x)
dat_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">data.frame</span>(x, <span class="dt">p_2 =</span> p_<span class="dv">2</span>)
<span class="kw">ggplot</span>(dat_<span class="dv">2</span>, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> p_<span class="dv">2</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()<span class="op">+</span>
<span class="kw">geom_line</span>(<span class="dt">data =</span> dat_p, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> p), <span class="dt">col=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span><span class="kw">ylim</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))<span class="op">+</span>
<span class="st">   </span><span class="kw">geom_point</span>(<span class="dt">data =</span> datos, <span class="kw">aes</span>(<span class="dt">x =</span> x_<span class="dv">1</span>, <span class="dt">y =</span> g_<span class="dv">1</span>))</code></pre>
<p><img src="07-redes-neuronales_files/figure-html/unnamed-chunk-16-1.png" width="480" /></p>
<p>y obtenemos un ajuste mucho más estable. Podemos también usar
la función <em>nnet</em> del paquete <em>nnet</em>. Ojo: en <em>nnet</em>,
el error es la devianza no está normalizada por número de casos y dividida entre dos:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(nnet)
<span class="kw">set.seed</span>(<span class="dv">12</span>)
nn &lt;-<span class="st"> </span><span class="kw">nnet</span>(g_<span class="dv">1</span> <span class="op">~</span><span class="st"> </span>x_<span class="dv">1</span>, <span class="dt">data =</span> datos, <span class="dt">size =</span> <span class="dv">2</span>, <span class="dt">decay =</span> <span class="fl">0.0</span>, <span class="dt">entropy =</span> T)</code></pre>
<pre><code>## # weights:  7
## initial  value 19.318858 
## iter  10 value 11.967705
## iter  20 value 10.251964
## iter  30 value 9.647707
## iter  40 value 9.573030
## iter  50 value 9.569389
## iter  60 value 9.555125
## iter  70 value 9.546210
## iter  80 value 9.544512
## iter  90 value 9.539825
## iter 100 value 9.535977
## final  value 9.535977 
## stopped after 100 iterations</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">nn<span class="op">$</span>wts</code></pre>
<pre><code>## [1] -51.274012  48.789640   8.764849   6.219901 -29.155181 -24.998108
## [7]  30.125349</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">nn<span class="op">$</span>value</code></pre>
<pre><code>## [1] 9.535977</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span><span class="op">*</span>nn<span class="op">$</span>value<span class="op">/</span><span class="dv">30</span></code></pre>
<pre><code>## [1] 0.6357318</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dev</span>(nn<span class="op">$</span>wts) </code></pre>
<pre><code>## [1] 0.6357318</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(x, <span class="kw">predict</span>(nn, <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">x_1 =</span> x)), <span class="dt">geom=</span><span class="st">&#39;line&#39;</span>)</code></pre>
<p><img src="07-redes-neuronales_files/figure-html/unnamed-chunk-18-1.png" width="480" /></p>
<div id="ejercicio-red" class="section level4">
<h4><span class="header-section-number">7.1.0.1</span> Ejercicio</h4>
<p>Un ejemplo más complejo. Utiliza los siguientes datos, y agrega
si es necesario variables derivadas <span class="math inline">\(a_3, a_4\)</span> en la capa oculta.</p>
<pre class="sourceCode r"><code class="sourceCode r">h &lt;-<span class="st"> </span><span class="cf">function</span>(x){
    <span class="kw">exp</span>(x)<span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(x))
}
x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>,<span class="fl">0.05</span>)
p &lt;-<span class="st"> </span><span class="kw">h</span>(<span class="dv">3</span> <span class="op">+</span><span class="st"> </span>x<span class="op">-</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>x <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(<span class="dv">4</span> <span class="op">*</span><span class="st"> </span>x))
<span class="kw">set.seed</span>(<span class="dv">280572</span>)
x<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">300</span>, <span class="dv">-2</span>, <span class="dv">2</span>)
g<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dv">300</span>, <span class="dv">1</span>, <span class="kw">h</span>(<span class="dv">3</span> <span class="op">+</span><span class="st"> </span>x<span class="fl">.2</span> <span class="op">-</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>x<span class="fl">.2</span> <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(<span class="dv">4</span> <span class="op">*</span><span class="st"> </span>x<span class="fl">.2</span>)))
datos &lt;-<span class="st"> </span><span class="kw">data.frame</span>(x<span class="fl">.2</span>,g<span class="fl">.2</span>)
dat.p &lt;-<span class="st"> </span><span class="kw">data.frame</span>(x,p)
g &lt;-<span class="st"> </span><span class="kw">qplot</span>(x,p, <span class="dt">geom=</span><span class="st">&#39;line&#39;</span>, <span class="dt">col=</span><span class="st">&#39;red&#39;</span>)
g <span class="op">+</span><span class="st"> </span><span class="kw">geom_jitter</span>(<span class="dt">data =</span> datos, <span class="kw">aes</span>(<span class="dt">x=</span>x<span class="fl">.2</span>,<span class="dt">y=</span>g<span class="fl">.2</span>), <span class="dt">col =</span><span class="st">&#39;black&#39;</span>,
  <span class="dt">position =</span><span class="kw">position_jitter</span>(<span class="dt">height=</span><span class="fl">0.05</span>), <span class="dt">alpha=</span><span class="fl">0.4</span>)</code></pre>
<p><img src="07-redes-neuronales_files/figure-html/unnamed-chunk-19-1.png" width="480" /></p>
</div>
</div>
</div>
<div id="interacciones-en-redes-neuronales" class="section level2">
<h2><span class="header-section-number">7.2</span> Interacciones en redes neuronales</h2>
<p>Es posible capturar interacciones con redes neuronales. Consideremos el siguiente
ejemplo simple:</p>
<pre class="sourceCode r"><code class="sourceCode r">p &lt;-<span class="st"> </span><span class="cf">function</span>(x1, x2){
  <span class="kw">h</span>(<span class="op">-</span><span class="dv">5</span> <span class="op">+</span><span class="st"> </span><span class="dv">10</span><span class="op">*</span>x1 <span class="op">+</span><span class="st"> </span><span class="dv">10</span><span class="op">*</span>x2 <span class="op">-</span><span class="st"> </span><span class="dv">30</span><span class="op">*</span>x1<span class="op">*</span>x2)
}
dat &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">x1 =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.05</span>), <span class="dt">x2 =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.05</span>))
dat &lt;-<span class="st"> </span>dat <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">p =</span> <span class="kw">p</span>(x1, x2))
<span class="kw">ggplot</span>(dat, <span class="kw">aes</span>(<span class="dt">x=</span>x1, <span class="dt">y=</span>x2)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_tile</span>(<span class="kw">aes</span>(<span class="dt">fill=</span>p))</code></pre>
<p><img src="07-redes-neuronales_files/figure-html/unnamed-chunk-20-1.png" width="480" /></p>
<p>Esta función puede entenderse como un o exclusivo: la probabilidad es alta
sólo cuando <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span> tienen valores opuestos (<span class="math inline">\(x_1\)</span> grande pero <span class="math inline">\(x_2\)</span> chica y viceversa).</p>
<p>No es posible modelar esta función mediante el modelo logístico (sin interacciones).
Pero podemos incluir la interacción en el modelo logístico o intentar
usar una red neuronal. Primero simulamos unos datos y probamos el modelo logístico
con y sin interacciones:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">322</span>)
n &lt;-<span class="st"> </span><span class="dv">500</span>
dat_ent &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x1=</span><span class="kw">runif</span>(n,<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">x2 =</span> <span class="kw">runif</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">p =</span> <span class="kw">p</span>(x1, x2)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">rbinom</span>(n, <span class="dv">1</span>, p))
mod_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> dat_ent, <span class="dt">family =</span> <span class="st">&#39;binomial&#39;</span>)
mod_<span class="dv">1</span></code></pre>
<pre><code>## 
## Call:  glm(formula = y ~ x1 + x2, family = &quot;binomial&quot;, data = dat_ent)
## 
## Coefficients:
## (Intercept)           x1           x2  
##    -0.01011     -1.47942     -1.19196  
## 
## Degrees of Freedom: 499 Total (i.e. Null);  497 Residual
## Null Deviance:       529.4 
## Residual Deviance: 504.5     AIC: 510.5</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(<span class="kw">predict</span>(mod_<span class="dv">1</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, dat_ent<span class="op">$</span>y)</code></pre>
<pre><code>##        
##           0   1
##   FALSE 389 111</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">mod_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span>x1<span class="op">:</span>x2, <span class="dt">data =</span> dat_ent, <span class="dt">family =</span> <span class="st">&#39;binomial&#39;</span>)
mod_<span class="dv">2</span></code></pre>
<pre><code>## 
## Call:  glm(formula = y ~ x1 + x2 + x1:x2, family = &quot;binomial&quot;, data = dat_ent)
## 
## Coefficients:
## (Intercept)           x1           x2        x1:x2  
##      -4.726        9.641        9.831      -32.466  
## 
## Degrees of Freedom: 499 Total (i.e. Null);  496 Residual
## Null Deviance:       529.4 
## Residual Deviance: 305.6     AIC: 313.6</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(<span class="kw">predict</span>(mod_<span class="dv">2</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, dat_ent<span class="op">$</span>y)</code></pre>
<pre><code>##        
##           0   1
##   FALSE 374  60
##   TRUE   15  51</code></pre>
<p>Observese la gran diferencia de devianza entre los dos modelos (en este caso,
el sobreajuste no es un problema).</p>
<p>Ahora consideramos qué red neuronal puede ser apropiada.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">11</span>)
nn &lt;-<span class="st"> </span><span class="kw">nnet</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> dat_ent, <span class="dt">size =</span> <span class="dv">3</span>, <span class="dt">decay =</span> <span class="fl">0.001</span>, 
           <span class="dt">entropy =</span> T, <span class="dt">maxit =</span> <span class="dv">500</span>)</code></pre>
<pre><code>## # weights:  13
## initial  value 294.186925 
## iter  10 value 233.560013
## iter  20 value 195.096851
## iter  30 value 190.466423
## iter  40 value 184.454612
## iter  50 value 170.767082
## iter  60 value 156.347417
## iter  70 value 153.521658
## iter  80 value 153.069566
## iter  90 value 152.852374
## iter 100 value 152.835812
## iter 110 value 152.826924
## iter 120 value 152.825819
## final  value 152.825815 
## converged</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#primera capa</span>
<span class="kw">matrix</span>(<span class="kw">round</span>(nn<span class="op">$</span>wts[<span class="dv">1</span><span class="op">:</span><span class="dv">9</span>], <span class="dv">1</span>), <span class="dv">3</span>,<span class="dv">3</span>, <span class="dt">byrow=</span>T)</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,] -2.2  3.0 -2.4
## [2,] -8.2  5.9  8.7
## [3,] -2.7 -1.6  3.6</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#segunda capa</span>
<span class="kw">round</span>(nn<span class="op">$</span>wts[<span class="dv">10</span><span class="op">:</span><span class="dv">13</span>], <span class="dv">1</span>)</code></pre>
<pre><code>## [1] -5.7 15.1 -8.6 19.8</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#2*nn$value</span></code></pre>
<p>El cálculo de esta red es:</p>
<pre class="sourceCode r"><code class="sourceCode r">feed_fow &lt;-<span class="st"> </span><span class="cf">function</span>(beta, x){
  a_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">h</span>(beta[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>beta[<span class="dv">2</span>]<span class="op">*</span>x[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>beta[<span class="dv">3</span>]<span class="op">*</span>x[<span class="dv">2</span>]) 
  a_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">h</span>(beta[<span class="dv">4</span>] <span class="op">+</span><span class="st"> </span>beta[<span class="dv">5</span>]<span class="op">*</span>x[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>beta[<span class="dv">6</span>]<span class="op">*</span>x[<span class="dv">2</span>]) 
  a_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">h</span>(beta[<span class="dv">7</span>] <span class="op">+</span><span class="st"> </span>beta[<span class="dv">8</span>]<span class="op">*</span>x[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>beta[<span class="dv">9</span>]<span class="op">*</span>x[<span class="dv">2</span>])
  p &lt;-<span class="st"> </span><span class="kw">h</span>(beta[<span class="dv">10</span>]<span class="op">+</span>beta[<span class="dv">11</span>]<span class="op">*</span>a_<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>beta[<span class="dv">12</span>]<span class="op">*</span>a_<span class="dv">2</span> <span class="op">+</span><span class="st"> </span>beta[<span class="dv">13</span>]<span class="op">*</span>a_<span class="dv">3</span>) <span class="co"># calcula capa de salida</span>
  p
}</code></pre>
<p>Y vemos que esta red captura la interacción:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">feed_fow</span>(nn<span class="op">$</span>wts, <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>))</code></pre>
<pre><code>## [1] 0.04946031</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">feed_fow</span>(nn<span class="op">$</span>wts, <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</code></pre>
<pre><code>## [1] 0.9560235</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">feed_fow</span>(nn<span class="op">$</span>wts, <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">0</span>))</code></pre>
<pre><code>## [1] 0.9830594</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">feed_fow</span>(nn<span class="op">$</span>wts, <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre>
<pre><code>## [1] 0.004197137</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">dat &lt;-<span class="st"> </span>dat <span class="op">%&gt;%</span><span class="st"> </span>rowwise <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">p_red =</span> <span class="kw">feed_fow</span>(nn<span class="op">$</span>wts, <span class="kw">c</span>(x1, x2)))
<span class="kw">ggplot</span>(dat, <span class="kw">aes</span>(<span class="dt">x=</span>x1, <span class="dt">y=</span>x2)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_tile</span>(<span class="kw">aes</span>(<span class="dt">fill=</span>p_red))</code></pre>
<p><img src="07-redes-neuronales_files/figure-html/unnamed-chunk-25-1.png" width="480" /></p>
<p><strong>Observación</strong>: ¿cómo funciona esta red? Consideremos la capa intermedia.</p>
<pre class="sourceCode r"><code class="sourceCode r">dat_entrada &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x_1 =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">x_2 =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>))
a_<span class="dv">1</span> &lt;-<span class="st"> </span>dat_entrada <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">rowwise</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">a_1 =</span> <span class="kw">h</span>(<span class="kw">sum</span>(nn<span class="op">$</span>wts[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>] <span class="op">*</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,x_<span class="dv">1</span>,x_<span class="dv">2</span>) )))
a_<span class="dv">2</span> &lt;-<span class="st"> </span>dat_entrada <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">rowwise</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">a_2 =</span> <span class="kw">h</span>(<span class="kw">sum</span>(nn<span class="op">$</span>wts[<span class="dv">4</span><span class="op">:</span><span class="dv">6</span>] <span class="op">*</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,x_<span class="dv">1</span>,x_<span class="dv">2</span>) )))
a_<span class="dv">3</span> &lt;-<span class="st"> </span>dat_entrada <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">rowwise</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">a_3 =</span> <span class="kw">h</span>(<span class="kw">sum</span>(nn<span class="op">$</span>wts[<span class="dv">7</span><span class="op">:</span><span class="dv">9</span>] <span class="op">*</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,x_<span class="dv">1</span>,x_<span class="dv">2</span>) )))
capa_intermedia &lt;-<span class="st"> </span><span class="kw">left_join</span>(a_<span class="dv">1</span>, a_<span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">left_join</span>(a_<span class="dv">3</span>)</code></pre>
<pre><code>## Joining, by = c(&quot;x_1&quot;, &quot;x_2&quot;)
## Joining, by = c(&quot;x_1&quot;, &quot;x_2&quot;)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">a_<span class="dv">1</span></code></pre>
<pre><code>## Source: local data frame [4 x 3]
## Groups: &lt;by row&gt;
## 
## # A tibble: 4 x 3
##     x_1   x_2    a_1
##   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
## 1     0     0 0.102 
## 2     0     1 0.0101
## 3     1     0 0.686 
## 4     1     1 0.164</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">a_<span class="dv">3</span></code></pre>
<pre><code>## Source: local data frame [4 x 3]
## Groups: &lt;by row&gt;
## 
## # A tibble: 4 x 3
##     x_1   x_2    a_3
##   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
## 1     0     0 0.0629
## 2     0     1 0.709 
## 3     1     0 0.0130
## 4     1     1 0.324</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">a_<span class="dv">2</span></code></pre>
<pre><code>## Source: local data frame [4 x 3]
## Groups: &lt;by row&gt;
## 
## # A tibble: 4 x 3
##     x_1   x_2      a_2
##   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
## 1     0     0 0.000284
## 2     0     1 0.621   
## 3     1     0 0.0960  
## 4     1     1 0.998</code></pre>
<p>Y observamos que las unidades <span class="math inline">\(a_1\)</span> y <span class="math inline">\(a_3\)</span> tienen valor alto cuando
las variables <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span>, correspondientemente, tienen valores altos.
La unidad <span class="math inline">\(a_2\)</span> responde cuando tanto como <span class="math inline">\(x_1\)</span>y <span class="math inline">\(x_2\)</span> tienen valores altos.</p>
<p>En la capa final, le damos peso relativamente alto a las unidades <span class="math inline">\(a_1\)</span> y <span class="math inline">\(a_3\)</span>,
y peso negativo a la unidad <span class="math inline">\(a_2\)</span></p>
<pre class="sourceCode r"><code class="sourceCode r">nn<span class="op">$</span>wts[<span class="dv">10</span><span class="op">:</span><span class="dv">13</span>]</code></pre>
<pre><code>## [1] -5.747250 15.138708 -8.628917 19.801144</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">capa_final &lt;-<span class="st"> </span>capa_intermedia <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">rowwise</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">p=</span> <span class="kw">h</span>(<span class="kw">sum</span>(nn<span class="op">$</span>wts[<span class="dv">10</span><span class="op">:</span><span class="dv">13</span>]<span class="op">*</span><span class="kw">c</span>(<span class="dv">1</span>,a_<span class="dv">1</span>,a_<span class="dv">2</span>,a_<span class="dv">3</span>) ))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">p=</span><span class="kw">round</span>(p,<span class="dv">2</span>))
capa_final</code></pre>
<pre><code>## Source: local data frame [4 x 6]
## Groups: &lt;by row&gt;
## 
## # A tibble: 4 x 6
##     x_1   x_2    a_1      a_2    a_3     p
##   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1     0     0 0.102  0.000284 0.0629  0.05
## 2     0     1 0.0101 0.621    0.709   0.96
## 3     1     0 0.686  0.0960   0.0130  0.98
## 4     1     1 0.164  0.998    0.324   0</code></pre>
</div>
<div id="calculo-en-redes-feed-forward" class="section level2">
<h2><span class="header-section-number">7.3</span> Cálculo en redes: feed-forward</h2>
<p>Ahora generalizamos lo que vimos arriba para definir la arquitectura
básica de redes neuronales y cómo se hacen cálculos en las redes.</p>

<div class="comentario">
<p>A las variables originales les llamamos <em>capa de entrada</em> de la red,
y a la variable de salida <em>capa de salida</em>. Puede haber más de una
capa intermedia. A estas les llamamos <em>capas ocultas</em>.</p>
Cuando todas las conexiones posibles de cada capa a la siguiente están presente,
decimos que la red es <em>completamente conexa</em>.
</div>

<p><img src="07-redes-neuronales_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>Como vimos en el ejemplo de arriba, para hacer cálculos en la red empezamos
con la primera capa, hacemos combinaciones lineales y aplicamos nuestra función
no lineal <span class="math inline">\(h\)</span>. Una vez que calculamos la segunda capa, podemos calcular
la siguiente de la misma forma: combinaciones lineales y aplicación de <span class="math inline">\(h\)</span>. Y así
sucesivamente hasta que llegamos a la capa final.</p>
</div>
<div id="notacion" class="section level2 unnumbered">
<h2>Notación</h2>
<p>Sea <span class="math inline">\(L\)</span> el número total de capas. En primer lugar, para un cierto caso de entrada <span class="math inline">\(x = (x_1,x_2,\ldots, x_p)\)</span>,
denotamos por:</p>
<ul>
<li><span class="math inline">\(a^{(l)}_j\)</span> el valor que toma la unidad <span class="math inline">\(j\)</span> de la capa <span class="math inline">\(l\)</span>, para <span class="math inline">\(j=0,1,\ldots, n_{l}\)</span>, donde
<span class="math inline">\(n_l\)</span> es el número de unidades de la capa <span class="math inline">\(l\)</span>.</li>
<li>Ponemos <span class="math inline">\(a^{(l)}_0=1\)</span> para lidiar con los sesgos.</li>
<li>En particular, ponemos <span class="math inline">\(a^{(1)}_j = x_j\)</span>, que son los valores de las entradas (primera capa)</li>
<li>Para clasificación binaria, la última capa solo tiene un elemento, que es
<span class="math inline">\(p_1 = a^{(L)}\)</span>. Para un problema de clasificación en <span class="math inline">\(K&gt;2\)</span> clases, tenemos que
la última capa es de tamaño <span class="math inline">\(K\)</span>:
<span class="math inline">\(p_1 = a^{(L)}_1, p_2 = a^{(L)}_2,\ldots, p_K = a^{(L)}_K\)</span></li>
</ul>
<p>Adicionalmente, escribimos</p>
<p><span class="math inline">\(\theta_{i,k}^{(l)}=\)</span> es el peso de entrada <span class="math inline">\(a_{k}^{(l-1)}\)</span> de capa <span class="math inline">\(l-1\)</span>
en la entrada <span class="math inline">\(a_{i}^{(l)}\)</span> de la capa <span class="math inline">\(l\)</span>.</p>
<p>Los sesgos están dados por
<span class="math display">\[\theta_{i,0}^{(l)}\]</span></p>
<div id="ejemplo-33" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>En nuestro ejemplo, tenemos que en la capa <span class="math inline">\(l=3\)</span> hay dos unidades. Así que
podemos calcular los valores <span class="math inline">\(a^{(3)}_1\)</span> y <span class="math inline">\(a^{(3)}_2\)</span>. Están dados
por</p>
<p><span class="math display">\[a_1^{(3)} = h(\theta_{1,0}^{(2)} + \theta_{1,1}^{(2)} a_1^{(2)}+ \theta_{1,2}^{(2)}a_2^{(2)}+ \theta_{1,3}^{(2)} a_3^{(2)})\]</span>
<span class="math display">\[a_2^{(3)} = h(\theta_{2,0}^{(2)} + \theta_{2,1}^{(2)} a_1^{(2)}+ \theta_{2,2}^{(2)}a_2^{(2)}+ \theta_{2,3}^{(2)} a_3^{(2)})\]</span></p>
<p>Como se ilustra en la siguiente gráfica:</p>
<p><img src="07-redes-neuronales_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>Para visualizar las ordenadas (que también se llaman <strong>sesgos</strong> en este contexto),
ponemos <span class="math inline">\(a_{0}^{(2)}=1\)</span>.
<img src="07-redes-neuronales_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
</div>
<div id="ejemplo-34" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>Consideremos propagar a la capa 3 a partir de la capa 2. Usaremos los siguientes pesos para capa 3 y valores de la
capa 2 (en gris están los sesgos):
<img src="07-redes-neuronales_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>Que en nuestra notación escribimos como
<span class="math display">\[a^{(2)}_0 = 1, a^{(2)}_1 = -2, a^{(2)}_2 = 5, a^{(2)}=3\]</span>
y los pesos son, para la primera unidad:
<span class="math display">\[\theta^{(2)}_{1,0} = 3,  \,\,\, \theta^{(2)}_{1,1} = 1.5,\,\,\,\theta^{(2)}_{1,2} = -1,\,\,\theta^{(2)}_{1,3} = -0.5 \]</span>
y para la segunda unidad
<span class="math display">\[\theta^{(2)}_{2,0} = 1,  \,\,\, \theta^{(2)}_{2,1} = 2,\,\,\,\theta^{(2)}_{2,2} = 0.5,\,\, \theta^{(2)}_{2,3} = -0.2\]</span>
Y ahora queremos calcular los valores que toman las unidades de la capa 3,
que son <span class="math inline">\(a^{(3)}_1\)</span> y <span class="math inline">\(a^{(3)}_2\)</span>$</p>
<p>Para hacer feed forward a la siguiente capa, hacemos entonces</p>
<p><span class="math display">\[a^{(3)}_1 = h(3 + a^{(2)}_1 - a^{(2)}_2 -0.5 a_3^{(2)}),\]</span>
<span class="math display">\[a^{(3)}_2 = h(1 + 2a^{(2)}_1 + 0.5a^{(2)}_2 - 0.2 a_3^{(2)}),\]</span></p>
<p>Ponemos los pesos y valores de la capa 2 (incluyendo sesgo):</p>
<pre class="sourceCode r"><code class="sourceCode r">a_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">-2</span>, <span class="dv">5</span>, <span class="dv">3</span>) <span class="co"># ponemos un 1 al principio para el sesgo</span>
theta_<span class="dv">2</span>_<span class="dv">1</span> =<span class="st"> </span><span class="kw">c</span>(<span class="dv">3</span>, <span class="fl">1.5</span>, <span class="fl">-1.0</span>, <span class="fl">-0.5</span>)
theta_<span class="dv">2</span>_<span class="dv">2</span> =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="fl">0.5</span>, <span class="fl">-0.2</span>)</code></pre>
<p>y calculamos</p>
<pre class="sourceCode r"><code class="sourceCode r">a_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="kw">h</span>(<span class="kw">sum</span>(theta_<span class="dv">2</span>_<span class="dv">1</span><span class="op">*</span>a_<span class="dv">2</span>)),<span class="kw">h</span>(<span class="kw">sum</span>(theta_<span class="dv">2</span>_<span class="dv">2</span><span class="op">*</span>a_<span class="dv">2</span>))) <span class="co"># ponemos un 1 al principio</span>
a_<span class="dv">3</span></code></pre>
<pre><code>## [1] 1.000000000 0.001501182 0.249739894</code></pre>
<p><img src="07-redes-neuronales_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
</div>
</div>
<div id="feed-forward" class="section level2">
<h2><span class="header-section-number">7.4</span> Feed forward</h2>
<p>Para calcular los valores de salida de una red a partir de pesos y datos de entrada,
usamos el algoritmo feed-forward, calculando capa por capa.</p>

<div class="comentario">
<p>Cálculo en redes: <strong>Feed-forward</strong></p>
Para la primera capa,
escribimos las variables de entrada:
<span class="math display">\[a^{(1)}_j = x_j, j=1\ldots,n_1\]</span>
Para la primera capa oculta, o la segunda capa
<span class="math display">\[a^{(2)}_j = h\left( \theta_{j,0}^{(1)}+ \sum_{k=1}^{n_1}  \theta_{j,k}^{(1)}  a^{(1)}_k    \right), j=1\ldots,n_2\]</span>
para la <span class="math inline">\(l\)</span>-ésima capa:
<span class="math display">\[a^{(l)}_j = h\left( \theta_{j,0}^{(l-1)}+ \sum_{k=1}^{n_{l-1}}  \theta_{j,k}^{(l-1)}  a^{(l-1)}_k    \right), j=1\ldots,n_{l}\]</span>
y así sucesivamente.
Para la capa final o capa de salida (para problema binario), suponiendo
que tenemos <span class="math inline">\(L\)</span> capas (<span class="math inline">\(L-2\)</span> capas ocultas):
<span class="math display">\[p_1 = h\left(    \theta_{1,0}^{(L-1)}+ \sum_{k=1}^{n_{L-1}}  \theta_{1,k}^{(L-1)}  a^{(L-1)}_k     \right).\]</span>
</div>

<p>Nótese que entonces:</p>

<div class="comentario">
<p>Cada capa se caracteriza por el conjunto de parámetros <span class="math inline">\(\Theta^{(l)}\)</span>, que es una matriz
de <span class="math inline">\(n_l\times n_{l-1}\)</span>.</p>
<p>La red completa entonces se caracteriza por:</p>
<ul>
<li>La estructura elegida (número de capas ocultas y número de nodos en cada capa oculta).</li>
<li>Las matrices de pesos en cada capa <span class="math inline">\(\Theta^{(1)},\Theta^{(2)},\ldots, \Theta^{(L-1)}\)</span>
</div></li>
</ul>
<p>Adicionalmente, escribimos en forma vectorial:
<span class="math display">\[a^{(l)} = (a^{(l)}_0, a^{(l)}_1, a^{(l)}_2, \ldots, a^{(l)}_{n_l})^t\]</span></p>
<p>Para calcular la salidas, igual que hicimos, antes, propagaremos hacia
adelante los valores de las variables de entrada usando los <em>pesos</em>.
Agregando entradas adicionales en cada capa <span class="math inline">\(a_0^{(l)}\)</span>, <span class="math inline">\(l=1,2,\ldots, L-1\)</span>,
donde <span class="math inline">\(a_0^{l}=1\)</span>, y agregando a <span class="math inline">\(\Theta^{(l)}\)</span> una columna con
las ordenadas al origen (o sesgos) podemos escribir:</p>

<div class="comentario">
<p><strong>Feed-forward</strong>(matricial)</p>
<ul>
<li>Capa 1 (vector de entradas)
<span class="math display">\[ a^{(1)} = x\]</span></li>
<li>Capa 2
<span class="math display">\[ a^{(2)} = h(\Theta^{(1)}a^{(1)})\]</span></li>
<li>Capa <span class="math inline">\(l\)</span> (oculta)
<span class="math display">\[ a^{(l)} = h(\Theta^{(l-1)}a^{(l-1)})\]</span></li>
<li>Capa de salida:</li>
</ul>
<p>En un problema de clasificación binaria, la capa de salida se calcula como
en regresión logística:
<span class="math display">\[a^{(L)}= p = h(\Theta^{(L-1)}a^{(L-1)})\]</span>
donde <span class="math inline">\(h\)</span> se aplica componente a componente sobre los vectores correspondientes. Nótese
que feed-foward consiste principalmente de multiplicaciones de matrices con
algunas aplicaciones de <span class="math inline">\(h\)</span></p>
<p>Para un problema de regresión, la última capa se calcula como en regresión lineal:</p>
<span class="math display">\[a^{(L)} = p = \Theta^{(L-1)}a^{(L-1)}\]</span>
</div>

</div>
<div id="backpropagation-calculo-del-gradiente-clasificacion-binaria" class="section level2">
<h2><span class="header-section-number">7.5</span> Backpropagation: cálculo del gradiente (clasificación binaria)</h2>
<p>Más adelante, para ajustar los pesos y sesgos de las redes (valores <span class="math inline">\(\theta\)</span>),
utilizaremos descenso en gradiente y otros algoritmos derivados del gradiente
(descenso estocástico).
En esta parte entonces veremos cómo calcular estos gradientes con el algoritmo
de <em>back-propagation</em>, que es una aplicación de la regla de la cadena para derivar.
Back-propagation resulta en una fórmula recursiva donde propagamos errores de la red
como gradientes
desde el final de red (capa de salida) hasta el principio, capa por capa.</p>
<p><strong>Consideramos el problema de clasificación binaria</strong></p>
<p>Recordamos la devianza (con regularización ridge) es</p>
<p><span class="math display">\[D = -\frac{2}{n}\sum_{i=1}^n y_i\log(p_1(x_i)) +(1-y_i)\log(1-p_1(x_i)) + \lambda \sum_{l=2}^{L} \sum_{k=1}^{n_{l-1}} \sum_{j=1}^{n_l}(\theta_{j,k}^{(l)})^2.\]</span></p>
<p>Queremos entonces calcular las derivadas de la devianza con respecto a cada
parámetro <span class="math inline">\(\theta_{j,k}^{(l)}\)</span>. Esto nos proporciona el gradiente para
nuestro algoritmo de descenso.</p>
<p><strong>Consideramos aquí el problema de clasificación binaria con devianza como función
de pérdida, y sin regularización</strong>. La parte de la parcial que corresponde al término
de regularización es fácil de agregar al final.</p>
<p>Recordamos también nuestra notación para la función logística (o sigmoide):</p>
<p><span class="math display">\[h(z)=\frac{1}{1+e^{-z}}.\]</span>
Necesitaremos su derivada, que está dada por (cálculala):
<span class="math display">\[h&#39;(z) = h(z)(1-h(z))\]</span></p>
<div id="calculo-para-un-caso-de-entrenamiento" class="section level3">
<h3><span class="header-section-number">7.5.1</span> Cálculo para un caso de entrenamiento</h3>
<p>Como hicimos en regresión logística, primero simplificamos el problema
y consideramos calcular
las parciales <em>para un solo caso de entrenamiento</em> <span class="math inline">\((x,y)\)</span>:
<span class="math display">\[ D=  -\left ( y\log (p_1(x)) + (1-y)\log (1-p_1(x))\right) . \]</span></p>
<p>Después sumaremos sobre toda la muestra de entrenamiento. Entonces queremos
calcular
<span class="math display">\[\frac{\partial D}{\partial \theta_{j,k}^{(l)}}\]</span></p>
<p>Y escribiremos, con la notación de arriba,
<span class="math display">\[a^{(l+1)}_j = h(z^{(l+1)}_j)\]</span>
donde
<span class="math display">\[z^{(l+1)} = \Theta^{(l)} a^{(l)},\]</span>
que coordenada a coordenada se escribe como
<span class="math display">\[z^{(l+1)}_j =  \sum_{k=0}^{n_{l}}  \theta_{j,k}^{(l)}  a^{(l)}_k\]</span></p>
<div id="paso-1-derivar-respecto-a-capa-l1" class="section level4 unnumbered">
<h4>Paso 1: Derivar respecto a capa <span class="math inline">\(l+1\)</span></h4>
<p>Como los valores de cada capa determinan los valores de salida y la devianza,
podemos escribir (recordemos que <span class="math inline">\(a_0^{(l)}=1\)</span> es constante):
<span class="math display">\[D=D(a_0^{(l+1)},a_1^{(l+1)},a_2^{(l+1)},\ldots, a_{n_{l+1}}^{(l+1)})=D(a_1^{(l+1)},a_2^{(l+1)},\ldots, a_{n_{l+1}}^{(l+1)})\]</span></p>
<p>Así que por la regla de la cadena para varias variables:
<span class="math display">\[\frac{\partial D}{\partial \theta_{j,k}^{(l)}} =
\sum_{t=1}^{n_{l}} \frac{\partial D}{\partial a_t^{(l+1)}}\frac{\partial a_t^{(l+1)}}
{\partial \theta_{j,k}^{(l)} }\]</span></p>
<p>Pero si vemos dónde aparece <span class="math inline">\(\theta_{j,k}^{(l)}\)</span> en la gráfica de la red:</p>
<p><span class="math display">\[ \cdots a^{(l)}_k \xrightarrow{\theta_{j,k}^{(l)}} a^{(l+1)}_j  \cdots \rightarrow  D\]</span>
Entonces podemos concluir que
<span class="math inline">\(\frac{\partial a_t^{(l+1)}}{\partial \theta_{j,k}^{(l)}} =0\)</span> cuando <span class="math inline">\(t\neq j\)</span> (pues no
dependen de <span class="math inline">\(\theta_{j,k}^{(l)}\)</span>),</p>
<p>y entonces, para toda <span class="math inline">\(j=1,2,\ldots, n_{l+1}, k=0,1,\ldots, n_{l}\)</span>
<span class="math display" id="eq:parcial">\[\begin{equation}
\frac{\partial D}{\partial \theta_{j,k}^{(l)}} =
\frac{\partial D}{\partial a_j^{(l+1)}}\frac{\partial a_j^{(l+1)}}{\partial \theta_{j,k}^{(l)} }
.
  \tag{7.1}
\end{equation}\]</span></p>
<p>Adicionalmente, como
<span class="math display">\[a_j^{(l+1)} = h(z_j^{(l+1)}) = h\left (\sum_{k=0}^{n_{l}}  \theta_{j,k}^{(l)}  a^{(l)}_k \right )\]</span>
y las <span class="math inline">\(a_k^{(l)}\)</span> no dependen de <span class="math inline">\(\theta_{j,k}^{(l)}\)</span>, tenemos por la regla de la cadena que
<span class="math display">\[\begin{equation}
\frac{\partial a_j^{(l+1)}}{\partial \theta_{j,k}^{(l)} } = h&#39;(z_j^{(l+1)})a_k^{(l)}.
\end{equation}\]</span></p>
<p>Esta última expresión podemos calcularla pues sólo requiere la derivada de <span class="math inline">\(h\)</span> y
los valores otenidos en el paso de feed-forward.</p>
</div>
<div id="paso-2-obtener-formula-recursiva" class="section level4 unnumbered">
<h4>Paso 2: Obtener fórmula recursiva</h4>
<p>Así que sólo nos queda calcular las parciales (<span class="math inline">\(j = 1,\ldots, n_l\)</span>)
<span class="math display">\[\frac{\partial D}{\partial a_j^{(l)}}\]</span></p>
<p>Para obtener una fórmula recursiva para esta cantidad (hacia atrás),
aplicamos otra vez regla de la cadena, pero con respecto a la capa <span class="math inline">\(l\)</span> (ojo: queremos obtener
una fórmula recursiva!):</p>
<p><span class="math display">\[\frac{\partial D}{\partial a_j^{(l)}}= \sum_{s=1}^{n_{l+1}}
\frac{\partial D}{\partial a_s^{(l+1)}}\frac{\partial  a_s^{(l+1)}}{\partial a_j^{(l)}},\]</span></p>
<p>que se puede entender a partir de este diagrama:
<img src="07-redes-neuronales_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<p>Nótese que la suma empieza en <span class="math inline">\(s=1\)</span>, no en <span class="math inline">\(s=0\)</span>, pues <span class="math inline">\(a_0^{(l+1)}\)</span> no depende
de <span class="math inline">\(a_k^{(l)}\)</span>.</p>
<p>En este caso los elementos de la suma no se anulan necesariamente. Primero
consideramos la derivada de:</p>
<p><span class="math display">\[\frac{\partial  a_s^{(l+1)}}{\partial a_j^{(l)}}=h&#39;(z_s^{(l+1)})\theta_{s,j}^{(l)},\]</span></p>
<p>de modo que</p>
<p><span class="math display">\[\frac{\partial D}{\partial a_j^{(l)}}= \sum_{s=1}^{n_l}
\frac{\partial D}{\partial a_s^{(l+1)}} h&#39;(z_s^{(l+1)})\theta_{s,j}^{(l)}.\]</span></p>
<p>Nótese que esto nos da una fórmula recursiva para las parciales que nos
falta calcular (de <span class="math inline">\(D\)</span> con respecto a <span class="math inline">\(a\)</span>), pues las otras cantidades las
conocemos por backpropagation.</p>
</div>
<div id="paso-3-simplificacion-de-la-recursion" class="section level4 unnumbered">
<h4>Paso 3: Simplificación de la recursión</h4>
<p><span class="math display" id="eq:delta-def-a">\[\begin{equation}
\delta_s^{ (l+1)}=\frac{\partial D}{\partial a_s^{(l+1)}} h&#39;(z_s^{(l+1)})
  \tag{7.2}
\end{equation}\]</span></p>
<p>de manera que la ecuación recursiva es</p>
<p><span class="math display" id="eq:delta-def">\[\begin{equation}
\frac{\partial D}{\partial a_j^{(l)}} = \sum_{s=1}^{n_{l+1}}
\delta_s^{(l+1)}\theta_{s,j}^{(l)}.
  \tag{7.3}
\end{equation}\]</span></p>
<p>Tenemos que si <span class="math inline">\(l=2,\ldots,L-1\)</span>, entonces podemos escribir (usando <a href="redes-neuronales-parte-1.html#eq:delta-def">(7.3)</a>)
como fórmula recursiva:</p>
<p><span class="math display" id="eq:delta-recursion">\[\begin{equation}
\delta_j^{(l)} 
= \left (\sum_{s=1}^{n_l} \delta_s^{(l+1)} \theta_{s,j}^{(l)}\right ) h&#39;(z_j^{(l)}),
  \tag{7.4}
\end{equation}\]</span>
para <span class="math inline">\(j=1,2,\ldots, n_{l}\)</span>.</p>
</div>
<div id="paso-4-condiciones-inciales" class="section level4 unnumbered">
<h4>Paso 4: Condiciones inciales</h4>
<p>Para la última capa, tenemos que (demostrar!)</p>
<p><span class="math display">\[\delta_1^{(L)}=p - y.\]</span></p>
</div>
<div id="paso-5-calculo-de-parciales" class="section level4 unnumbered">
<h4>Paso 5: Cálculo de parciales</h4>
<p>Finalmente, usando <a href="redes-neuronales-parte-1.html#eq:parcial">(7.1)</a> y <a href="redes-neuronales-parte-1.html#eq:delta-def-a">(7.2)</a> , obtenemos
<span class="math display">\[\frac{\partial D}{\partial \theta_{j,k}^{(l)}} = \delta_j^{(l+1)}a_k^{(l)},\]</span></p>
<p>y con esto ya podemos hacer backpropagation para calcular el gradiente
sobre cada caso de entrenamiento, y solo resta acumular para obtener el gradiente
sobre la muestra de entrenamiento.</p>
<p>Muchas veces es útil escribir una versión vectorizada (importante para implementar):</p>
</div>
<div id="paso-6-version-matricial" class="section level4 unnumbered">
<h4>Paso 6: Versión matricial</h4>
<p>Ahora podemos escribir estas ecuaciones en forma vectorial. En primer lugar,
<span class="math display">\[\delta^{(L)}=p-y.\]</span>
Y además se puede ver de la ecuación <a href="redes-neuronales-parte-1.html#eq:delta-recursion">(7.4)</a> que
(<span class="math inline">\(\Theta_{*}^{(l+1)}\)</span> denota la matriz de pesos <em>sin</em> la columna correspondiente al sesgo):</p>
<p><span class="math display" id="eq:delta-recursion-mat">\[\begin{equation}
\delta^{(l)}=\left( \Theta_{*}^{(l)}    \right)^t\delta^{(l+1)} \circ h&#39;(z^{(l)})
\tag{7.5}
\end{equation}\]</span></p>
<p>donde <span class="math inline">\(\circ\)</span> denota el producto componente a componente.</p>
<p>Ahora todo ya está calculado. Lo interesante es que las <span class="math inline">\(\delta^{(l)}\)</span> se calculan
de manera recursiva.</p>
</div>
</div>
<div id="algoritmo-de-backpropagation" class="section level3">
<h3><span class="header-section-number">7.5.2</span> Algoritmo de backpropagation</h3>

<div class="comentario">
<p><strong>Backpropagation</strong> Para problema de clasificación con regularización $ 0 $.
Para <span class="math inline">\(i=1,\ldots, N,\)</span> tomamos el dato de entrenamiento <span class="math inline">\((x^{(i)}, y^{(i)})\)</span> y hacemos:</p>
<ol style="list-style-type: decimal">
<li>Ponemos <span class="math inline">\(a^{(1)}=x^{(i)}\)</span> (vector de entradas, incluyendo 1).</li>
<li>Calculamos <span class="math inline">\(a^{(2)},a^{(3)},\ldots, a^{(L)}\)</span> usando feed forward para la entrada <span class="math inline">\(x^{(i)}.\)</span></li>
<li>Calculamos <span class="math inline">\(\delta^{(L)}=a^{(L)}-y^{(i)}\)</span>, y luego
<span class="math inline">\(\delta^{(L-1)},\ldots, \delta^{(2)}\)</span> según la recursión <a href="redes-neuronales-parte-1.html#eq:delta-recursion">(7.4)</a>.</li>
<li>Acumulamos
<span class="math inline">\(\Delta_{j,k}^{(l)}=\Delta_{j,k}^{(l)} + \delta_j^{(l+1)}a_k^{(l)}\)</span>.</li>
<li>Finalmente, ponemos, si <span class="math inline">\(k\neq 0\)</span>,
<span class="math display">\[D_{j,k}^{(l)} = \frac{2}{N}\Delta_{j,k}^{(l)} + 2\lambda\theta_{j,k}^{(l)}\]</span>
y si <span class="math inline">\(k=0\)</span>,
<span class="math display">\[D_{j,k}^{(l)} = \frac{2}{N}\Delta_{j,k}^{(l)} .\]</span>
Entonces:
<span class="math display">\[D_{j,k}^{(l)} =\frac{\partial D}{\partial \theta_{j,k}^{(l)}}.\]</span></li>
</ol>
Nótese
que back-propagation consiste principalmente de mutliplicaciones de matrices con
algunas aplicaciones de <span class="math inline">\(h\)</span> y acumulaciones, igual que feed-forward.
</div>

</div>
</div>
<div id="ajuste-de-parametros-introduccion" class="section level2">
<h2><span class="header-section-number">7.6</span> Ajuste de parámetros (introducción)</h2>
<p>Consideramos la versión con regularización ridge (también llamada L2)
de la devianza de entrenamiento como nuestro función objetivo:</p>

<div class="comentario">
<strong>Ajuste de redes neuronales</strong>
Para un problema de clasificación binaria con
<span class="math inline">\(y_i=0\)</span> o <span class="math inline">\(y_i=1\)</span>, ajustamos los pesos <span class="math inline">\(\Theta^{(1)},\Theta^{(2)},\ldots, \Theta^{(L)}\)</span>
de la red minimizando la devianza (penalizada) sobre la muestra de entrenamiento:
<span class="math display">\[D = -\frac{2}{n}\sum_{i=1}^n y_i\log(p_1(x_i)) +(1-y_i)\log(1-p_1(x_i)) + \lambda \sum_{l=2}^{L} \sum_{k=1}^{n_{l-1}} \sum_{j=1}^{n_l}(\theta_{j,k}^{(l)})^2.\]</span>
Este problema en general no es convexo y <em>puede tener múltiples mínimos</em>.
</div>

<p>Veremos el proceso de ajuste, selección de arquitectura, etc. más adelante.
Por el momento hacemos unas observaciones acerca de este problema de minimización:</p>
<ul>
<li><p>Hay varios algoritmos para minimizar esta devianza,
algunos avanzados incluyendo información de segundo orden (como Newton), pero
actualmente las técnicas más populares, para redes grandes, están
derivadas de descenso en gradiente. Más
específicamente, una variación, que es <em>descenso estocástico</em>.</p></li>
<li><p>Que el algoritmo depende principalmente de multiplicaciones de matrices y
acumulaciones implica que puede escalarse de diversas maneras. Una es paralelizando
sobre la muestra de entrenamiento (y acumular acumulados al final), pero también
se puede paralelizar la de multiplicaciones de matrices (para lo cual los GPUs
se prestan muy bien).</p></li>
<li><p>Para redes neuronales, el gradiente se calcula con un algoritmo que se llama
<em>back-propagation</em>, que es una aplicación de la regla de la cadena para propagar
errores desde la capa de salida a lo largo de todas las capas para ajustar los pesos y sesgos.</p></li>
<li><p>En estos problemas no buscamos el mínimo global, sino un mínimo
local de buen desempeño. Puede haber múltiples mínimos, puntos silla, regiones
relativamente planas, precipicios (curvatura alta). Todo esto dificulta el
entrenamiento de redes neuronales grandes. Para redes grandes, ni siquiera esperamos a alcanzar
un mínimo local, sino que nos detenemos prematuramente cuando obtenemos
el mejor desempeño posible.</p></li>
<li><p>Nótese que la simetría implica que podemos obtener la misma red cambiando
pesos entre neuronas y las conexiones correspondientes. Esto implica que necesariamente
hay varios mínimos.</p></li>
<li><p>Para este problema, no tiene sentido comenzar las iteraciones con todos los pesos
igual a cero, pues las unidades de la red son simétricas: no hay nada que
distinga una de otra si todos los pesos son iguales. Esto quiere decir que si iteramos,
¡todas las neuronas van a aprender lo mismo!</p></li>
<li><p>Es importante
no comenzar valores de los pesos grandes, pues las funciones logísticas pueden
quedar en regiones planas donde la minimización es lenta, o podemos
tener gradientes demasiado grandes y produzcan inestabilidad en el cálculo
del gradiente.</p></li>
<li><p>Generalmente los pesos se inicializan al azar con variables independientes
gaussianas o uniformes centradas en cero, y con varianza chica
(por ejemplo <span class="math inline">\(U(-0.5,0.5)\)</span>). Una recomendación es usar <span class="math inline">\(U(-1/\sqrt{m}, 1/\sqrt{m})\)</span>
donde <span class="math inline">\(m\)</span> es el número de entradas. En general, hay que experimentar con este
parámetro.</p></li>
</ul>
<p>El proceso para ajustar una red es entonces:</p>
<ul>
<li>Definir número de capas ocultas, número de neuronas por cada capa, y un valor del parámetro de regularización. Estandarizar las entradas.</li>
<li>Seleccionar parámetros al azar para <span class="math inline">\(\Theta^{(2)},\Theta^{(3)},\ldots, \Theta^{(L)}\)</span>.
Se toman, por ejemplo, normales con media 0 y varianza chica.</li>
<li>Correr un algoritmo de minimización de la devianza mostrada arriba.</li>
<li>Verificar convergencia del algoritmo a un mínimo local (o el algoritmo no está mejorando).</li>
<li>Predecir usando el modelo ajustado.</li>
</ul>
<p>Finalmente, podemos probar distintas arquitecturas y valores del parámetros de regularización,
para afinar estos parámetros según validación cruzada o una muestra de validación.</p>
<div id="ejemplo-35" class="section level3">
<h3><span class="header-section-number">7.6.1</span> Ejemplo</h3>
<p>Consideramos una arquitectura de dos capas para el problema de diabetes</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(keras)</code></pre>
<pre><code>## 
## Attaching package: &#39;keras&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:igraph&#39;:
## 
##     %&lt;-%, normalize</code></pre>
<p>Escalamos y preparamos los datos:</p>
<pre class="sourceCode r"><code class="sourceCode r">diabetes_ent &lt;-<span class="st"> </span>MASS<span class="op">::</span>Pima.tr
diabetes_pr &lt;-<span class="st"> </span>MASS<span class="op">::</span>Pima.te
x_ent &lt;-<span class="st"> </span>diabetes_ent <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>type) <span class="op">%&gt;%</span><span class="st"> </span>as.matrix
x_ent_s &lt;-<span class="st"> </span><span class="kw">scale</span>(x_ent)
x_valid &lt;-<span class="st"> </span>diabetes_pr <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>type) <span class="op">%&gt;%</span><span class="st"> </span>as.matrix 
x_valid_s &lt;-<span class="st"> </span>x_valid <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">scale</span>(<span class="dt">center =</span> <span class="kw">attr</span>(x_ent_s, <span class="st">&#39;scaled:center&#39;</span>), 
        <span class="dt">scale =</span> <span class="kw">attr</span>(x_ent_s,  <span class="st">&#39;scaled:scale&#39;</span>))
y_ent &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(diabetes_ent<span class="op">$</span>type <span class="op">==</span><span class="st"> &#39;Yes&#39;</span>)
y_valid &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(diabetes_pr<span class="op">$</span>type <span class="op">==</span><span class="st"> &#39;Yes&#39;</span>)</code></pre>
<p>Para definir la arquitectura de dos capas con:</p>
<ul>
<li>10 unidades en cada capa</li>
<li>función de activación sigmoide,</li>
<li>regularización L2 (ridge),</li>
<li>salida logística (<span class="math inline">\(p_1\)</span>), escribimos:</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">923</span>)
modelo_tc &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>() 
<span class="co"># no es necesario asignar a nuevo objeto, modelo_tc es modificado al agregar capas</span>
modelo_tc <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">10</span>, <span class="dt">activation =</span> <span class="st">&#39;sigmoid&#39;</span>, 
              <span class="dt">kernel_regularizer =</span> <span class="kw">regularizer_l2</span>(<span class="dt">l =</span> <span class="fl">1e-3</span>), 
              <span class="dt">kernel_initializer =</span> <span class="kw">initializer_random_uniform</span>(<span class="dt">minval =</span> <span class="fl">-0.5</span>, <span class="dt">maxval =</span> <span class="fl">0.5</span>),
              <span class="dt">input_shape=</span><span class="dv">7</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">10</span>, <span class="dt">activation =</span> <span class="st">&#39;sigmoid&#39;</span>, 
              <span class="dt">kernel_regularizer =</span> <span class="kw">regularizer_l2</span>(<span class="dt">l =</span> <span class="fl">1e-3</span>), 
              <span class="dt">kernel_initializer =</span> <span class="kw">initializer_random_uniform</span>(<span class="dt">minval =</span> <span class="fl">-0.5</span>, <span class="dt">maxval =</span> <span class="fl">0.5</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">1</span>, <span class="dt">activation =</span> <span class="st">&#39;sigmoid&#39;</span>,
              <span class="dt">kernel_regularizer =</span> <span class="kw">regularizer_l2</span>(<span class="dt">l =</span> <span class="fl">1e-3</span>),
              <span class="dt">kernel_initializer =</span> <span class="kw">initializer_random_uniform</span>(<span class="dt">minval =</span> <span class="fl">-0.5</span>, <span class="dt">maxval =</span> <span class="fl">0.5</span>)
)</code></pre>
<p>Ahora difinimos la función de pérdida (devianza es equivalente a entropía
cruzada binaria), y pedimos registrar porcentaje de correctos (accuracy) y compilamos
en tensorflow:</p>
<pre class="sourceCode r"><code class="sourceCode r">modelo_tc <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(
  <span class="dt">loss =</span> <span class="st">&#39;binary_crossentropy&#39;</span>,
  <span class="dt">optimizer =</span> <span class="kw">optimizer_sgd</span>(<span class="dt">lr =</span> <span class="fl">0.8</span>),
  <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">&#39;accuracy&#39;</span>,<span class="st">&#39;binary_crossentropy&#39;</span>))</code></pre>
<p>Iteramos con descenso en gradiente y monitoreamos el error de validación. Hacemos
100 iteraciones de descenso en gradiente (épocas=100)</p>
<pre class="sourceCode r"><code class="sourceCode r">iteraciones &lt;-<span class="st"> </span>modelo_tc <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(
  x_ent_s, y_ent, 
  <span class="co">#batch size mismo que nrow(x_ent_s) es descenso en grad.</span>
  <span class="dt">epochs =</span> <span class="dv">1000</span>, <span class="dt">batch_size =</span> <span class="kw">nrow</span>(x_ent_s), 
  <span class="dt">verbose =</span> <span class="dv">1</span>,
  <span class="dt">validation_data =</span> <span class="kw">list</span>(x_valid_s, y_valid)
)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">score &lt;-<span class="st"> </span>modelo_tc <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">evaluate</span>(x_valid_s, y_valid)
score</code></pre>
<pre><code>## $loss
## [1] 0.4757808
## 
## $acc
## [1] 0.7771084
## 
## $binary_crossentropy
## [1] 0.4355485</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">tab_confusion &lt;-<span class="st"> </span><span class="kw">table</span>(modelo_tc <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict_classes</span>(x_valid_s),y_valid) 
tab_confusion</code></pre>
<pre><code>##    y_valid
##       0   1
##   0 194  45
##   1  29  64</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prop.table</span>(tab_confusion, <span class="dv">2</span>)</code></pre>
<pre><code>##    y_valid
##             0         1
##   0 0.8699552 0.4128440
##   1 0.1300448 0.5871560</code></pre>
<p>Es importante monitorear las curvas de aprendizaje (entrenamiento y
validación) para diagnosticar mejoras:</p>
<pre class="sourceCode r"><code class="sourceCode r">df_iteraciones &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(iteraciones)
<span class="kw">ggplot</span>(df_iteraciones, <span class="kw">aes</span>(<span class="dt">x=</span>epoch, <span class="dt">y=</span>value, <span class="dt">colour=</span>data, <span class="dt">group=</span>data)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span><span class="kw">facet_wrap</span>(<span class="op">~</span>metric, <span class="dt">ncol=</span><span class="dv">1</span>, <span class="dt">scales =</span> <span class="st">&#39;free&#39;</span>)</code></pre>
<p><img src="07-redes-neuronales_files/figure-html/unnamed-chunk-48-1.png" width="768" /></p>
<p><strong>Observación</strong>: puedes utilizar <em>Tensorboard</em>, una herramienta
para visualizar resultados del entrenamiento de modelos incluída
en <em>Tensorflow</em> (que es lo que usa <em>keras</em> para hacer los cálculos):</p>
<pre class="sourceCode r"><code class="sourceCode r">iteraciones &lt;-<span class="st"> </span>modelo_tc <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(
  x_ent_s, y_ent, 
  <span class="co">#batch size mismo que nrow(x_ent_s) es descenso en grad.</span>
  <span class="dt">epochs =</span> <span class="dv">500</span>, <span class="dt">batch_size =</span> <span class="kw">nrow</span>(x_ent_s), 
  <span class="dt">verbose =</span> <span class="dv">0</span>,
  <span class="dt">callbacks =</span> <span class="kw">callback_tensorboard</span>(<span class="st">&quot;logs/diabetes/run_1&quot;</span>),
  <span class="dt">validation_data =</span> <span class="kw">list</span>(x_valid_s, y_valid)
)</code></pre>
<p>y después puedes hacer:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tensorboard</span>(<span class="st">&quot;logs/diabetes/&quot;</span>)</code></pre>
<div id="ejercicio-6" class="section level4 unnumbered">
<h4>Ejercicio</h4>
<p>Corre el ejemplo anterior con distintos parámetros de tasa de aprendizaje,
número de unidades en las capas de intermedia y regularización (cambia
arriba verbose=1 para monitorear al correr).</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="extensiones-para-regresion-lineal-y-logistica.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="redes-neuronales-parte-2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/felipegonzalez/aprendizaje-maquina-mcd/edit/master/07-redes-neuronales.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
