<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Aprendizaje de máquina</title>
  <meta name="description" content="Notas y material para el curso de aprendizaje de máquina (ITAM)">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Aprendizaje de máquina" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notas y material para el curso de aprendizaje de máquina (ITAM)" />
  <meta name="github-repo" content="felipegonzalez/aprendizaje-maquina-mcd" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Aprendizaje de máquina" />
  
  <meta name="twitter:description" content="Notas y material para el curso de aprendizaje de máquina (ITAM)" />
  

<meta name="author" content="Felipe González">


<meta name="date" content="2018-11-12">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="metodos-basados-en-arboles.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
<link rel="stylesheet" href="css/font-awesome.min.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Aprendizaje Máquina</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Temario y referencias</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#evaluacion"><i class="fa fa-check"></i>Evaluación</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-r-y-rstudio"><i class="fa fa-check"></i>Software: R y Rstudio</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#referencias-principales"><i class="fa fa-check"></i>Referencias principales</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#otras-referencias"><i class="fa fa-check"></i>Otras referencias</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduccion.html"><a href="introduccion.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="introduccion.html"><a href="introduccion.html#que-es-aprendizaje-de-maquina-machine-learning"><i class="fa fa-check"></i><b>1.1</b> ¿Qué es aprendizaje de máquina (machine learning)?</a></li>
<li class="chapter" data-level="1.2" data-path="introduccion.html"><a href="introduccion.html#aprendizaje-supervisado-1"><i class="fa fa-check"></i><b>1.2</b> Aprendizaje Supervisado</a><ul>
<li class="chapter" data-level="" data-path="introduccion.html"><a href="introduccion.html#proceso-generador-de-datos-modelo-teorico"><i class="fa fa-check"></i>Proceso generador de datos (modelo teórico)</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduccion.html"><a href="introduccion.html#predicciones"><i class="fa fa-check"></i><b>1.3</b> Predicciones</a></li>
<li class="chapter" data-level="1.4" data-path="introduccion.html"><a href="introduccion.html#aprendizaje"><i class="fa fa-check"></i><b>1.4</b> Tarea de aprendizaje supervisado</a></li>
<li class="chapter" data-level="1.5" data-path="introduccion.html"><a href="introduccion.html#error"><i class="fa fa-check"></i><b>1.5</b> Balance de complejidad y rigidez</a></li>
<li class="chapter" data-level="1.6" data-path="introduccion.html"><a href="introduccion.html#como-estimar-f"><i class="fa fa-check"></i><b>1.6</b> ¿Cómo estimar f?</a></li>
<li class="chapter" data-level="1.7" data-path="introduccion.html"><a href="introduccion.html#resumen"><i class="fa fa-check"></i><b>1.7</b> Resumen</a></li>
<li class="chapter" data-level="1.8" data-path="introduccion.html"><a href="introduccion.html#tarea"><i class="fa fa-check"></i><b>1.8</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regresion.html"><a href="regresion.html"><i class="fa fa-check"></i><b>2</b> Regresión lineal</a><ul>
<li class="chapter" data-level="2.1" data-path="introduccion.html"><a href="introduccion.html#introduccion"><i class="fa fa-check"></i><b>2.1</b> Introducción</a></li>
<li class="chapter" data-level="2.2" data-path="regresion.html"><a href="regresion.html#aprendizaje-de-coeficientes-ajuste"><i class="fa fa-check"></i><b>2.2</b> Aprendizaje de coeficientes (ajuste)</a></li>
<li class="chapter" data-level="2.3" data-path="regresion.html"><a href="regresion.html#descenso-en-gradiente"><i class="fa fa-check"></i><b>2.3</b> Descenso en gradiente</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regresion.html"><a href="regresion.html#seleccion-de-tamano-de-paso-eta"><i class="fa fa-check"></i><b>2.3.1</b> Selección de tamaño de paso <span class="math inline">\(\eta\)</span></a></li>
<li class="chapter" data-level="2.3.2" data-path="regresion.html"><a href="regresion.html#funciones-de-varias-variables"><i class="fa fa-check"></i><b>2.3.2</b> Funciones de varias variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regresion.html"><a href="regresion.html#descenso-en-gradiente-para-regresion-lineal"><i class="fa fa-check"></i><b>2.4</b> Descenso en gradiente para regresión lineal</a></li>
<li class="chapter" data-level="2.5" data-path="regresion.html"><a href="regresion.html#normalizacion-de-entradas"><i class="fa fa-check"></i><b>2.5</b> Normalización de entradas</a></li>
<li class="chapter" data-level="2.6" data-path="regresion.html"><a href="regresion.html#interpretacion-de-modelos-lineales"><i class="fa fa-check"></i><b>2.6</b> Interpretación de modelos lineales</a></li>
<li class="chapter" data-level="2.7" data-path="regresion.html"><a href="regresion.html#solucion-analitica"><i class="fa fa-check"></i><b>2.7</b> Solución analítica</a></li>
<li class="chapter" data-level="2.8" data-path="regresion.html"><a href="regresion.html#por-que-el-modelo-lineal-funciona-bien-muchas-veces"><i class="fa fa-check"></i><b>2.8</b> ¿Por qué el modelo lineal funciona bien (muchas veces)?</a><ul>
<li class="chapter" data-level="2.8.1" data-path="regresion.html"><a href="regresion.html#k-vecinos-mas-cercanos"><i class="fa fa-check"></i><b>2.8.1</b> k vecinos más cercanos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regresion.html"><a href="regresion.html#tarea-1"><i class="fa fa-check"></i>Tarea</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="logistica.html"><a href="logistica.html"><i class="fa fa-check"></i><b>3</b> Regresión logística</a><ul>
<li class="chapter" data-level="3.1" data-path="logistica.html"><a href="logistica.html#el-problema-de-clasificacion"><i class="fa fa-check"></i><b>3.1</b> El problema de clasificación</a><ul>
<li class="chapter" data-level="" data-path="logistica.html"><a href="logistica.html#que-estimar-en-problemas-de-clasificacion"><i class="fa fa-check"></i>¿Qué estimar en problemas de clasificación?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="logistica.html"><a href="logistica.html#estimacion-de-probabilidades-de-clase"><i class="fa fa-check"></i><b>3.2</b> Estimación de probabilidades de clase</a><ul>
<li class="chapter" data-level="" data-path="logistica.html"><a href="logistica.html#ejemplo-9"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="3.2.1" data-path="logistica.html"><a href="logistica.html#k-vecinos-mas-cercanos-1"><i class="fa fa-check"></i><b>3.2.1</b> k-vecinos más cercanos</a></li>
<li class="chapter" data-level="" data-path="logistica.html"><a href="logistica.html#ejemplo-11"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="logistica.html"><a href="logistica.html#error-para-modelos-de-clasificacion"><i class="fa fa-check"></i><b>3.3</b> Error para modelos de clasificación</a><ul>
<li class="chapter" data-level="3.3.1" data-path="logistica.html"><a href="logistica.html#ejercicio-1"><i class="fa fa-check"></i><b>3.3.1</b> Ejercicio</a></li>
<li class="chapter" data-level="3.3.2" data-path="logistica.html"><a href="logistica.html#error-de-clasificacion-y-funcion-de-perdida-0-1"><i class="fa fa-check"></i><b>3.3.2</b> Error de clasificación y función de pérdida 0-1</a></li>
<li class="chapter" data-level="3.3.3" data-path="logistica.html"><a href="logistica.html#discusion-relacion-entre-devianza-y-error-de-clasificacion"><i class="fa fa-check"></i><b>3.3.3</b> Discusión: relación entre devianza y error de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="logistica.html"><a href="logistica.html#regresion-logistica"><i class="fa fa-check"></i><b>3.4</b> Regresión logística</a><ul>
<li class="chapter" data-level="3.4.1" data-path="logistica.html"><a href="logistica.html#regresion-logistica-simple"><i class="fa fa-check"></i><b>3.4.1</b> Regresión logística simple</a></li>
<li class="chapter" data-level="3.4.2" data-path="logistica.html"><a href="logistica.html#funcion-logistica"><i class="fa fa-check"></i><b>3.4.2</b> Función logística</a></li>
<li class="chapter" data-level="3.4.3" data-path="logistica.html"><a href="logistica.html#regresion-logistica-1"><i class="fa fa-check"></i><b>3.4.3</b> Regresión logística</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="logistica.html"><a href="logistica.html#aprendizaje-de-coeficientes-para-regresion-logistica-binomial."><i class="fa fa-check"></i><b>3.5</b> Aprendizaje de coeficientes para regresión logística (binomial).</a></li>
<li class="chapter" data-level="3.6" data-path="logistica.html"><a href="logistica.html#ejercicio-datos-de-diabetes"><i class="fa fa-check"></i><b>3.6</b> Ejercicio: datos de diabetes</a><ul>
<li class="chapter" data-level="" data-path="logistica.html"><a href="logistica.html#tarea-2"><i class="fa fa-check"></i>Tarea</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html"><i class="fa fa-check"></i><b>4</b> Más sobre problemas de clasificación</a><ul>
<li class="chapter" data-level="4.1" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#analisis-de-error-para-clasificadores-binarios"><i class="fa fa-check"></i><b>4.1</b> Análisis de error para clasificadores binarios</a><ul>
<li class="chapter" data-level="" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#medidas-resumen-de-desempeno"><i class="fa fa-check"></i>Medidas resumen de desempeño</a></li>
<li class="chapter" data-level="" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#interpetacion-de-resumenes-de-desempeno-y-tasas-base"><i class="fa fa-check"></i>Interpetación de resúmenes de desempeño y tasas base</a></li>
<li class="chapter" data-level="4.1.1" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#puntos-de-corte-para-un-clasificador-binario"><i class="fa fa-check"></i><b>4.1.1</b> Puntos de corte para un clasificador binario</a></li>
<li class="chapter" data-level="4.1.2" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#espacio-roc-de-clasificadores"><i class="fa fa-check"></i><b>4.1.2</b> Espacio ROC de clasificadores</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#perfil-de-un-clasificador-binario-y-curvas-roc"><i class="fa fa-check"></i><b>4.2</b> Perfil de un clasificador binario y curvas ROC</a></li>
<li class="chapter" data-level="4.3" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#regresion-logistica-para-problemas-de-mas-de-2-clases"><i class="fa fa-check"></i><b>4.3</b> Regresión logística para problemas de más de 2 clases</a><ul>
<li class="chapter" data-level="4.3.1" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#regresion-logistica-multinomial"><i class="fa fa-check"></i><b>4.3.1</b> Regresión logística multinomial</a></li>
<li class="chapter" data-level="4.3.2" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#interpretacion-de-coeficientes"><i class="fa fa-check"></i><b>4.3.2</b> Interpretación de coeficientes</a></li>
<li class="chapter" data-level="4.3.3" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#ejemplo-clasificacion-de-digitos-con-regresion-multinomial"><i class="fa fa-check"></i><b>4.3.3</b> Ejemplo: Clasificación de dígitos con regresión multinomial</a></li>
<li class="chapter" data-level="" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#discusion"><i class="fa fa-check"></i>Discusión</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#descenso-en-gradiente-para-regresion-multinomial-logistica"><i class="fa fa-check"></i><b>4.4</b> Descenso en gradiente para regresión multinomial logística</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regularizacion.html"><a href="regularizacion.html"><i class="fa fa-check"></i><b>5</b> Regularización</a><ul>
<li class="chapter" data-level="5.1" data-path="regularizacion.html"><a href="regularizacion.html#sesgo-y-varianza-de-predictores"><i class="fa fa-check"></i><b>5.1</b> Sesgo y varianza de predictores</a><ul>
<li class="chapter" data-level="5.1.1" data-path="regularizacion.html"><a href="regularizacion.html#sesgo-y-varianza-en-modelos-lineales"><i class="fa fa-check"></i><b>5.1.1</b> Sesgo y varianza en modelos lineales</a></li>
<li class="chapter" data-level="5.1.2" data-path="regularizacion.html"><a href="regularizacion.html#reduciendo-varianza-de-los-coeficientes"><i class="fa fa-check"></i><b>5.1.2</b> Reduciendo varianza de los coeficientes</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="regularizacion.html"><a href="regularizacion.html#regularizacion-ridge"><i class="fa fa-check"></i><b>5.2</b> Regularización ridge</a><ul>
<li class="chapter" data-level="5.2.1" data-path="regularizacion.html"><a href="regularizacion.html#seleccion-de-coeficiente-de-regularizacion"><i class="fa fa-check"></i><b>5.2.1</b> Selección de coeficiente de regularización</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="regularizacion.html"><a href="regularizacion.html#entrenamiento-validacion-y-prueba"><i class="fa fa-check"></i><b>5.3</b> Entrenamiento, Validación y Prueba</a><ul>
<li class="chapter" data-level="5.3.1" data-path="regularizacion.html"><a href="regularizacion.html#validacion-cruzada"><i class="fa fa-check"></i><b>5.3.1</b> Validación cruzada</a></li>
<li class="chapter" data-level="5.3.2" data-path="regularizacion.html"><a href="regularizacion.html#como-se-desempena-validacion-cruzada-como-estimacion-del-error"><i class="fa fa-check"></i><b>5.3.2</b> ¿Cómo se desempeña validación cruzada como estimación del error?</a></li>
<li class="chapter" data-level="" data-path="regularizacion.html"><a href="regularizacion.html#ejercicio-4"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="regularizacion.html"><a href="regularizacion.html#regularizacion-lasso"><i class="fa fa-check"></i><b>5.4</b> Regularización lasso</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html"><i class="fa fa-check"></i><b>6</b> Extensiones para regresión lineal y logística</a><ul>
<li class="chapter" data-level="6.1" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#como-hacer-mas-flexible-el-modelo-lineal"><i class="fa fa-check"></i><b>6.1</b> Cómo hacer más flexible el modelo lineal</a></li>
<li class="chapter" data-level="6.2" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#transformacion-de-entradas"><i class="fa fa-check"></i><b>6.2</b> Transformación de entradas</a></li>
<li class="chapter" data-level="6.3" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#variables-cualitativas"><i class="fa fa-check"></i><b>6.3</b> Variables cualitativas</a></li>
<li class="chapter" data-level="6.4" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#interacciones"><i class="fa fa-check"></i><b>6.4</b> Interacciones</a></li>
<li class="chapter" data-level="6.5" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#categorizacion-de-variables"><i class="fa fa-check"></i><b>6.5</b> Categorización de variables</a></li>
<li class="chapter" data-level="6.6" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#splines-opcional"><i class="fa fa-check"></i><b>6.6</b> Splines (opcional)</a></li>
<li class="chapter" data-level="6.7" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#modelando-en-escala-logaritmica"><i class="fa fa-check"></i><b>6.7</b> Modelando en escala logarítmica</a><ul>
<li class="chapter" data-level="6.7.1" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#cuando-usar-estas-tecnicas"><i class="fa fa-check"></i><b>6.7.1</b> ¿Cuándo usar estas técnicas?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html"><i class="fa fa-check"></i><b>7</b> Redes neuronales (parte 1)</a><ul>
<li class="chapter" data-level="7.1" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#introduccion-a-redes-neuronales"><i class="fa fa-check"></i><b>7.1</b> Introducción a redes neuronales</a><ul>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#como-construyen-entradas-las-redes-neuronales"><i class="fa fa-check"></i>¿Cómo construyen entradas las redes neuronales?</a></li>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#como-ajustar-los-parametros"><i class="fa fa-check"></i>¿Cómo ajustar los parámetros?</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#interacciones-en-redes-neuronales"><i class="fa fa-check"></i><b>7.2</b> Interacciones en redes neuronales</a></li>
<li class="chapter" data-level="7.3" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#calculo-en-redes-feed-forward"><i class="fa fa-check"></i><b>7.3</b> Cálculo en redes: feed-forward</a></li>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#notacion"><i class="fa fa-check"></i>Notación</a></li>
<li class="chapter" data-level="7.4" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#feed-forward"><i class="fa fa-check"></i><b>7.4</b> Feed forward</a></li>
<li class="chapter" data-level="7.5" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#backpropagation-calculo-del-gradiente-clasificacion-binaria"><i class="fa fa-check"></i><b>7.5</b> Backpropagation: cálculo del gradiente (clasificación binaria)</a><ul>
<li class="chapter" data-level="7.5.1" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#calculo-para-un-caso-de-entrenamiento"><i class="fa fa-check"></i><b>7.5.1</b> Cálculo para un caso de entrenamiento</a></li>
<li class="chapter" data-level="7.5.2" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#algoritmo-de-backpropagation"><i class="fa fa-check"></i><b>7.5.2</b> Algoritmo de backpropagation</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#ajuste-de-parametros-introduccion"><i class="fa fa-check"></i><b>7.6</b> Ajuste de parámetros (introducción)</a><ul>
<li class="chapter" data-level="7.6.1" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#ejemplo-35"><i class="fa fa-check"></i><b>7.6.1</b> Ejemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html"><i class="fa fa-check"></i><b>8</b> Redes neuronales (parte 2)</a><ul>
<li class="chapter" data-level="8.1" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#descenso-estocastico"><i class="fa fa-check"></i><b>8.1</b> Descenso estocástico</a></li>
<li class="chapter" data-level="8.2" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#algoritmo-de-descenso-estocastico"><i class="fa fa-check"></i><b>8.2</b> Algoritmo de descenso estocástico</a></li>
<li class="chapter" data-level="8.3" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#por-que-usar-descenso-estocastico-por-minilotes"><i class="fa fa-check"></i><b>8.3</b> ¿Por qué usar descenso estocástico por minilotes?</a></li>
<li class="chapter" data-level="8.4" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#escogiendo-la-tasa-de-aprendizaje"><i class="fa fa-check"></i><b>8.4</b> Escogiendo la tasa de aprendizaje</a></li>
<li class="chapter" data-level="8.5" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#mejoras-al-algoritmo-de-descenso-estocastico."><i class="fa fa-check"></i><b>8.5</b> Mejoras al algoritmo de descenso estocástico.</a><ul>
<li class="chapter" data-level="8.5.1" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#decaimiento-de-tasa-de-aprendizaje"><i class="fa fa-check"></i><b>8.5.1</b> Decaimiento de tasa de aprendizaje</a></li>
<li class="chapter" data-level="8.5.2" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#momento"><i class="fa fa-check"></i><b>8.5.2</b> Momento</a></li>
<li class="chapter" data-level="8.5.3" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#otras-variaciones"><i class="fa fa-check"></i><b>8.5.3</b> Otras variaciones</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#ajuste-de-redes-con-descenso-estocastico"><i class="fa fa-check"></i><b>8.6</b> Ajuste de redes con descenso estocástico</a></li>
<li class="chapter" data-level="8.7" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#activaciones-relu"><i class="fa fa-check"></i><b>8.7</b> Activaciones relu</a></li>
<li class="chapter" data-level="8.8" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#dropout-para-regularizacion"><i class="fa fa-check"></i><b>8.8</b> Dropout para regularización</a><ul>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#ejemplo-39"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#ajuste-de-hiperparametros"><i class="fa fa-check"></i><b>8.9</b> Ajuste de hiperparámetros</a><ul>
<li class="chapter" data-level="8.9.1" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#ajuste-manual-de-hiperparametros"><i class="fa fa-check"></i><b>8.9.1</b> Ajuste Manual de Hiperparámetros</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#ajuste-automatico"><i class="fa fa-check"></i><b>8.10</b> Ajuste automático</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html"><i class="fa fa-check"></i><b>9</b> Redes convolucionales</a><ul>
<li class="chapter" data-level="9.1" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#filtros-convolucionales"><i class="fa fa-check"></i><b>9.1</b> Filtros convolucionales</a><ul>
<li class="chapter" data-level="" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#filtros-en-una-dimension"><i class="fa fa-check"></i>Filtros en una dimensión</a></li>
<li class="chapter" data-level="" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#filtros-convolucionales-en-dos-dimensiones"><i class="fa fa-check"></i>Filtros convolucionales en dos dimensiones</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#filtros-convolucionales-para-redes-neuronales"><i class="fa fa-check"></i><b>9.2</b> Filtros convolucionales para redes neuronales</a></li>
<li class="chapter" data-level="9.3" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#capas-de-agregacion-pooling"><i class="fa fa-check"></i><b>9.3</b> Capas de agregación (pooling)</a></li>
<li class="chapter" data-level="9.4" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#ejemplo-arquitectura-lenet"><i class="fa fa-check"></i><b>9.4</b> Ejemplo (arquitectura LeNet):</a><ul>
<li class="chapter" data-level="" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#conteo-de-parametros"><i class="fa fa-check"></i>Conteo de parámetros</a></li>
<li class="chapter" data-level="" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#pesos-y-activaciones"><i class="fa fa-check"></i>Pesos y activaciones</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html"><i class="fa fa-check"></i><b>10</b> Diagnóstico y mejora de modelos</a><ul>
<li class="chapter" data-level="10.1" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#aspectos-generales"><i class="fa fa-check"></i><b>10.1</b> Aspectos generales</a></li>
<li class="chapter" data-level="10.2" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#que-hacer-cuando-el-desempeno-no-es-satisfactorio"><i class="fa fa-check"></i><b>10.2</b> ¿Qué hacer cuando el desempeño no es satisfactorio?</a></li>
<li class="chapter" data-level="10.3" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#pipeline-de-procesamiento"><i class="fa fa-check"></i><b>10.3</b> Pipeline de procesamiento</a></li>
<li class="chapter" data-level="10.4" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#diagnosticos-sesgo-y-varianza"><i class="fa fa-check"></i><b>10.4</b> Diagnósticos: sesgo y varianza</a></li>
<li class="chapter" data-level="10.5" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#refinando-el-pipeline"><i class="fa fa-check"></i><b>10.5</b> Refinando el pipeline</a></li>
<li class="chapter" data-level="10.6" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#consiguiendo-mas-datos"><i class="fa fa-check"></i><b>10.6</b> Consiguiendo más datos</a></li>
<li class="chapter" data-level="10.7" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#usar-datos-adicionales"><i class="fa fa-check"></i><b>10.7</b> Usar datos adicionales</a></li>
<li class="chapter" data-level="10.8" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#examen-de-modelo-y-analisis-de-errores"><i class="fa fa-check"></i><b>10.8</b> Examen de modelo y Análisis de errores</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html"><i class="fa fa-check"></i><b>11</b> Validación de modelos: problemas comunes</a><ul>
<li class="chapter" data-level="11.1" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#filtracion-de-datos"><i class="fa fa-check"></i><b>11.1</b> Filtración de datos</a></li>
<li class="chapter" data-level="11.2" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#series-de-tiempo"><i class="fa fa-check"></i><b>11.2</b> Series de tiempo</a></li>
<li class="chapter" data-level="11.3" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#filtracion-en-el-preprocesamiento"><i class="fa fa-check"></i><b>11.3</b> Filtración en el preprocesamiento</a></li>
<li class="chapter" data-level="11.4" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#uso-de-variables-fuera-de-rango-temporal"><i class="fa fa-check"></i><b>11.4</b> Uso de variables fuera de rango temporal</a></li>
<li class="chapter" data-level="11.5" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#datos-en-conglomerados-y-muestreo-complejo"><i class="fa fa-check"></i><b>11.5</b> Datos en conglomerados y muestreo complejo</a><ul>
<li class="chapter" data-level="" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejemplo-41"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="11.5.1" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#censura-y-evaluacion-incompleta"><i class="fa fa-check"></i><b>11.5.1</b> Censura y evaluación incompleta</a></li>
<li class="chapter" data-level="11.5.2" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejemplo-tiendas-cerradas"><i class="fa fa-check"></i><b>11.5.2</b> Ejemplo: tiendas cerradas</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#muestras-de-validacion-chicas"><i class="fa fa-check"></i><b>11.6</b> Muestras de validación chicas</a><ul>
<li class="chapter" data-level="" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejercicio-7"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#otros-ejemplos"><i class="fa fa-check"></i><b>11.7</b> Otros ejemplos</a></li>
<li class="chapter" data-level="11.8" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#resumen-1"><i class="fa fa-check"></i><b>11.8</b> Resumen</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html"><i class="fa fa-check"></i><b>12</b> Métodos basados en árboles</a><ul>
<li class="chapter" data-level="12.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-regresion-y-clasificacion."><i class="fa fa-check"></i><b>12.1</b> Árboles para regresión y clasificación.</a><ul>
<li class="chapter" data-level="12.1.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-clasificacion"><i class="fa fa-check"></i><b>12.1.1</b> Árboles para clasificación</a></li>
<li class="chapter" data-level="12.1.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#tipos-de-particion"><i class="fa fa-check"></i><b>12.1.2</b> Tipos de partición</a></li>
<li class="chapter" data-level="12.1.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#medidas-de-impureza"><i class="fa fa-check"></i><b>12.1.3</b> Medidas de impureza</a></li>
<li class="chapter" data-level="12.1.4" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#reglas-de-particion-y-tamano-del-arobl"><i class="fa fa-check"></i><b>12.1.4</b> Reglas de partición y tamaño del árobl</a></li>
<li class="chapter" data-level="12.1.5" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#costo---complejidad-breiman"><i class="fa fa-check"></i><b>12.1.5</b> Costo - Complejidad (Breiman)</a></li>
<li class="chapter" data-level="12.1.6" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#opcional-predicciones-con-cart"><i class="fa fa-check"></i><b>12.1.6</b> (Opcional) Predicciones con CART</a></li>
<li class="chapter" data-level="12.1.7" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-regresion"><i class="fa fa-check"></i><b>12.1.7</b> Árboles para regresión</a></li>
<li class="chapter" data-level="12.1.8" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#variabilidad-en-el-proceso-de-construccion"><i class="fa fa-check"></i><b>12.1.8</b> Variabilidad en el proceso de construcción</a></li>
<li class="chapter" data-level="12.1.9" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#relaciones-lineales"><i class="fa fa-check"></i><b>12.1.9</b> Relaciones lineales</a></li>
<li class="chapter" data-level="12.1.10" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ventajas-y-desventajas-de-arboles"><i class="fa fa-check"></i><b>12.1.10</b> Ventajas y desventajas de árboles</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#bagging-de-arboles"><i class="fa fa-check"></i><b>12.2</b> Bagging de árboles</a><ul>
<li class="chapter" data-level="12.2.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ejemplo-48"><i class="fa fa-check"></i><b>12.2.1</b> Ejemplo</a></li>
<li class="chapter" data-level="12.2.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#mejorando-bagging"><i class="fa fa-check"></i><b>12.2.2</b> Mejorando bagging</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#bosques-aleatorios"><i class="fa fa-check"></i><b>12.3</b> Bosques aleatorios</a><ul>
<li class="chapter" data-level="12.3.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#sabiduria-de-las-masas"><i class="fa fa-check"></i><b>12.3.1</b> Sabiduría de las masas</a></li>
<li class="chapter" data-level="12.3.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ejemplo-49"><i class="fa fa-check"></i><b>12.3.2</b> Ejemplo</a></li>
<li class="chapter" data-level="12.3.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#mas-detalles-de-bosques-aleatorios."><i class="fa fa-check"></i><b>12.3.3</b> Más detalles de bosques aleatorios.</a></li>
<li class="chapter" data-level="12.3.4" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#importancia-de-variables"><i class="fa fa-check"></i><b>12.3.4</b> Importancia de variables</a></li>
<li class="chapter" data-level="12.3.5" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ajustando-arboles-aleatorios."><i class="fa fa-check"></i><b>12.3.5</b> Ajustando árboles aleatorios.</a></li>
<li class="chapter" data-level="12.3.6" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ventajas-y-desventajas-de-bosques-aleatorios"><i class="fa fa-check"></i><b>12.3.6</b> Ventajas y desventajas de bosques aleatorios</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html"><i class="fa fa-check"></i><b>13</b> Métodos basados en árboles: boosting</a><ul>
<li class="chapter" data-level="13.1" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#forward-stagewise-additive-modeling-fsam"><i class="fa fa-check"></i><b>13.1</b> Forward stagewise additive modeling (FSAM)</a></li>
<li class="chapter" data-level="13.2" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#discusion-1"><i class="fa fa-check"></i><b>13.2</b> Discusión</a></li>
<li class="chapter" data-level="13.3" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#algoritmo-fsam"><i class="fa fa-check"></i><b>13.3</b> Algoritmo FSAM</a></li>
<li class="chapter" data-level="13.4" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#fsam-para-clasificacion-binaria."><i class="fa fa-check"></i><b>13.4</b> FSAM para clasificación binaria.</a></li>
<li class="chapter" data-level="13.5" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#gradient-boosting"><i class="fa fa-check"></i><b>13.5</b> Gradient boosting</a></li>
<li class="chapter" data-level="13.6" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#algoritmo-de-gradient-boosting"><i class="fa fa-check"></i><b>13.6</b> Algoritmo de gradient boosting</a></li>
<li class="chapter" data-level="13.7" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#funciones-de-perdida"><i class="fa fa-check"></i><b>13.7</b> Funciones de pérdida</a><ul>
<li class="chapter" data-level="13.7.1" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#discusion-adaboost-opcional"><i class="fa fa-check"></i><b>13.7.1</b> Discusión: adaboost (opcional)</a></li>
<li class="chapter" data-level="" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#ejemplo-52"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#modificaciones-de-gradient-boosting"><i class="fa fa-check"></i><b>13.8</b> Modificaciones de Gradient Boosting</a><ul>
<li class="chapter" data-level="13.8.1" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#tasa-de-aprendizaje-shrinkage"><i class="fa fa-check"></i><b>13.8.1</b> Tasa de aprendizaje (shrinkage)</a></li>
<li class="chapter" data-level="13.8.2" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#submuestreo-bag.fraction"><i class="fa fa-check"></i><b>13.8.2</b> Submuestreo (bag.fraction)</a></li>
<li class="chapter" data-level="13.8.3" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#numero-de-arboles-m"><i class="fa fa-check"></i><b>13.8.3</b> Número de árboles M</a></li>
<li class="chapter" data-level="13.8.4" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#tamano-de-arboles"><i class="fa fa-check"></i><b>13.8.4</b> Tamaño de árboles</a></li>
<li class="chapter" data-level="13.8.5" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#controlar-numero-de-casos-para-cortes"><i class="fa fa-check"></i><b>13.8.5</b> Controlar número de casos para cortes</a></li>
<li class="chapter" data-level="" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#ejemplo-53"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="13.8.6" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#evaluacion-con-validacion-cruzada."><i class="fa fa-check"></i><b>13.8.6</b> Evaluación con validación cruzada.</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#graficas-de-dependencia-parcial"><i class="fa fa-check"></i><b>13.9</b> Gráficas de dependencia parcial</a><ul>
<li class="chapter" data-level="13.9.1" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#dependencia-parcial"><i class="fa fa-check"></i><b>13.9.1</b> Dependencia parcial</a></li>
<li class="chapter" data-level="" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#ejemplo-54"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#discusion-2"><i class="fa fa-check"></i>Discusión</a></li>
<li class="chapter" data-level="13.9.2" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#graficas-de-dependencia-parcial-para-otros-modelos"><i class="fa fa-check"></i><b>13.9.2</b> Gráficas de dependencia parcial para otros modelos</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#xgboost-y-gbm"><i class="fa fa-check"></i><b>13.10</b> xgboost y gbm</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Aprendizaje de máquina</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="metodos-basados-en-arboles-boosting" class="section level1">
<h1><span class="header-section-number">Clase 13</span> Métodos basados en árboles: boosting</h1>
<p>Boosting también utiliza la idea de un “ensamble” de árboles. La diferencia
grande con
bagging y bosques aleatorios en que la sucesión de árboles de boosting se
‘adapta’ al comportamiento del predictor a lo largo de las iteraciones,
haciendo reponderaciones de los datos de entrenamiento para que el algoritmo
se concentre en las predicciones más pobres. Boosting generalmente funciona
bien con árboles chicos (cada uno con sesgo alto), mientras que bosques
aleatorios funciona con árboles grandes (sesgo bajo).</p>
<ul>
<li><p>En boosting usamos muchos árboles chicos adaptados secuencialmente. La disminución
del sesgo proviene de usar distintos árboles que se encargan de adaptar el predictor
a distintas partes del conjunto de entrenamiento. El control de varianza se
logra con tasas de aprendizaje y tamaño de árboles, como veremos más adelante.</p></li>
<li><p>En bosques aleatorios usamos muchos árboles grandes, cada uno con una muestra
de entrenamiento perturbada (bootstrap). El control de varianza se logra promediando sobre esas muestras bootstrap de entrenamiento.</p></li>
</ul>
<p>Igual que bosques aleatorios, boosting es también un método que generalmente
tiene alto poder predictivo.</p>
<div id="forward-stagewise-additive-modeling-fsam" class="section level2">
<h2><span class="header-section-number">13.1</span> Forward stagewise additive modeling (FSAM)</h2>
<p>Aunque existen versiones de boosting (Adaboost) desde los 90s, una buena
manera de entender los algoritmos es mediante un proceso general
de modelado por estapas (FSAM).</p>
</div>
<div id="discusion-1" class="section level2">
<h2><span class="header-section-number">13.2</span> Discusión</h2>
<p>Consideramos primero un problema de <em>regresión</em>, que queremos atacar
con un predictor de la forma
<span class="math display">\[f(x) = \sum_{k=1}^m \beta_k b_k(x),\]</span>
donde los <span class="math inline">\(b_k\)</span> son árboles. Podemos absorber el coeficiente <span class="math inline">\(\beta_k\)</span>
dentro del árbol <span class="math inline">\(b_k(x)\)</span>, y escribimos</p>
<p><span class="math display">\[f(x) = \sum_{k=1}^m T_k(x),\]</span></p>
<p>Para ajustar este tipo de modelos, buscamos minimizar
la pérdida de entrenamiento:</p>
<p><span class="math display">\[\begin{equation}
\min \sum_{i=1}^N L(y^{(i)}, \sum_{k=1}^M T_k(x^{(i)}))
\end{equation}\]</span></p>
<p>Este puede ser un problema difícil, dependiendo de la familia
que usemos para los árboles <span class="math inline">\(T_k\)</span>, y sería difícil resolver por fuerza bruta. Para resolver este problema, podemos
intentar una heurística secuencial o por etapas:</p>
<p>Si tenemos
<span class="math display">\[f_{m-1}(x) = \sum_{k=1}^{m-1} T_k(x),\]</span></p>
<p>intentamos resolver el problema (añadir un término adicional)</p>
<p><span class="math display">\[\begin{equation}
\min_{T} \sum_{i=1}^N L(y^{(i)}, f_{m-1}(x^{(i)}) + T(x^{(i)}))
\end{equation}\]</span></p>
<p>Por ejemplo, para pérdida cuadrática (en regresión), buscamos resolver</p>
<p><span class="math display">\[\begin{equation}
\min_{T} \sum_{i=1}^N (y^{(i)} - f_{m-1}(x^{(i)}) - T(x^{(i)}))^2
\end{equation}\]</span></p>
<p>Si ponemos
<span class="math display">\[ r_{m-1}^{(i)} = y^{(i)} - f_{m-1}(x^{(i)}),\]</span>
que es el error para el caso <span class="math inline">\(i\)</span> bajo el modelo <span class="math inline">\(f_{m-1}\)</span>, entonces
reescribimos el problema anterior como
<span class="math display">\[\begin{equation}
\min_{T} \sum_{i=1}^N ( r_{m-1}^{(i)} - T(x^{(i)}))^2
\end{equation}\]</span></p>
<p>Este problema consiste en <em>ajustar un árbol a los residuales o errores
del paso anterior</em>. Otra manera de decir esto es que añadimos un término adicional
que intenta corregir los que el modelo anterior no pudo predecir bien.
La idea es repetir este proceso para ir reduciendo los residuales, agregando
un árbol a la vez.</p>

<div class="comentario">
La primera idea central de boosting es concentrarnos, en el siguiente paso, en los datos donde tengamos errores, e intentar corregir añadiendo un término
adicional al modelo.
</div>

</div>
<div id="algoritmo-fsam" class="section level2">
<h2><span class="header-section-number">13.3</span> Algoritmo FSAM</h2>
<p>Esta idea es la base del siguiente algoritmo:</p>

<div class="comentario">
<p><strong>Algoritmo FSAM</strong> (forward stagewise additive modeling)</p>
<ol style="list-style-type: decimal">
<li>Tomamos <span class="math inline">\(f_0(x)=0\)</span></li>
<li>Para <span class="math inline">\(m=1\)</span> hasta <span class="math inline">\(M\)</span>,</li>
</ol>
<ul>
<li>Resolvemos
<span class="math display">\[T_m = argmin_{T} \sum_{i=1}^N L(y^{(i)}, f_{m-1}(x^{(i)}) + T(x^{(i)}))\]</span></li>
<li>Ponemos
<span class="math display">\[f_m(x) = f_{m-1}(x) + T_m(x)\]</span></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Nuestro predictor final es <span class="math inline">\(f(x) = \sum_{m=1}^M T_(x)\)</span>.
</div></li>
</ol>
<p><strong>Observaciones</strong>:
Generalmente los árboles sobre los que optimizamos están restringidos a una familia relativamente chica: por ejemplo, árboles de profundidad no mayor a
<span class="math inline">\(2,3,\ldots, 8\)</span>.</p>
<p>Este algoritmo se puede aplicar directamente para problemas de regresión, como vimos en la discusión anterior: simplemente hay que ajustar árboles a los residuales del modelo del paso anterior. Sin embargo, no está claro cómo aplicarlo cuando la función de pérdida no es mínimos cuadrados (por ejemplo,
regresión logística).</p>
<div id="ejemplo-regresion" class="section level4 unnumbered">
<h4>Ejemplo (regresión)</h4>
<p>Podemos hacer FSAM directamente sobre un problema de regresión.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">227818</span>)
<span class="kw">library</span>(rpart)
<span class="kw">library</span>(tidyverse)
x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">200</span>, <span class="dv">0</span>, <span class="dv">30</span>)
y &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">ifelse</span>(x <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>, <span class="dv">0</span>, <span class="kw">sqrt</span>(x)) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">200</span>, <span class="dv">0</span>, <span class="fl">0.5</span>)
dat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y)</code></pre>
<p>Pondremos los árboles de cada paso en una lista. Podemos comenzar con una constante
en lugar de 0.</p>
<pre class="sourceCode r"><code class="sourceCode r">arboles_fsam &lt;-<span class="st"> </span><span class="kw">list</span>()
arboles_fsam[[<span class="dv">1</span>]] &lt;-<span class="st"> </span><span class="kw">rpart</span>(y<span class="op">~</span>x, <span class="dt">data =</span> dat, 
                           <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">maxdepth=</span><span class="dv">0</span>))
arboles_fsam[[<span class="dv">1</span>]]</code></pre>
<pre><code>## n= 200 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
## 1) root 200 5370.398 4.675925 *</code></pre>
<p>Ahora construirmos nuestra función de predicción y el paso
que agrega un árbol</p>
<pre class="sourceCode r"><code class="sourceCode r">predecir_arboles &lt;-<span class="st"> </span><span class="cf">function</span>(arboles_fsam, x){
  preds &lt;-<span class="st"> </span><span class="kw">lapply</span>(arboles_fsam, <span class="cf">function</span>(arbol){
    <span class="kw">predict</span>(arbol, <span class="kw">data.frame</span>(<span class="dt">x=</span>x))
  })
  <span class="kw">reduce</span>(preds, <span class="st">`</span><span class="dt">+</span><span class="st">`</span>)
}
agregar_arbol &lt;-<span class="st"> </span><span class="cf">function</span>(arboles_fsam, dat, <span class="dt">plot=</span><span class="ot">TRUE</span>){
  n &lt;-<span class="st"> </span><span class="kw">length</span>(arboles_fsam)
  preds &lt;-<span class="st"> </span><span class="kw">predecir_arboles</span>(arboles_fsam, <span class="dt">x=</span>dat<span class="op">$</span>x)
  dat<span class="op">$</span>res &lt;-<span class="st"> </span>y <span class="op">-</span><span class="st"> </span>preds
  arboles_fsam[[n<span class="op">+</span><span class="dv">1</span>]] &lt;-<span class="st"> </span><span class="kw">rpart</span>(res <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> dat, 
                           <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">maxdepth =</span> <span class="dv">1</span>))
  dat<span class="op">$</span>preds_nuevo &lt;-<span class="st"> </span><span class="kw">predict</span>(arboles_fsam[[n<span class="op">+</span><span class="dv">1</span>]])
  dat<span class="op">$</span>preds &lt;-<span class="st"> </span><span class="kw">predecir_arboles</span>(arboles_fsam, <span class="dt">x=</span>dat<span class="op">$</span>x)
  g_res &lt;-<span class="st"> </span><span class="kw">ggplot</span>(dat, <span class="kw">aes</span>(<span class="dt">x =</span> x)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y=</span>preds_nuevo)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y=</span>res)) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&#39;Residuales&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylim</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>))
  g_agregado &lt;-<span class="st"> </span><span class="kw">ggplot</span>(dat, <span class="kw">aes</span>(<span class="dt">x=</span>x)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y=</span>preds), <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>,
                                                  <span class="dt">size=</span><span class="fl">1.1</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y=</span>y)) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">title =</span><span class="st">&#39;Ajuste&#39;</span>)
  <span class="cf">if</span>(plot){
    <span class="kw">print</span>(g_res)
    <span class="kw">print</span>(g_agregado)
  }
  arboles_fsam
}</code></pre>
<p>Ahora construiremos el primer árbol. Usaremos ‘troncos’ (stumps), árboles con
un solo corte: Los primeros residuales son simplemente las <span class="math inline">\(y\)</span>’s observadas</p>
<pre class="sourceCode r"><code class="sourceCode r">arboles_fsam &lt;-<span class="st"> </span><span class="kw">agregar_arbol</span>(arboles_fsam, dat)</code></pre>
<pre><code>## Warning: Removed 8 rows containing missing values (geom_point).</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-7-1.png" width="384" /><img src="13-arboles-2_files/figure-html/unnamed-chunk-7-2.png" width="384" /></p>
<p>Ajustamos un árbol de regresión a los residuales:</p>
<pre class="sourceCode r"><code class="sourceCode r">arboles_fsam &lt;-<span class="st"> </span><span class="kw">agregar_arbol</span>(arboles_fsam, dat)</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-8-1.png" width="384" /><img src="13-arboles-2_files/figure-html/unnamed-chunk-8-2.png" width="384" /></p>
<pre class="sourceCode r"><code class="sourceCode r">arboles_fsam &lt;-<span class="st"> </span><span class="kw">agregar_arbol</span>(arboles_fsam, dat)</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-9-1.png" width="384" /><img src="13-arboles-2_files/figure-html/unnamed-chunk-9-2.png" width="384" /></p>
<pre class="sourceCode r"><code class="sourceCode r">arboles_fsam &lt;-<span class="st"> </span><span class="kw">agregar_arbol</span>(arboles_fsam, dat)</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-10-1.png" width="384" /><img src="13-arboles-2_files/figure-html/unnamed-chunk-10-2.png" width="384" /></p>
<pre class="sourceCode r"><code class="sourceCode r">arboles_fsam &lt;-<span class="st"> </span><span class="kw">agregar_arbol</span>(arboles_fsam, dat)</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-11-1.png" width="384" /><img src="13-arboles-2_files/figure-html/unnamed-chunk-11-2.png" width="384" /></p>
<pre class="sourceCode r"><code class="sourceCode r">arboles_fsam &lt;-<span class="st"> </span><span class="kw">agregar_arbol</span>(arboles_fsam, dat)</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-12-1.png" width="384" /><img src="13-arboles-2_files/figure-html/unnamed-chunk-12-2.png" width="384" /></p>
<p>Después de 20 iteraciones obtenemos:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">19</span>){
arboles_fsam &lt;-<span class="st"> </span><span class="kw">agregar_arbol</span>(arboles_fsam, dat, <span class="dt">plot =</span> <span class="ot">FALSE</span>)
}
arboles_fsam &lt;-<span class="st"> </span><span class="kw">agregar_arbol</span>(arboles_fsam, dat)</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-13-1.png" width="384" /><img src="13-arboles-2_files/figure-html/unnamed-chunk-13-2.png" width="384" /></p>
</div>
</div>
<div id="fsam-para-clasificacion-binaria." class="section level2">
<h2><span class="header-section-number">13.4</span> FSAM para clasificación binaria.</h2>
<p>Para problemas de clasificación, no tiene mucho sentido trabajar con un modelo
aditivo sobre las probabilidades:</p>
<p><span class="math display">\[p(x) = \sum_{k=1}^m T_k(x),\]</span></p>
<p>Así que hacemos lo mismo que en regresión logística. Ponemos</p>
<p><span class="math display">\[f(x) = \sum_{k=1}^m T_k(x),\]</span></p>
<p>y entonces las probabilidades son
<span class="math display">\[p(x) = h(f(x)),\]</span></p>
<p>donde <span class="math inline">\(h(z)=1/(1+e^{-z})\)</span> es la función logística. La optimización de la etapa <span class="math inline">\(m\)</span> según fsam es</p>
<p><span class="math display" id="eq:fsam-paso">\[\begin{equation}
T = argmin_{T} \sum_{i=1}^N L(y^{(i)}, f_{m-1}(x^{(i)}) + T(x^{(i)}))
\tag{13.1}
\end{equation}\]</span></p>
<p>y queremos usar la devianza como función de pérdida. Por razones
de comparación (con nuestro libro de texto y con el algoritmo Adaboost
que mencionaremos más adelante), escogemos usar
<span class="math display">\[y \in \{1,-1\}\]</span></p>
<p>en lugar de nuestro tradicional <span class="math inline">\(y \in \{1,0\}\)</span>. En ese caso, la devianza
binomial se ve como</p>
<p><span class="math display">\[L(y, z) = -\left [ (y+1)\log h(z) - (y-1)\log(1-h(z))\right ],\]</span>
que a su vez se puede escribir como (demostrar):</p>
<p><span class="math display">\[L(y,z) = 2\log(1+e^{-yz})\]</span>
Ahora consideremos cómo se ve nuestro problema de optimización:</p>
<p><span class="math display">\[T = argmin_{T} 2\sum_{i=1}^N \log (1+ e^{-y^{(i)}(f_{m-1}(x^{(i)}) + T(x^{(i)})})\]</span></p>
<p>Nótese que sólo optimizamos con respecto a <span class="math inline">\(T\)</span>, así que
podemos escribir</p>
<p><span class="math display">\[T = argmin_{T} 2\sum_{i=1}^N \log (1+ d_{m,i}e^{- y^{(i)}T(x^{(i)})})\]</span></p>
<p>Y vemos que el problema es más difícil que en regresión. No podemos usar
un ajuste de árbol usual de regresión o clasificación, <em>como hicimos en
regresión</em>. No está claro, por ejemplo, cuál debería ser el residual
que tenemos que ajustar (aunque parece un problema donde los casos
de entrenamiento están ponderados por <span class="math inline">\(d_{m,i}\)</span>). Una solución para resolver aproximadamente este problema de minimización, es <strong>gradient boosting</strong>.</p>
</div>
<div id="gradient-boosting" class="section level2">
<h2><span class="header-section-number">13.5</span> Gradient boosting</h2>
<p>La idea de gradient boosting es replicar la idea del residual en regresión, y usar
árboles de regresión para resolver <a href="metodos-basados-en-arboles-boosting.html#eq:fsam-paso">(13.1)</a>.</p>
<p>Gradient boosting es una técnica general para funciones de pérdida
generales.Regresamos entonces a nuestro problema original</p>
<p><span class="math display">\[(\beta_m, b_m) = argmin_{T} \sum_{i=1}^N L(y^{(i)}, f_{m-1}(x^{(i)}) + T(x^{(i)}))\]</span></p>
<p>La pregunta es: ¿hacia dónde tenemos qué mover la predicción de
<span class="math inline">\(f_{m-1}(x^{(i)})\)</span> sumando
el término <span class="math inline">\(T(x^{(i)})\)</span>? Consideremos un solo término de esta suma,
y denotemos <span class="math inline">\(z_i = T(x^{(i)})\)</span>. Queremos agregar una cantidad <span class="math inline">\(z_i\)</span>
tal que el valor de la pérdida
<span class="math display">\[L(y, f_{m-1}(x^{(i)})+z_i)\]</span>
se reduzca. Entonces sabemos que podemos mover la z en la dirección opuesta al gradiente</p>
<p><span class="math display">\[z_i = -\gamma \frac{\partial L}{\partial z}(y^{(i)}, f_{m-1}(x^{(i)}))\]</span></p>
<p>Sin embargo, necesitamos que las <span class="math inline">\(z_i\)</span> estén generadas por una función <span class="math inline">\(T(x)\)</span> que se pueda evaluar en toda <span class="math inline">\(x\)</span>. Quisiéramos que
<span class="math display">\[T(x^{(i)})\approx -\gamma \frac{\partial L}{\partial z}(y^{(i)}, f_{m-1}(x^{(i)}))\]</span>
Para tener esta aproximación, podemos poner
<span class="math display">\[g_{i,m} = -\frac{\partial L}{\partial z}(y^{(i)}, f_{m-1}(x^{(i)}))\]</span>
e intentar resolver
<span class="math display" id="eq:min-cuad-boost">\[\begin{equation}
\min_T \sum_{i=1}^n (g_{i,m} - T(x^{(i)}))^2,
\tag{13.2}
\end{equation}\]</span></p>
<p>es decir, intentamos replicar los gradientes lo más que sea posible. <strong>Este problema lo podemos resolver con un árbol usual de regresión</strong>. Finalmente,
podríamos escoger <span class="math inline">\(\nu\)</span> (tamaño de paso) suficientemente chica y ponemos
<span class="math display">\[f_m(x) = f_{m-1}(x)+\nu T(x).\]</span></p>
<p>Podemos hacer un refinamiento adicional que consiste en encontrar los cortes del árbol <span class="math inline">\(T\)</span> según <a href="metodos-basados-en-arboles-boosting.html#eq:min-cuad-boost">(13.2)</a>, pero optimizando por separado los valores que T(x) toma en cada una de las regiones encontradas.</p>
</div>
<div id="algoritmo-de-gradient-boosting" class="section level2">
<h2><span class="header-section-number">13.6</span> Algoritmo de gradient boosting</h2>

<div class="comentario">
<p><strong>Gradient boosting</strong> (versión simple)</p>
<ol style="list-style-type: decimal">
<li><p>Inicializar con <span class="math inline">\(f_0(x) =\gamma\)</span></p></li>
<li><p>Para <span class="math inline">\(m=0,1,\ldots, M\)</span>,</p></li>
</ol>
<ul>
<li><p>Para <span class="math inline">\(i=1,\ldots, n\)</span>, calculamos el residual
<span class="math display">\[r_{i,m}=-\frac{\partial L}{\partial z}(y^{(i)}, f_{m-1}(x^{(i)}))\]</span></p></li>
<li><p>Ajustamos un árbol de regresión a la respuesta <span class="math inline">\(r_{1,m},r_{2,m},\ldots, r_{n,m}\)</span>. Supongamos que tiene regiones <span class="math inline">\(R_{j,m}\)</span>.</p></li>
<li>Resolvemos (optimizamos directamente el valor que toma el árbol en cada región - este es un problema univariado, más fácil de resolver)
<span class="math display">\[\gamma_{j,m} = argmin_\gamma \sum_{x^{(i)}\in R_{j,m}} L(y^{(i)},f_{m-1}(x^{i})+\gamma )\]</span>
para cada región <span class="math inline">\(R_{j,m}\)</span> del árbol del inciso anterior.</li>
<li>Actualizamos <span class="math display">\[f_m (x) = f_{m-1}(x) + \sum_j \gamma_{j,m} I(x\in R_{j,m})\]</span></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>El predictor final es <span class="math inline">\(f_M(x)\)</span>.
</div></li>
</ol>
</div>
<div id="funciones-de-perdida" class="section level2">
<h2><span class="header-section-number">13.7</span> Funciones de pérdida</h2>
<p>Para aplicar gradient boosting, tenemos primero que poder calcular
el gradiente de la función de pérdida. Algunos ejemplos populares son:</p>
<ul>
<li>Pérdida cuadrática: <span class="math inline">\(L(y,f(x))=(y-f(x))^2\)</span>,
<span class="math inline">\(\frac{\partial L}{\partial z} = -2(y-f(x))\)</span>.</li>
<li>Pérdida absoluta (más robusta a atípicos que la cuadrática) <span class="math inline">\(L(y,f(x))=|y-f(x)|\)</span>,
<span class="math inline">\(\frac{\partial L}{\partial z} = signo(y-f(x))\)</span>.</li>
<li>Devianza binomial <span class="math inline">\(L(y, f(x))\)</span> = -(1+e^{-yf(x)}), <span class="math inline">\(y\in\{-1,1\}\)</span>,
<span class="math inline">\(\frac{\partial L}{\partial z} = I(y=1) - h(f(x))\)</span>.</li>
<li>Adaboost, pérdida exponencial (para clasificación) <span class="math inline">\(L(y,z) = e^{-yf(x)}\)</span>,
<span class="math inline">\(y\in\{-1,1\}\)</span>,
<span class="math inline">\(\frac{\partial L}{\partial z} = -ye^{-yf(x)}\)</span>.</li>
</ul>
<div id="discusion-adaboost-opcional" class="section level3">
<h3><span class="header-section-number">13.7.1</span> Discusión: adaboost (opcional)</h3>
<p>Adaboost es uno de los algoritmos originales para boosting, y no es necesario
usar gradient boosting para aplicarlo. La razón es que los árboles de clasificación
<span class="math inline">\(T(x)\)</span> toman valores <span class="math inline">\(T(x)\in \{-1,1\}\)</span>, y el paso de optimización
<a href="metodos-basados-en-arboles-boosting.html#eq:fsam-paso">(13.1)</a> de cada árbol queda</p>
<p><span class="math display">\[T = argmin_{T} \sum_{i=1}^N e^{-y^{(i)}f_{m-1}(x^{(i)})} e^{-y^{(i)}T(x^{(i)})}
\]</span>
<span class="math display">\[T = argmin_{T} \sum_{i=1}^N d_{m,i} e^{-y^{(i)}T(x^{(i)})}
\]</span>
De modo que la función objetivo toma dos valores: Si <span class="math inline">\(T(x^{i})\)</span> clasifica
correctamente, entonces <span class="math inline">\(e^{-y^{(i)}T(x^{(i)})}=e^{-1}\)</span>, y si
clasifica incorrectamente <span class="math inline">\(e^{-y^{(i)}T(x^{(i)})}=e^{1}\)</span>. Podemos entonces
encontrar el árbol <span class="math inline">\(T\)</span> construyendo un árbol usual pero con datos ponderados
por <span class="math inline">\(d_{m,i}\)</span>, donde buscamos maximizar la tasa de clasificación correcta (puedes
ver más en nuestro libro de texto, o en <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-ESL">2017</a>)</span>.</p>
<p>¿Cuáles son las consecuencias de usar la pérdida exponencial? Una es que perdemos
la conexión con los modelos logísticos e interpretación de probabilidad que tenemos
cuando usamos la devianza. Sin embargo, son similares: compara cómo se ve
la devianza (como la formulamos arriba, con <span class="math inline">\(y\in\{-1,1\}\)</span>) con la pérdida exponencial.</p>
</div>
<div id="ejemplo-52" class="section level3 unnumbered">
<h3>Ejemplo</h3>
<p>Podemos usar el paquete de R <em>gbm</em> para hacer gradient boosting. Para el
caso de precios de casas de la sección anterior (un problema de regresión).
Para ver un ejemplo distinto, utilizaremos la pérdida absoluta en lugar
de pérdida cuadrática:</p>
<p>Fijaremos el número de árboles en 200, de profundidad 3, usando
75% de la muestra para entrenar y el restante para validación:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gbm)
entrena &lt;-<span class="st"> </span><span class="kw">read_rds</span>(<span class="st">&#39;datos/ameshousing-entrena-procesado.rds&#39;</span>)
<span class="kw">set.seed</span>(<span class="dv">23411</span>)

ajustar_boost &lt;-<span class="st"> </span><span class="cf">function</span>(entrena, ...){
  mod_boosting &lt;-<span class="st"> </span><span class="kw">gbm</span>(<span class="kw">log</span>(vSalePrice) <span class="op">~</span>.,  <span class="dt">data =</span> entrena,
                <span class="dt">distribution =</span> <span class="st">&#39;laplace&#39;</span>,
                <span class="dt">n.trees =</span> <span class="dv">200</span>, 
                <span class="dt">interaction.depth =</span> <span class="dv">3</span>,
                <span class="dt">shrinkage =</span> <span class="dv">1</span>, <span class="co"># tasa de aprendizaje</span>
                <span class="dt">bag.fraction =</span> <span class="dv">1</span>,
                <span class="dt">train.fraction =</span> <span class="fl">0.75</span>)
  mod_boosting
}

house_boosting &lt;-<span class="st"> </span><span class="kw">ajustar_boost</span>(entrena)
dat_entrenamiento &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">entrena =</span> house_boosting<span class="op">$</span>train.error,
                                <span class="dt">valida =</span> house_boosting<span class="op">$</span>valid.error,
                                <span class="dt">n_arbol =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(house_boosting<span class="op">$</span>train.error)) <span class="op">%&gt;%</span>
<span class="st">                      </span><span class="kw">gather</span>(tipo, valor, <span class="op">-</span>n_arbol)
<span class="kw">print</span>(house_boosting)</code></pre>
<pre><code>## gbm(formula = log(vSalePrice) ~ ., distribution = &quot;laplace&quot;, 
##     data = entrena, n.trees = 200, interaction.depth = 3, shrinkage = 1, 
##     bag.fraction = 1, train.fraction = 0.75)
## A gradient boosted model with laplace loss function.
## 200 iterations were performed.</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-15-1.png" width="480" /></p>
<pre><code>## The best test-set iteration was 161.
## There were 79 predictors of which 63 had non-zero influence.</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(dat_entrenamiento, <span class="kw">aes</span>(<span class="dt">x=</span>n_arbol, <span class="dt">y=</span>valor, <span class="dt">colour=</span>tipo, <span class="dt">group=</span>tipo)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>()</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-15-2.png" width="480" /></p>
<p>Que se puede graficar también así:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gbm.perf</span>(house_boosting)</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-16-1.png" width="480" /></p>
<pre><code>## [1] 161</code></pre>
<p>Como vemos, tenemos que afinar los parámetros del algoritmo.</p>
</div>
</div>
<div id="modificaciones-de-gradient-boosting" class="section level2">
<h2><span class="header-section-number">13.8</span> Modificaciones de Gradient Boosting</h2>
<p>Hay algunas adiciones al algoritmo de gradient boosting que podemos
usar para mejorar el desempeño. Los dos métodos que comunmente se
usan son encogimiento (<em>shrinkage</em>), que es una especie de tasa de
aprendizaje, y submuestreo, donde construimos cada árbol adicional
usando una submuestra de la muestra de entrenamiento.</p>
<p>Ambas podemos verlas como técnicas de regularización, que limitan
sobreajuste producido por el algoritmo agresivo de boosting.</p>
<div id="tasa-de-aprendizaje-shrinkage" class="section level3">
<h3><span class="header-section-number">13.8.1</span> Tasa de aprendizaje (shrinkage)</h3>
<p>Funciona bien modificar el algoritmo usando una tasa de aprendizae
<span class="math inline">\(0&lt;\nu&lt;1\)</span>:
<span class="math display">\[f_m(x) = f_{m-1}(x) + \nu \sum_j \gamma_{j,m} I(x\in R_{j,m})\]</span></p>
<p>Este parámetro sirve como una manera de evitar sobreajuste rápido cuando
construimos los predictores. Si este número es muy alto, podemos sobreajustar
rápidamente con pocos árboles, y terminar con predictor de varianza alta. Si este
número es muy bajo, puede ser que necesitemos demasiadas iteraciones para llegar
a buen desempeño.</p>
<p>Igualmente se prueba con varios valores de <span class="math inline">\(0&lt;\nu&lt;1\)</span> (típicamente <span class="math inline">\(\nu&lt;0.1\)</span>)
para mejorar el desempeño en validación. <strong>Nota</strong>: cuando hacemos <span class="math inline">\(\nu\)</span> más chica, es necesario hacer <span class="math inline">\(M\)</span> más grande (correr más árboles) para obtener desempeño
óptimo.</p>
<p>Veamos que efecto tiene en nuestro ejemplo:</p>
<pre class="sourceCode r"><code class="sourceCode r">modelos_dat &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">n_modelo =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">shrinkage =</span> <span class="kw">c</span>(<span class="fl">0.02</span>, <span class="fl">0.05</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>))
modelos_dat &lt;-<span class="st"> </span>modelos_dat <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">modelo =</span> <span class="kw">map</span>(shrinkage, boost)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">eval =</span> <span class="kw">map</span>(modelo, eval_modelo))
modelos_dat</code></pre>
<pre><code>## # A tibble: 4 x 4
##   n_modelo shrinkage modelo    eval                
##      &lt;int&gt;     &lt;dbl&gt; &lt;list&gt;    &lt;list&gt;              
## 1        1      0.02 &lt;S3: gbm&gt; &lt;tibble [1,000 × 3]&gt;
## 2        2      0.05 &lt;S3: gbm&gt; &lt;tibble [1,000 × 3]&gt;
## 3        3      0.25 &lt;S3: gbm&gt; &lt;tibble [1,000 × 3]&gt;
## 4        4      0.5  &lt;S3: gbm&gt; &lt;tibble [1,000 × 3]&gt;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">graf_eval &lt;-<span class="st"> </span>modelos_dat <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(shrinkage, eval) <span class="op">%&gt;%</span><span class="st"> </span>unnest
graf_eval</code></pre>
<pre><code>## # A tibble: 4,000 x 4
##    shrinkage n_arbol tipo    valor
##        &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;
##  1      0.02       1 entrena 0.309
##  2      0.02       2 entrena 0.305
##  3      0.02       3 entrena 0.301
##  4      0.02       4 entrena 0.297
##  5      0.02       5 entrena 0.294
##  6      0.02       6 entrena 0.290
##  7      0.02       7 entrena 0.287
##  8      0.02       8 entrena 0.283
##  9      0.02       9 entrena 0.280
## 10      0.02      10 entrena 0.277
## # ... with 3,990 more rows</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">filter</span>(graf_eval), 
       <span class="kw">aes</span>(<span class="dt">x =</span> n_arbol, <span class="dt">y=</span> valor, 
           <span class="dt">colour=</span><span class="kw">factor</span>(shrinkage), <span class="dt">group =</span> shrinkage)) <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>tipo)</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-18-1.png" width="480" /></p>
<p>Obsérvese que podemos obtener un mejor resultado de validación afinando
la tasa de aprendizaje. Cuando es muy grande, el modelo rápidamente sobreajusta
cuando agregamos árboles. Si la tasa es demasiado chica, podos tardar
mucho en llegar a un predictor de buen desempeño.</p>
</div>
<div id="submuestreo-bag.fraction" class="section level3">
<h3><span class="header-section-number">13.8.2</span> Submuestreo (bag.fraction)</h3>
<p>Funciona bien construir cada uno de los árboles con submuestras de la muestra
de entrenamiento, como una manera adicional de reducir varianza al construir
nuestro predictor (esta idea es parecida a la de los bosques aleatorios,
aquí igualmente perturbamos la muestra de entrenamiento en cada paso para evitar
sobreajuste). Adicionalmente, este proceso acelera considerablemente las
iteraciones de boosting, y en algunos casos sin penalización en desempeño.</p>
<p>En boosting generalmente se toman submuestras (una
fracción de alrededor de 0.5 de la muestra de entrenamiento, pero puede
ser más chica para conjuntos grandes de entrenamiento) sin reemplazo.</p>
<p>Este parámetro también puede ser afinado con muestra
de validación o validación cruzada.</p>
<pre class="sourceCode r"><code class="sourceCode r">boost &lt;-<span class="st"> </span><span class="kw">ajustar_boost</span>(entrena)
modelos_dat &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">n_modelo =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, 
                          <span class="dt">bag.fraction =</span> <span class="kw">c</span>(<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="dv">1</span>),
                          <span class="dt">shrinkage =</span> <span class="fl">0.25</span>)
modelos_dat &lt;-<span class="st"> </span>modelos_dat <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">modelo =</span> <span class="kw">pmap</span>(., boost)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">eval =</span> <span class="kw">map</span>(modelo, eval_modelo))
modelos_dat</code></pre>
<pre><code>## # A tibble: 3 x 5
##   n_modelo bag.fraction shrinkage modelo    eval                
##      &lt;int&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;list&gt;    &lt;list&gt;              
## 1        1         0.25      0.25 &lt;S3: gbm&gt; &lt;tibble [1,000 × 3]&gt;
## 2        2         0.5       0.25 &lt;S3: gbm&gt; &lt;tibble [1,000 × 3]&gt;
## 3        3         1         0.25 &lt;S3: gbm&gt; &lt;tibble [1,000 × 3]&gt;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">graf_eval &lt;-<span class="st"> </span>modelos_dat <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(bag.fraction, eval) <span class="op">%&gt;%</span><span class="st"> </span>unnest
graf_eval</code></pre>
<pre><code>## # A tibble: 3,000 x 4
##    bag.fraction n_arbol tipo    valor
##           &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;
##  1         0.25       1 entrena 0.269
##  2         0.25       2 entrena 0.232
##  3         0.25       3 entrena 0.211
##  4         0.25       4 entrena 0.194
##  5         0.25       5 entrena 0.179
##  6         0.25       6 entrena 0.170
##  7         0.25       7 entrena 0.162
##  8         0.25       8 entrena 0.152
##  9         0.25       9 entrena 0.147
## 10         0.25      10 entrena 0.142
## # ... with 2,990 more rows</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>((graf_eval), <span class="kw">aes</span>(<span class="dt">x =</span> n_arbol, <span class="dt">y=</span> valor, <span class="dt">colour=</span><span class="kw">factor</span>(bag.fraction), <span class="dt">group =</span>
                        bag.fraction)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>tipo, <span class="dt">ncol =</span> <span class="dv">1</span>)</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-19-1.png" width="480" /></p>
<p>En este ejemplo, podemos reducir el tiempo de ajuste usando una
fracción de submuestro de 0.5, con quizá algunas mejoras en desempeño.</p>
<p>Ahora veamos los dos parámetros actuando en conjunto:</p>
<pre class="sourceCode r"><code class="sourceCode r">modelos_dat &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">bag.fraction =</span> <span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="dv">1</span>),
                          <span class="dt">shrinkage =</span> <span class="kw">c</span>(<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>)) <span class="op">%&gt;%</span><span class="st"> </span>expand.grid
modelos_dat &lt;-<span class="st"> </span>modelos_dat <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">modelo =</span> <span class="kw">pmap</span>(., boost)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">eval =</span> <span class="kw">map</span>(modelo, eval_modelo))
graf_eval &lt;-<span class="st"> </span>modelos_dat <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(shrinkage, bag.fraction, eval) <span class="op">%&gt;%</span><span class="st"> </span>unnest
<span class="kw">head</span>(graf_eval)</code></pre>
<pre><code>##   shrinkage bag.fraction n_arbol    tipo     valor
## 1      0.01          0.1       1 entrena 0.3108616
## 2      0.01          0.1       2 entrena 0.3087372
## 3      0.01          0.1       3 entrena 0.3065518
## 4      0.01          0.1       4 entrena 0.3047564
## 5      0.01          0.1       5 entrena 0.3027629
## 6      0.01          0.1       6 entrena 0.3010770</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">filter</span>(graf_eval, tipo <span class="op">==</span><span class="st">&#39;valida&#39;</span>), <span class="kw">aes</span>(<span class="dt">x =</span> n_arbol, <span class="dt">y=</span> valor, <span class="dt">colour=</span><span class="kw">factor</span>(bag.fraction), <span class="dt">group =</span>
                        bag.fraction)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>shrinkage)</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-20-1.png" width="480" /></p>
<p>Bag fraction demasiado chico no funciona bien, especialmente si la tasa
de aprendizaje es alta (¿Por qué?). Filtremos para ver con detalle el resto
de los datos:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">filter</span>(graf_eval, tipo <span class="op">==</span><span class="st">&#39;valida&#39;</span>, bag.fraction<span class="op">&gt;</span><span class="fl">0.1</span>), <span class="kw">aes</span>(<span class="dt">x =</span> n_arbol, <span class="dt">y=</span> valor, <span class="dt">colour=</span><span class="kw">factor</span>(bag.fraction), <span class="dt">group =</span>
                        bag.fraction)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>shrinkage) <span class="op">+</span><span class="st"> </span><span class="kw">scale_y_log10</span>()</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-21-1.png" width="480" /></p>
<p>Y parece ser que para este número de iteraciones, una tasa de aprendizaje
de 0.1 junto con un bag fraction de 0.5 funciona bien:</p>
<pre class="sourceCode r"><code class="sourceCode r">graf_eval <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(tipo<span class="op">==</span><span class="st">&#39;valida&#39;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(shrinkage, bag.fraction) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">valor =</span> <span class="kw">min</span>(valor)) <span class="op">%&gt;%</span>
<span class="st">   </span><span class="kw">arrange</span>(valor) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>(<span class="dv">10</span>)</code></pre>
<pre><code>## # A tibble: 10 x 3
## # Groups:   shrinkage [4]
##    shrinkage bag.fraction  valor
##        &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;
##  1      0.1          0.25 0.0830
##  2      0.1          0.5  0.0837
##  3      0.1          1    0.0864
##  4      0.25         0.5  0.0865
##  5      0.25         1    0.0903
##  6      0.1          0.1  0.0964
##  7      0.25         0.25 0.0979
##  8      0.01         1    0.100 
##  9      0.01         0.5  0.100 
## 10      0.5          0.5  0.101</code></pre>
</div>
<div id="numero-de-arboles-m" class="section level3">
<h3><span class="header-section-number">13.8.3</span> Número de árboles M</h3>
<p>Se monitorea el error sobre una muestra de validación cuando agregamos
cada árboles. Escogemos el número de árboles de manera que minimize el
error de validación. Demasiados árboles pueden producir sobreajuste. Ver el ejemplo
de arriba.</p>
</div>
<div id="tamano-de-arboles" class="section level3">
<h3><span class="header-section-number">13.8.4</span> Tamaño de árboles</h3>
<p>Los árboles se construyen de tamaño fijo <span class="math inline">\(J\)</span>, donde <span class="math inline">\(J\)</span> es el número
de cortes. Usualmente <span class="math inline">\(J=1,2,\ldots, 10\)</span>, y es un parámetro que hay que
elegir. <span class="math inline">\(J\)</span> más grande permite interacciones de orden más alto entre
las variables de entrada. Se intenta con varias <span class="math inline">\(J\)</span> y <span class="math inline">\(M\)</span> para minimizar
el error de validación.</p>
</div>
<div id="controlar-numero-de-casos-para-cortes" class="section level3">
<h3><span class="header-section-number">13.8.5</span> Controlar número de casos para cortes</h3>
<p>Igual que en bosques aleatorios, podemos establecer mínimos de muestra en nodos
terminales, o mínimo de casos necesarios para hacer un corte.</p>
</div>
<div id="ejemplo-53" class="section level3 unnumbered">
<h3>Ejemplo</h3>
<pre class="sourceCode r"><code class="sourceCode r">modelos_dat &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">bag.fraction =</span> <span class="kw">c</span>( <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="dv">1</span>),
                          <span class="dt">shrinkage =</span> <span class="kw">c</span>(<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>),
                    <span class="dt">depth =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">12</span>)) <span class="op">%&gt;%</span><span class="st"> </span>expand.grid
modelos_dat &lt;-<span class="st"> </span>modelos_dat <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">modelo =</span> <span class="kw">pmap</span>(., boost)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">eval =</span> <span class="kw">map</span>(modelo, eval_modelo))
graf_eval &lt;-<span class="st"> </span>modelos_dat <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(shrinkage, bag.fraction, depth, eval) <span class="op">%&gt;%</span><span class="st"> </span>unnest
<span class="kw">ggplot</span>(<span class="kw">filter</span>(graf_eval, tipo <span class="op">==</span><span class="st">&#39;valida&#39;</span>), <span class="kw">aes</span>(<span class="dt">x =</span> n_arbol, <span class="dt">y=</span> valor,
    <span class="dt">colour=</span><span class="kw">factor</span>(bag.fraction), <span class="dt">group =</span> bag.fraction)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_grid</span>(depth<span class="op">~</span>shrinkage) <span class="op">+</span><span class="st"> </span><span class="kw">scale_y_log10</span>()</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-23-1.png" width="480" /></p>
<p>Podemos ver con más detalle donde ocurre el mejor desempeño:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(
    <span class="kw">filter</span>(graf_eval, tipo <span class="op">==</span><span class="st">&#39;valida&#39;</span>, shrinkage <span class="op">==</span><span class="st"> </span><span class="fl">0.1</span>, n_arbol <span class="op">&gt;</span><span class="st"> </span><span class="dv">100</span>), <span class="kw">aes</span>(<span class="dt">x =</span> n_arbol, <span class="dt">y=</span> valor, <span class="dt">colour=</span><span class="kw">factor</span>(bag.fraction), <span class="dt">group =</span>
                        bag.fraction)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_grid</span>(depth<span class="op">~</span>shrinkage) </code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-24-1.png" width="480" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">arrange</span>(<span class="kw">filter</span>(graf_eval,tipo<span class="op">==</span><span class="st">&#39;valida&#39;</span>), valor))</code></pre>
<pre><code>##   shrinkage bag.fraction depth n_arbol   tipo      valor
## 1       0.1          0.5    10     348 valida 0.08126603
## 2       0.1          0.5    10     342 valida 0.08128108
## 3       0.1          0.5    10     346 valida 0.08128677
## 4       0.1          0.5    10     343 valida 0.08129096
## 5       0.1          0.5    10     347 valida 0.08129531
## 6       0.1          0.5    10     341 valida 0.08129727</code></pre>
</div>
<div id="evaluacion-con-validacion-cruzada." class="section level3">
<h3><span class="header-section-number">13.8.6</span> Evaluación con validación cruzada.</h3>
<p>Para datos no muy grandes, conviene escoger modelos usando validación cruzada.</p>
<p>Por ejemplo,</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">9983</span>)
<span class="kw">rm</span>(<span class="st">&#39;modelos_dat&#39;</span>)
mod_boosting &lt;-<span class="st"> </span><span class="kw">gbm</span>(<span class="kw">log</span>(vSalePrice) <span class="op">~</span>.,  <span class="dt">data =</span> entrena,
                <span class="dt">distribution =</span> <span class="st">&#39;laplace&#39;</span>,
                <span class="dt">n.trees =</span> <span class="dv">200</span>, 
                <span class="dt">interaction.depth =</span> <span class="dv">10</span>,
                <span class="dt">shrinkage =</span> <span class="fl">0.1</span>, <span class="co"># tasa de aprendizaje</span>
                <span class="dt">bag.fraction =</span> <span class="fl">0.5</span>,
                <span class="dt">cv.folds =</span> <span class="dv">10</span>)
<span class="kw">gbm.perf</span>(mod_boosting)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">eval_modelo_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="cf">function</span>(modelo){
   dat_eval &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">entrena =</span> modelo<span class="op">$</span>train.error,
                          <span class="dt">valida =</span> modelo<span class="op">$</span>cv.error,
                          <span class="dt">n_arbol =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(modelo<span class="op">$</span>train.error)) <span class="op">%&gt;%</span>
<span class="st">                      </span><span class="kw">gather</span>(tipo, valor, <span class="op">-</span>n_arbol)
   dat_eval
}
dat &lt;-<span class="st"> </span><span class="kw">eval_modelo_2</span>(mod_boosting)
(<span class="kw">min</span>(mod_boosting<span class="op">$</span>cv.error))
<span class="kw">ggplot</span>(dat, <span class="kw">aes</span>(<span class="dt">x =</span> n_arbol, <span class="dt">y=</span>valor, <span class="dt">colour=</span>tipo, <span class="dt">group=</span>tipo)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()</code></pre>
</div>
</div>
<div id="graficas-de-dependencia-parcial" class="section level2">
<h2><span class="header-section-number">13.9</span> Gráficas de dependencia parcial</h2>
<p>La idea de dependencia parcial que veremos a continuación se puede aplicar a cualquier método de aprendizaje,
y en boosting ayuda a entender el funcionamiento del predictor complejo que resulta
del algoritmo. Aunque podemos evaluar el predictor en distintos valores y observar
cómo se comporta, cuando tenemos varias variables de entrada este proceso no
siempre tiene resultados muy claros o completos. Dependencia parcial es un intento
por entender de manera más sistemática parte del funcionamiento de
un modelo complejo.</p>
<div id="dependencia-parcial" class="section level3">
<h3><span class="header-section-number">13.9.1</span> Dependencia parcial</h3>
<p>Supongamos que tenemos un predictor <span class="math inline">\(f(x_1,x_2)\)</span> que depende de dos variables de
entrada. Podemos considerar la función
<span class="math display">\[{f}_{1}(x_1) = E_{x_2}[f(x_1,x_2)],\]</span>
que es el promedio de <span class="math inline">\(f(x)\)</span> fijando <span class="math inline">\(x_1\)</span> sobre la marginal de <span class="math inline">\(x_2\)</span>. Si tenemos
una muestra de entrenamiento, podríamos estimarla promediando sobre la muestra
de entrenamiento</p>
<p><span class="math display">\[\bar{f}_1(x_1) = \frac{1}{n}\sum_{i=1}^n f(x_1, x_2^{(i)}),\]</span>
que consiste en fijar el valor de <span class="math inline">\(x_1\)</span> y promediar sobre todos los valores
de la muestra de entrenamiento para <span class="math inline">\(x_2\)</span>.</p>
</div>
<div id="ejemplo-54" class="section level3 unnumbered">
<h3>Ejemplo</h3>
<p>Construimos un modelo con solamente tres variables para nuestro ejemplo anterior</p>
<pre class="sourceCode r"><code class="sourceCode r">mod_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">gbm</span>(<span class="kw">log</span>(vSalePrice) <span class="op">~</span><span class="st"> </span>vGrLivArea <span class="op">+</span>vNeighborhood  <span class="op">+</span>
<span class="st">                 </span>vOverallQual <span class="op">+</span><span class="st"> </span>vBsmtFinSF1,  
                <span class="dt">data =</span> entrena,
                <span class="dt">distribution =</span> <span class="st">&#39;laplace&#39;</span>,
                <span class="dt">n.trees =</span> <span class="dv">500</span>, 
                <span class="dt">interaction.depth =</span> <span class="dv">4</span>,
                <span class="dt">shrinkage =</span> <span class="fl">0.1</span>, 
                <span class="dt">bag.fraction =</span> <span class="fl">0.5</span>,
                <span class="dt">train.fraction =</span> <span class="fl">0.75</span>)</code></pre>
<p>Podemos calcular a mano la gráfica de dependencia parcial para
el tamaño de la “General Living Area”. Seleccionamos las variables:</p>
<pre class="sourceCode r"><code class="sourceCode r">dat_dp &lt;-<span class="st"> </span>entrena <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(vGrLivArea, vNeighborhood, vOverallQual, vBsmtFinSF1) </code></pre>
<p>Ahora consideramos el rango de la variable para establecer en dónde
vamos evaluar las función de dependiencia parcial:</p>
<pre class="sourceCode r"><code class="sourceCode r">cuantiles &lt;-<span class="st"> </span><span class="kw">quantile</span>(entrena<span class="op">$</span>vGrLivArea, <span class="dt">probs=</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.1</span>))
cuantiles</code></pre>
<pre><code>##     0%    10%    20%    30%    40%    50%    60%    70%    80%    90% 
##  334.0  912.0 1066.6 1208.0 1339.0 1464.0 1578.0 1709.3 1869.0 2158.3 
##   100% 
## 5642.0</code></pre>
<p>Por ejemplo, vamos evaluar el efecto parcial cuando vGrLivArea = 912. Hacemos</p>
<pre class="sourceCode r"><code class="sourceCode r">dat_dp_<span class="dv">1</span> &lt;-<span class="st"> </span>dat_dp <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">vGrLivArea =</span> <span class="dv">912</span>) <span class="op">%&gt;%</span>
<span class="st">            </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">predict</span>(mod_<span class="dv">2</span>, .)) <span class="op">%&gt;%</span>
<span class="st">            </span><span class="kw">summarise</span>(<span class="dt">mean_pred =</span> <span class="kw">mean</span>(pred))</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-29-1.png" width="480" /></p>
<pre><code>## Using 105 trees...</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">dat_dp_<span class="dv">1</span></code></pre>
<pre><code>##   mean_pred
## 1  11.84386</code></pre>
<p>Evaluamos en vGrLivArea = 1208</p>
<pre class="sourceCode r"><code class="sourceCode r">dat_dp_<span class="dv">1</span> &lt;-<span class="st"> </span>dat_dp <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">vGrLivArea =</span> <span class="dv">1208</span>) <span class="op">%&gt;%</span>
<span class="st">            </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">predict</span>(mod_<span class="dv">2</span>, .)) <span class="op">%&gt;%</span>
<span class="st">            </span><span class="kw">summarise</span>(<span class="dt">mean_pred =</span> <span class="kw">mean</span>(pred))</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-30-1.png" width="480" /></p>
<pre><code>## Using 105 trees...</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">dat_dp_<span class="dv">1</span></code></pre>
<pre><code>##   mean_pred
## 1  11.96169</code></pre>
<p>(un incremento de alrededor del 10% en el precio de venta).</p>
<p>Hacemos todos los percentiles como sigue:</p>
<pre class="sourceCode r"><code class="sourceCode r">cuantiles &lt;-<span class="st"> </span><span class="kw">quantile</span>(entrena<span class="op">$</span>vGrLivArea, <span class="dt">probs=</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.01</span>))

prom_parcial &lt;-<span class="st"> </span><span class="cf">function</span>(x, variable, df, mod){
  variable &lt;-<span class="st"> </span><span class="kw">enquo</span>(variable)
  variable_nom &lt;-<span class="st"> </span><span class="kw">quo_name</span>(variable)
  salida &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="op">!!</span>variable_nom <span class="op">:</span><span class="er">=</span><span class="st"> </span>x) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">predict</span>(mod, ., <span class="dt">n.trees =</span> <span class="dv">500</span>)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">group_by</span>(<span class="op">!!</span>variable) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">summarise</span>(<span class="dt">f_1 =</span> <span class="kw">mean</span>(pred)) 
  salida
}
dep_parcial &lt;-<span class="st"> </span><span class="kw">map_dfr</span>(cuantiles, 
                       <span class="op">~</span><span class="kw">prom_parcial</span>(.x, vGrLivArea, entrena, mod_<span class="dv">2</span>))
<span class="kw">ggplot</span>(dep_parcial, <span class="kw">aes</span>(<span class="dt">x=</span>vGrLivArea, <span class="dt">y =</span> f_<span class="dv">1</span>)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_rug</span>(<span class="dt">sides=</span><span class="st">&#39;b&#39;</span>)</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-31-1.png" width="480" />
Y transformando a las unidades originales</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(dep_parcial, <span class="kw">aes</span>(<span class="dt">x=</span>vGrLivArea, <span class="dt">y=</span> <span class="kw">exp</span>(f_<span class="dv">1</span>))) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_rug</span>(<span class="dt">sides=</span><span class="st">&#39;b&#39;</span>)</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-32-1.png" width="480" />
Y vemos que cuando aumenta el area de habitación, aumenta el precio. Podemos hacer esta gráfica más simple haciendo</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(mod_<span class="dv">2</span>, <span class="dv">1</span>) <span class="co"># 1 pues es vGrLivArea la primer variable </span></code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-33-1.png" width="480" /></p>
</div>
<div id="discusion-2" class="section level3 unnumbered">
<h3>Discusión</h3>
<p>En primer lugar, veamos qué obtenemos de la dependencia parcial
cuando aplicamos al modelo lineal sin interacciones. En el caso de dos variables,</p>
<p><span class="math display">\[f_1(x_1) = E_{x_2}[f(x_1,x_2)] =E_{x_2}[a + bx_1 + cx_2)] = \mu + bx_1,\]</span>
que es equivalente al análisis marginal que hacemos en regresión lineal (
incrementos en la variable <span class="math inline">\(x_1\)</span> con todo lo demás fijo, donde el incremento
marginal de la respuesta es el coeficiente <span class="math inline">\(b\)</span>).</p>
<p>Desde este punto de vista, dependencia parcial da una interpretación similar
a la del análisis usual de coeficientes en regresión lineal, donde pensamos
en “todo lo demás constante”.</p>
<p>Igualmente, si el modelo fuera aditivo de la forma
<span class="math inline">\(f(x_1,x_2) = h_1(x_1) + h_2(x_2)\)</span>
obtendríamos
<span class="math display">\[f_1(x_1) = E_{x_2}[h_1(x_1) + h_2(x_2)] = \mu + h_1(x_1),\]</span>
y recuperaríamos otra vez la interpetación de “todo lo demás constante”.</p>
<hr />
<p>Para una variable categórica las gráficas de dependencia
parcial se ven como sigue. Escribimos las
cantidades logarítmicas en la escala original:</p>
<pre class="sourceCode r"><code class="sourceCode r">dep_parcial &lt;-<span class="st"> </span><span class="kw">plot</span>(mod_<span class="dv">2</span>, <span class="dv">2</span>, <span class="dt">return.grid =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(y)
dep_parcial<span class="op">$</span>vNeighborhood &lt;-<span class="st"> </span><span class="kw">reorder</span>(dep_parcial<span class="op">$</span>vNeighborhood, dep_parcial<span class="op">$</span>y)
<span class="kw">ggplot</span>(dep_parcial, <span class="kw">aes</span>(<span class="dt">x =</span> vNeighborhood, <span class="dt">y =</span> <span class="kw">exp</span>(y))) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">coord_flip</span>()</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-34-1.png" width="480" /></p>
<hr />
<p>En general, si nuestro predictor depende de más variables
<span class="math inline">\(f(x_1,x_2, \ldots, x_p)\)</span>
entrada. Podemos considerar las funciones
<span class="math display">\[{f}_{j}(x_j) = E_{(x_1,x_2, \ldots x_p) - x_j}[f(x_1,x_2, \ldots, x_p)],\]</span>
que es el valor esperado de <span class="math inline">\(f(x)\)</span> fijando <span class="math inline">\(x_j\)</span>, y promediando sobre el resto
de las variables. Si tenemos
una muestra de entrenamiento, podríamos estimarla promediando sobre la muestra
de entrenamiento</p>
<p><span class="math display">\[\bar{f}_j(x_j) = \frac{1}{n}\sum_{i=1}^n f(x_1^{(i)}, x_2^{(i)}, \ldots, x_{j-1}^{(i)},\, x_j,\,  x_{j+1}^{(i)},\ldots, x_p^{(i)}).\]</span></p>
<p>Podemos hacer también gráficas de dependencia parcial para más de una variable,
si fijamos un subconjunto de variables y promediamos sobre el resto.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(mod_<span class="dv">2</span>, <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-35-1.png" width="480" /></p>
<p>Que también podemos graficar como</p>
<pre class="sourceCode r"><code class="sourceCode r">grid_dp &lt;-<span class="st"> </span><span class="kw">plot</span>(mod_<span class="dv">2</span>, <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>), <span class="dt">level.plot =</span> <span class="ot">FALSE</span>, <span class="dt">return.grid =</span> <span class="ot">TRUE</span>)
<span class="kw">ggplot</span>(grid_dp, <span class="kw">aes</span>(<span class="dt">x =</span> vGrLivArea, <span class="dt">y =</span> y, 
        <span class="dt">colour =</span> vOverallQual, <span class="dt">group =</span> vOverallQual)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">xlim</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">3000</span>))</code></pre>
<pre><code>## Warning: Removed 5000 rows containing missing values (geom_path).</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-36-1.png" width="480" /></p>
<p>En este caso, no vemos interacciones grandes (GrLivArea y OverallQual)
en nuestro modelo.</p>
<hr />
<div id="mas-de-interpretacion" class="section level4 unnumbered">
<h4>Más de interpretación</h4>
<p>Es importante evitar la interpretación incorrecta de que la función
de dependencia parcial da el valor esperado del predictor condicionado a valores
de la variable cuya dependencia examinamos. Es decir,
<span class="math display">\[f_1(x_1) = E_{x_2}(f(x_1,x_2)) \neq E(f(x_1,x_2)|x_1).\]</span>
La última cantidad es un valor esperado diferente (calculado sobre la
condicional de <span class="math inline">\(x_2\)</span> dada <span class="math inline">\(x_1\)</span>), de manera que utiliza información acerca
de la relación que hay entre <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span>, y se puede interpretar
como el valor esperado del predictor ingorando <span class="math inline">\(x_2\)</span>.
La función de dependencia parcial
da el efecto de <span class="math inline">\(x_1\)</span> tomando en cuenta los efectos <em>promedio</em> de las otras variables.</p>
</div>
<div id="ejemplos-2" class="section level4 unnumbered">
<h4>Ejemplos</h4>
<p>Considramos <span class="math inline">\(f(x_1,x_2) = h_1(x_1)h_2(x_2) = x_1x_2\)</span>, donde x_1 y x_2 tienen medias <span class="math inline">\(a_1\)</span> y <span class="math inline">\(a_2\)</span>.
La función de dependiencia parcial de <span class="math inline">\(x_1\)</span> es (demuéstralo):
<span class="math inline">\(\bar{f}_1(x_1) = a_2 x_1,\)</span>
que nos muestra el efecto de <span class="math inline">\(x_1\)</span> promediando sobre <span class="math inline">\(x_2\)</span>.
Sin embargo, la condicional de la predicción dada <span class="math inline">\(x_1\)</span> es diferente:
<span class="math display">\[f_1(x_1) = E(x_1x_2 | x_1) = x_1 E(x_2 | x_1)\]</span>
y el valor esperado condicional puede ser una función complicada. Por ejemplo,
si hay correlación lineal entre <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span> podríamos tener
<span class="math inline">\(E(x_2 | x_1) = ax_1 + b\)</span>, etc. Esta cantidad tiene sus usos (por ejemplo,
hacer predicciones cuando no tenemos <span class="math inline">\(x_2\)</span>), pero para entender el
efecto univariado de <span class="math inline">\(x_1\)</span> generalmente es más fácil considerar la
función de dependiencia parcial.</p>
<hr />
<p>Finalmente, nótese que cuando hay <strong>interacciones</strong> fuertes entre las variables, ningún análisis marginal (dependencia parcial o examen de coeficientes) da un resultado tan fácilmente interpretabl. La única solución es considerar el efecto conjunto de las variables que interactúan. De modo que este tipo de análisis funciona mejor
cuando no hay interacciones grandes entre las variables (es cercano a un modelo
aditivo con efectos no lineales).</p>
</div>
</div>
<div id="graficas-de-dependencia-parcial-para-otros-modelos" class="section level3">
<h3><span class="header-section-number">13.9.2</span> Gráficas de dependencia parcial para otros modelos</h3>
<p>Como dijimos en la introducción, las gráficas de dependiencia parcial
pueden utilizarse para cualquier tipo de modelo.</p>
<div id="ejemplo-regresion-lineal" class="section level4">
<h4><span class="header-section-number">13.9.2.1</span> Ejemplo: regresión lineal</h4>
<p>¿Qué esperamos si aplicamos a un modelo de regresión lineal?</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(pdp)</code></pre>
<pre><code>## 
## Attaching package: &#39;pdp&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     partial</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">mod_lm &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(vSalePrice) <span class="op">~</span><span class="st"> </span>vGrLivArea <span class="op">+</span>vNeighborhood  <span class="op">+</span>
<span class="st">                 </span>vOverallQual <span class="op">+</span><span class="st"> </span>vBsmtFinSF1, entrena)
mod_lm</code></pre>
<pre><code>## 
## Call:
## lm(formula = log(vSalePrice) ~ vGrLivArea + vNeighborhood + vOverallQual + 
##     vBsmtFinSF1, data = entrena)
## 
## Coefficients:
##          (Intercept)            vGrLivArea   vNeighborhoodBrDale  
##           10.9781318             0.0002519            -0.3876381  
## vNeighborhoodBrkSide  vNeighborhoodClearCr  vNeighborhoodCollgCr  
##           -0.1931159             0.0784516             0.0131924  
## vNeighborhoodCrawfor  vNeighborhoodEdwards  vNeighborhoodGilbert  
##            0.0180111            -0.2231209             0.0006351  
##  vNeighborhoodIDOTRR  vNeighborhoodMeadowV  vNeighborhoodMitchel  
##           -0.3716332            -0.3176807            -0.0713348  
##   vNeighborhoodNAmes  vNeighborhoodNoRidge  vNeighborhoodNridgHt  
##           -0.0981734             0.0806283             0.1502544  
##  vNeighborhoodNWAmes  vNeighborhoodOldTown    vNeighborhoodOtros  
##           -0.0528433            -0.2732835            -0.1615437  
##  vNeighborhoodSawyer  vNeighborhoodSawyerW  vNeighborhoodSomerst  
##           -0.0956738            -0.0621679             0.0521564  
## vNeighborhoodStoneBr    vNeighborhoodSWISU   vNeighborhoodTimber  
##            0.1231436            -0.2302935             0.0593881  
## vNeighborhoodVeenker          vOverallQual           vBsmtFinSF1  
##            0.1294244             0.1129118             0.0001089</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">mod_lm <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">partial</span>(<span class="dt">pred.var =</span> <span class="st">&quot;vGrLivArea&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">autoplot</span>(<span class="dt">rug =</span> <span class="ot">TRUE</span>, <span class="dt">train =</span> entrena)</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-37-1.png" width="480" /></p>
</div>
<div id="ejemplo-bosque-aleatorio" class="section level4">
<h4><span class="header-section-number">13.9.2.2</span> Ejemplo: bosque aleatorio</h4>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(randomForest)
mod_bosque &lt;-<span class="st"> </span><span class="kw">randomForest</span>(<span class="kw">log</span>(vSalePrice) <span class="op">~</span><span class="st"> </span>vGrLivArea <span class="op">+</span>vNeighborhood  <span class="op">+</span>
<span class="st">                 </span>vOverallQual <span class="op">+</span><span class="st"> </span>vBsmtFinSF1, <span class="dt">data =</span> entrena)
mod_bosque</code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = log(vSalePrice) ~ vGrLivArea + vNeighborhood +      vOverallQual + vBsmtFinSF1, data = entrena) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 1
## 
##           Mean of squared residuals: 0.02367802
##                     % Var explained: 85.15</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">mod_bosque <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">partial</span>(<span class="dt">pred.var =</span> <span class="st">&quot;vGrLivArea&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">autoplot</span>(<span class="dt">rug =</span> <span class="ot">TRUE</span>, <span class="dt">train =</span> entrena)</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-38-1.png" width="480" /></p>
<pre class="sourceCode r"><code class="sourceCode r">mod_bosque <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">partial</span>(<span class="dt">pred.var =</span> <span class="kw">c</span>(<span class="st">&quot;vGrLivArea&quot;</span>, <span class="st">&quot;vOverallQual&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">autoplot</span>(<span class="dt">rug =</span> <span class="ot">TRUE</span>, <span class="dt">train =</span> entrena)</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-39-1.png" width="480" /></p>
<p>Puedes ver más técnicas en <a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning</a>, por ejemplo.</p>
</div>
</div>
</div>
<div id="xgboost-y-gbm" class="section level2">
<h2><span class="header-section-number">13.10</span> xgboost y gbm</h2>
<p>Los paquetes <em>xgboost</em> y <em>gbm</em> parecen ser los más populares para hacer
gradient boosting. <em>xgboost</em>,
adicionalmente, parece ser más rápido y más flexible que <em>gbm</em> (paralelización, uso de GPU integrado). Existe una lista considerable de competencias de predicción donde el algoritmo/implementación
ganadora es <em>xgboost</em>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(xgboost)</code></pre>
<pre><code>## 
## Attaching package: &#39;xgboost&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     slice</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span>entrena <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>vSalePrice) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span>., .)
x_entrena &lt;-<span class="st"> </span>x[<span class="dv">1</span><span class="op">:</span><span class="dv">1100</span>, ]
x_valida &lt;-<span class="st"> </span>x[<span class="dv">1101</span><span class="op">:</span><span class="dv">1460</span>, ]
<span class="kw">set.seed</span>(<span class="dv">1293</span>)
d_entrena &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(x_entrena, <span class="dt">label =</span> <span class="kw">log</span>(entrena<span class="op">$</span>vSalePrice[<span class="dv">1</span><span class="op">:</span><span class="dv">1100</span>])) 
d_valida &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(x_valida, <span class="dt">label =</span> <span class="kw">log</span>(entrena<span class="op">$</span>vSalePrice[<span class="dv">1101</span><span class="op">:</span><span class="dv">1460</span>])) 
watchlist &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">eval =</span> d_valida, <span class="dt">train =</span> d_entrena)
params &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">booster =</span> <span class="st">&quot;gbtree&quot;</span>,
               <span class="dt">max_depth =</span> <span class="dv">3</span>, 
               <span class="dt">eta =</span> <span class="fl">0.03</span>, 
               <span class="dt">nthread =</span> <span class="dv">1</span>, 
               <span class="dt">subsample =</span> <span class="fl">0.75</span>, 
               <span class="dt">lambda =</span> <span class="fl">0.001</span>,
               <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>, 
               <span class="dt">eval_metric =</span> <span class="st">&quot;mae&quot;</span>) <span class="co"># error absoluto</span>
bst &lt;-<span class="st"> </span><span class="kw">xgb.train</span>(params, d_entrena, <span class="dt">nrounds =</span> <span class="dv">1000</span>, <span class="dt">watchlist =</span> watchlist, <span class="dt">verbose=</span><span class="dv">1</span>)</code></pre>
<pre><code>## [1]  eval-mae:11.176984  train-mae:11.178999 
## [2]  eval-mae:10.841721  train-mae:10.843556 
## [3]  eval-mae:10.516091  train-mae:10.518085 
## [4]  eval-mae:10.200583  train-mae:10.202491 
## [5]  eval-mae:9.894920   train-mae:9.896530 
## [6]  eval-mae:9.598317   train-mae:9.599773 
## [7]  eval-mae:9.310502   train-mae:9.311866 
## [8]  eval-mae:9.031566   train-mae:9.032389 
## [9]  eval-mae:8.760724   train-mae:8.761462 
## [10] eval-mae:8.498172   train-mae:8.498637 
## [11] eval-mae:8.243278   train-mae:8.243702 
## [12] eval-mae:7.996028   train-mae:7.996329 
## [13] eval-mae:7.756287   train-mae:7.756411 
## [14] eval-mae:7.523738   train-mae:7.523695 
## [15] eval-mae:7.298147   train-mae:7.298057 
## [16] eval-mae:7.079457   train-mae:7.079143 
## [17] eval-mae:6.867540   train-mae:6.866744 
## [18] eval-mae:6.661871   train-mae:6.660701 
## [19] eval-mae:6.462197   train-mae:6.460776 
## [20] eval-mae:6.268476   train-mae:6.266927 
## [21] eval-mae:6.080146   train-mae:6.078687 
## [22] eval-mae:5.897626   train-mae:5.896358 
## [23] eval-mae:5.720685   train-mae:5.719357 
## [24] eval-mae:5.549398   train-mae:5.547802 
## [25] eval-mae:5.382868   train-mae:5.381410 
## [26] eval-mae:5.221207   train-mae:5.219763 
## [27] eval-mae:5.064470   train-mae:5.063203 
## [28] eval-mae:4.912503   train-mae:4.911200 
## [29] eval-mae:4.764926   train-mae:4.763807 
## [30] eval-mae:4.622063   train-mae:4.620884 
## [31] eval-mae:4.483413   train-mae:4.482223 
## [32] eval-mae:4.348776   train-mae:4.347674 
## [33] eval-mae:4.218169   train-mae:4.217224 
## [34] eval-mae:4.091733   train-mae:4.090778 
## [35] eval-mae:3.968975   train-mae:3.968010 
## [36] eval-mae:3.849841   train-mae:3.848888 
## [37] eval-mae:3.734298   train-mae:3.733345 
## [38] eval-mae:3.622312   train-mae:3.621352 
## [39] eval-mae:3.513997   train-mae:3.512828 
## [40] eval-mae:3.408348   train-mae:3.407215 
## [41] eval-mae:3.306162   train-mae:3.304984 
## [42] eval-mae:3.206973   train-mae:3.205757 
## [43] eval-mae:3.110841   train-mae:3.109580 
## [44] eval-mae:3.017576   train-mae:3.016211 
## [45] eval-mae:2.926954   train-mae:2.925621 
## [46] eval-mae:2.839241   train-mae:2.837812 
## [47] eval-mae:2.754123   train-mae:2.752701 
## [48] eval-mae:2.671518   train-mae:2.670100 
## [49] eval-mae:2.591545   train-mae:2.590034 
## [50] eval-mae:2.513927   train-mae:2.512405 
## [51] eval-mae:2.438531   train-mae:2.437090 
## [52] eval-mae:2.365263   train-mae:2.363928 
## [53] eval-mae:2.294239   train-mae:2.292855 
## [54] eval-mae:2.225363   train-mae:2.224102 
## [55] eval-mae:2.158767   train-mae:2.157321 
## [56] eval-mae:2.093980   train-mae:2.092589 
## [57] eval-mae:2.031284   train-mae:2.029821 
## [58] eval-mae:1.970396   train-mae:1.969090 
## [59] eval-mae:1.911263   train-mae:1.910023 
## [60] eval-mae:1.854034   train-mae:1.852790 
## [61] eval-mae:1.798455   train-mae:1.797090 
## [62] eval-mae:1.744618   train-mae:1.743325 
## [63] eval-mae:1.692358   train-mae:1.691019 
## [64] eval-mae:1.641327   train-mae:1.640058 
## [65] eval-mae:1.592239   train-mae:1.590907 
## [66] eval-mae:1.544698   train-mae:1.543235 
## [67] eval-mae:1.498477   train-mae:1.496958 
## [68] eval-mae:1.453635   train-mae:1.452130 
## [69] eval-mae:1.409772   train-mae:1.408428 
## [70] eval-mae:1.367715   train-mae:1.366239 
## [71] eval-mae:1.326800   train-mae:1.325253 
## [72] eval-mae:1.287248   train-mae:1.285523 
## [73] eval-mae:1.248763   train-mae:1.246932 
## [74] eval-mae:1.211261   train-mae:1.209524 
## [75] eval-mae:1.174987   train-mae:1.173262 
## [76] eval-mae:1.139906   train-mae:1.138175 
## [77] eval-mae:1.105752   train-mae:1.104045 
## [78] eval-mae:1.072770   train-mae:1.070972 
## [79] eval-mae:1.040598   train-mae:1.038826 
## [80] eval-mae:1.009650   train-mae:1.007721 
## [81] eval-mae:0.979265   train-mae:0.977407 
## [82] eval-mae:0.950001   train-mae:0.948159 
## [83] eval-mae:0.921491   train-mae:0.919640 
## [84] eval-mae:0.894094   train-mae:0.892078 
## [85] eval-mae:0.867323   train-mae:0.865286 
## [86] eval-mae:0.841607   train-mae:0.839350 
## [87] eval-mae:0.816515   train-mae:0.814126 
## [88] eval-mae:0.792066   train-mae:0.789763 
## [89] eval-mae:0.768292   train-mae:0.766059 
## [90] eval-mae:0.745335   train-mae:0.743182 
## [91] eval-mae:0.723135   train-mae:0.720921 
## [92] eval-mae:0.701431   train-mae:0.699273 
## [93] eval-mae:0.680305   train-mae:0.678268 
## [94] eval-mae:0.659951   train-mae:0.657979 
## [95] eval-mae:0.640289   train-mae:0.638384 
## [96] eval-mae:0.621096   train-mae:0.619237 
## [97] eval-mae:0.602619   train-mae:0.600687 
## [98] eval-mae:0.584839   train-mae:0.582761 
## [99] eval-mae:0.567640   train-mae:0.565343 
## [100]    eval-mae:0.550959   train-mae:0.548542 
## [101]    eval-mae:0.534757   train-mae:0.532259 
## [102]    eval-mae:0.519005   train-mae:0.516405 
## [103]    eval-mae:0.503871   train-mae:0.501128 
## [104]    eval-mae:0.489255   train-mae:0.486071 
## [105]    eval-mae:0.475083   train-mae:0.471560 
## [106]    eval-mae:0.461362   train-mae:0.457412 
## [107]    eval-mae:0.448123   train-mae:0.443822 
## [108]    eval-mae:0.435431   train-mae:0.430689 
## [109]    eval-mae:0.422947   train-mae:0.418007 
## [110]    eval-mae:0.410675   train-mae:0.405594 
## [111]    eval-mae:0.399015   train-mae:0.393689 
## [112]    eval-mae:0.387629   train-mae:0.382120 
## [113]    eval-mae:0.376682   train-mae:0.370912 
## [114]    eval-mae:0.366002   train-mae:0.359868 
## [115]    eval-mae:0.355613   train-mae:0.349159 
## [116]    eval-mae:0.345714   train-mae:0.338928 
## [117]    eval-mae:0.336081   train-mae:0.328886 
## [118]    eval-mae:0.326819   train-mae:0.319230 
## [119]    eval-mae:0.317691   train-mae:0.309853 
## [120]    eval-mae:0.308935   train-mae:0.300856 
## [121]    eval-mae:0.300444   train-mae:0.292126 
## [122]    eval-mae:0.292296   train-mae:0.283533 
## [123]    eval-mae:0.284549   train-mae:0.275342 
## [124]    eval-mae:0.277110   train-mae:0.267480 
## [125]    eval-mae:0.269827   train-mae:0.259787 
## [126]    eval-mae:0.262780   train-mae:0.252439 
## [127]    eval-mae:0.255950   train-mae:0.245250 
## [128]    eval-mae:0.249369   train-mae:0.238362 
## [129]    eval-mae:0.242996   train-mae:0.231782 
## [130]    eval-mae:0.236763   train-mae:0.225267 
## [131]    eval-mae:0.230745   train-mae:0.219100 
## [132]    eval-mae:0.225008   train-mae:0.213131 
## [133]    eval-mae:0.219477   train-mae:0.207348 
## [134]    eval-mae:0.214161   train-mae:0.201821 
## [135]    eval-mae:0.208863   train-mae:0.196452 
## [136]    eval-mae:0.203891   train-mae:0.191283 
## [137]    eval-mae:0.198914   train-mae:0.186304 
## [138]    eval-mae:0.194195   train-mae:0.181375 
## [139]    eval-mae:0.189724   train-mae:0.176576 
## [140]    eval-mae:0.185307   train-mae:0.171958 
## [141]    eval-mae:0.181102   train-mae:0.167600 
## [142]    eval-mae:0.177018   train-mae:0.163403 
## [143]    eval-mae:0.173138   train-mae:0.159351 
## [144]    eval-mae:0.169522   train-mae:0.155450 
## [145]    eval-mae:0.165944   train-mae:0.151665 
## [146]    eval-mae:0.162522   train-mae:0.148044 
## [147]    eval-mae:0.159126   train-mae:0.144454 
## [148]    eval-mae:0.155916   train-mae:0.140983 
## [149]    eval-mae:0.152798   train-mae:0.137733 
## [150]    eval-mae:0.149890   train-mae:0.134611 
## [151]    eval-mae:0.147011   train-mae:0.131581 
## [152]    eval-mae:0.144343   train-mae:0.128727 
## [153]    eval-mae:0.141851   train-mae:0.126030 
## [154]    eval-mae:0.139432   train-mae:0.123388 
## [155]    eval-mae:0.137020   train-mae:0.120825 
## [156]    eval-mae:0.134766   train-mae:0.118328 
## [157]    eval-mae:0.132575   train-mae:0.115898 
## [158]    eval-mae:0.130664   train-mae:0.113702 
## [159]    eval-mae:0.128784   train-mae:0.111465 
## [160]    eval-mae:0.126796   train-mae:0.109349 
## [161]    eval-mae:0.125017   train-mae:0.107313 
## [162]    eval-mae:0.123313   train-mae:0.105365 
## [163]    eval-mae:0.121812   train-mae:0.103522 
## [164]    eval-mae:0.120360   train-mae:0.101748 
## [165]    eval-mae:0.118789   train-mae:0.100075 
## [166]    eval-mae:0.117342   train-mae:0.098434 
## [167]    eval-mae:0.116027   train-mae:0.096974 
## [168]    eval-mae:0.114808   train-mae:0.095542 
## [169]    eval-mae:0.113652   train-mae:0.094151 
## [170]    eval-mae:0.112540   train-mae:0.092748 
## [171]    eval-mae:0.111490   train-mae:0.091464 
## [172]    eval-mae:0.110391   train-mae:0.090240 
## [173]    eval-mae:0.109336   train-mae:0.089060 
## [174]    eval-mae:0.108291   train-mae:0.087905 
## [175]    eval-mae:0.107341   train-mae:0.086859 
## [176]    eval-mae:0.106423   train-mae:0.085829 
## [177]    eval-mae:0.105520   train-mae:0.084811 
## [178]    eval-mae:0.104731   train-mae:0.083838 
## [179]    eval-mae:0.104073   train-mae:0.083000 
## [180]    eval-mae:0.103307   train-mae:0.082091 
## [181]    eval-mae:0.102615   train-mae:0.081298 
## [182]    eval-mae:0.102002   train-mae:0.080498 
## [183]    eval-mae:0.101427   train-mae:0.079755 
## [184]    eval-mae:0.100969   train-mae:0.079049 
## [185]    eval-mae:0.100446   train-mae:0.078360 
## [186]    eval-mae:0.100050   train-mae:0.077707 
## [187]    eval-mae:0.099440   train-mae:0.077120 
## [188]    eval-mae:0.098967   train-mae:0.076555 
## [189]    eval-mae:0.098478   train-mae:0.075945 
## [190]    eval-mae:0.098064   train-mae:0.075435 
## [191]    eval-mae:0.097584   train-mae:0.074891 
## [192]    eval-mae:0.097268   train-mae:0.074412 
## [193]    eval-mae:0.096946   train-mae:0.073957 
## [194]    eval-mae:0.096569   train-mae:0.073517 
## [195]    eval-mae:0.096221   train-mae:0.073024 
## [196]    eval-mae:0.095884   train-mae:0.072631 
## [197]    eval-mae:0.095583   train-mae:0.072257 
## [198]    eval-mae:0.095326   train-mae:0.071892 
## [199]    eval-mae:0.095087   train-mae:0.071487 
## [200]    eval-mae:0.094727   train-mae:0.071093 
## [201]    eval-mae:0.094426   train-mae:0.070704 
## [202]    eval-mae:0.094213   train-mae:0.070354 
## [203]    eval-mae:0.093980   train-mae:0.070053 
## [204]    eval-mae:0.093653   train-mae:0.069705 
## [205]    eval-mae:0.093427   train-mae:0.069476 
## [206]    eval-mae:0.093285   train-mae:0.069242 
## [207]    eval-mae:0.093102   train-mae:0.068947 
## [208]    eval-mae:0.092898   train-mae:0.068708 
## [209]    eval-mae:0.092622   train-mae:0.068512 
## [210]    eval-mae:0.092443   train-mae:0.068279 
## [211]    eval-mae:0.092221   train-mae:0.068082 
## [212]    eval-mae:0.092098   train-mae:0.067902 
## [213]    eval-mae:0.091922   train-mae:0.067684 
## [214]    eval-mae:0.091705   train-mae:0.067478 
## [215]    eval-mae:0.091618   train-mae:0.067311 
## [216]    eval-mae:0.091529   train-mae:0.067056 
## [217]    eval-mae:0.091330   train-mae:0.066887 
## [218]    eval-mae:0.091146   train-mae:0.066725 
## [219]    eval-mae:0.091042   train-mae:0.066575 
## [220]    eval-mae:0.090911   train-mae:0.066396 
## [221]    eval-mae:0.090879   train-mae:0.066248 
## [222]    eval-mae:0.090784   train-mae:0.066083 
## [223]    eval-mae:0.090649   train-mae:0.065939 
## [224]    eval-mae:0.090566   train-mae:0.065818 
## [225]    eval-mae:0.090553   train-mae:0.065679 
## [226]    eval-mae:0.090387   train-mae:0.065510 
## [227]    eval-mae:0.090297   train-mae:0.065415 
## [228]    eval-mae:0.090133   train-mae:0.065294 
## [229]    eval-mae:0.090028   train-mae:0.065166 
## [230]    eval-mae:0.089975   train-mae:0.065034 
## [231]    eval-mae:0.089858   train-mae:0.064864 
## [232]    eval-mae:0.089758   train-mae:0.064732 
## [233]    eval-mae:0.089635   train-mae:0.064615 
## [234]    eval-mae:0.089514   train-mae:0.064502 
## [235]    eval-mae:0.089433   train-mae:0.064429 
## [236]    eval-mae:0.089307   train-mae:0.064289 
## [237]    eval-mae:0.089243   train-mae:0.064163 
## [238]    eval-mae:0.089158   train-mae:0.064053 
## [239]    eval-mae:0.089103   train-mae:0.063912 
## [240]    eval-mae:0.088984   train-mae:0.063769 
## [241]    eval-mae:0.088899   train-mae:0.063658 
## [242]    eval-mae:0.088844   train-mae:0.063538 
## [243]    eval-mae:0.088773   train-mae:0.063402 
## [244]    eval-mae:0.088694   train-mae:0.063309 
## [245]    eval-mae:0.088552   train-mae:0.063216 
## [246]    eval-mae:0.088483   train-mae:0.063079 
## [247]    eval-mae:0.088426   train-mae:0.062929 
## [248]    eval-mae:0.088428   train-mae:0.062880 
## [249]    eval-mae:0.088324   train-mae:0.062803 
## [250]    eval-mae:0.088255   train-mae:0.062691 
## [251]    eval-mae:0.088286   train-mae:0.062589 
## [252]    eval-mae:0.088173   train-mae:0.062481 
## [253]    eval-mae:0.088107   train-mae:0.062389 
## [254]    eval-mae:0.088114   train-mae:0.062245 
## [255]    eval-mae:0.087986   train-mae:0.062171 
## [256]    eval-mae:0.087940   train-mae:0.062072 
## [257]    eval-mae:0.087850   train-mae:0.061937 
## [258]    eval-mae:0.087858   train-mae:0.061872 
## [259]    eval-mae:0.087836   train-mae:0.061784 
## [260]    eval-mae:0.087886   train-mae:0.061682 
## [261]    eval-mae:0.087875   train-mae:0.061586 
## [262]    eval-mae:0.087792   train-mae:0.061518 
## [263]    eval-mae:0.087855   train-mae:0.061449 
## [264]    eval-mae:0.087852   train-mae:0.061385 
## [265]    eval-mae:0.087874   train-mae:0.061345 
## [266]    eval-mae:0.087845   train-mae:0.061241 
## [267]    eval-mae:0.087827   train-mae:0.061160 
## [268]    eval-mae:0.087786   train-mae:0.061073 
## [269]    eval-mae:0.087769   train-mae:0.060986 
## [270]    eval-mae:0.087661   train-mae:0.060906 
## [271]    eval-mae:0.087615   train-mae:0.060839 
## [272]    eval-mae:0.087578   train-mae:0.060745 
## [273]    eval-mae:0.087529   train-mae:0.060659 
## [274]    eval-mae:0.087554   train-mae:0.060569 
## [275]    eval-mae:0.087489   train-mae:0.060491 
## [276]    eval-mae:0.087411   train-mae:0.060398 
## [277]    eval-mae:0.087333   train-mae:0.060327 
## [278]    eval-mae:0.087272   train-mae:0.060290 
## [279]    eval-mae:0.087254   train-mae:0.060206 
## [280]    eval-mae:0.087197   train-mae:0.060154 
## [281]    eval-mae:0.087200   train-mae:0.060103 
## [282]    eval-mae:0.087172   train-mae:0.060053 
## [283]    eval-mae:0.087125   train-mae:0.059954 
## [284]    eval-mae:0.087026   train-mae:0.059857 
## [285]    eval-mae:0.086999   train-mae:0.059762 
## [286]    eval-mae:0.086915   train-mae:0.059643 
## [287]    eval-mae:0.086855   train-mae:0.059590 
## [288]    eval-mae:0.086838   train-mae:0.059497 
## [289]    eval-mae:0.086809   train-mae:0.059417 
## [290]    eval-mae:0.086732   train-mae:0.059345 
## [291]    eval-mae:0.086776   train-mae:0.059269 
## [292]    eval-mae:0.086760   train-mae:0.059201 
## [293]    eval-mae:0.086720   train-mae:0.059145 
## [294]    eval-mae:0.086673   train-mae:0.059100 
## [295]    eval-mae:0.086609   train-mae:0.059041 
## [296]    eval-mae:0.086595   train-mae:0.058986 
## [297]    eval-mae:0.086593   train-mae:0.058957 
## [298]    eval-mae:0.086586   train-mae:0.058895 
## [299]    eval-mae:0.086577   train-mae:0.058870 
## [300]    eval-mae:0.086516   train-mae:0.058809 
## [301]    eval-mae:0.086485   train-mae:0.058778 
## [302]    eval-mae:0.086479   train-mae:0.058720 
## [303]    eval-mae:0.086427   train-mae:0.058670 
## [304]    eval-mae:0.086438   train-mae:0.058585 
## [305]    eval-mae:0.086284   train-mae:0.058460 
## [306]    eval-mae:0.086287   train-mae:0.058395 
## [307]    eval-mae:0.086234   train-mae:0.058296 
## [308]    eval-mae:0.086151   train-mae:0.058170 
## [309]    eval-mae:0.086172   train-mae:0.058146 
## [310]    eval-mae:0.086082   train-mae:0.058028 
## [311]    eval-mae:0.086119   train-mae:0.057975 
## [312]    eval-mae:0.085976   train-mae:0.057914 
## [313]    eval-mae:0.086000   train-mae:0.057830 
## [314]    eval-mae:0.085938   train-mae:0.057790 
## [315]    eval-mae:0.085945   train-mae:0.057748 
## [316]    eval-mae:0.085901   train-mae:0.057673 
## [317]    eval-mae:0.085880   train-mae:0.057618 
## [318]    eval-mae:0.085865   train-mae:0.057565 
## [319]    eval-mae:0.085865   train-mae:0.057540 
## [320]    eval-mae:0.085835   train-mae:0.057481 
## [321]    eval-mae:0.085732   train-mae:0.057387 
## [322]    eval-mae:0.085633   train-mae:0.057302 
## [323]    eval-mae:0.085625   train-mae:0.057261 
## [324]    eval-mae:0.085565   train-mae:0.057225 
## [325]    eval-mae:0.085518   train-mae:0.057109 
## [326]    eval-mae:0.085518   train-mae:0.057035 
## [327]    eval-mae:0.085509   train-mae:0.056989 
## [328]    eval-mae:0.085450   train-mae:0.056925 
## [329]    eval-mae:0.085354   train-mae:0.056854 
## [330]    eval-mae:0.085405   train-mae:0.056818 
## [331]    eval-mae:0.085403   train-mae:0.056759 
## [332]    eval-mae:0.085380   train-mae:0.056720 
## [333]    eval-mae:0.085301   train-mae:0.056663 
## [334]    eval-mae:0.085302   train-mae:0.056629 
## [335]    eval-mae:0.085303   train-mae:0.056604 
## [336]    eval-mae:0.085306   train-mae:0.056546 
## [337]    eval-mae:0.085299   train-mae:0.056521 
## [338]    eval-mae:0.085303   train-mae:0.056471 
## [339]    eval-mae:0.085293   train-mae:0.056430 
## [340]    eval-mae:0.085250   train-mae:0.056383 
## [341]    eval-mae:0.085262   train-mae:0.056357 
## [342]    eval-mae:0.085198   train-mae:0.056260 
## [343]    eval-mae:0.085143   train-mae:0.056180 
## [344]    eval-mae:0.085119   train-mae:0.056119 
## [345]    eval-mae:0.085114   train-mae:0.056049 
## [346]    eval-mae:0.085076   train-mae:0.055955 
## [347]    eval-mae:0.085052   train-mae:0.055891 
## [348]    eval-mae:0.084995   train-mae:0.055784 
## [349]    eval-mae:0.084884   train-mae:0.055763 
## [350]    eval-mae:0.084881   train-mae:0.055723 
## [351]    eval-mae:0.084850   train-mae:0.055677 
## [352]    eval-mae:0.084835   train-mae:0.055634 
## [353]    eval-mae:0.084813   train-mae:0.055604 
## [354]    eval-mae:0.084814   train-mae:0.055518 
## [355]    eval-mae:0.084729   train-mae:0.055429 
## [356]    eval-mae:0.084664   train-mae:0.055366 
## [357]    eval-mae:0.084641   train-mae:0.055308 
## [358]    eval-mae:0.084652   train-mae:0.055266 
## [359]    eval-mae:0.084666   train-mae:0.055200 
## [360]    eval-mae:0.084570   train-mae:0.055146 
## [361]    eval-mae:0.084525   train-mae:0.055081 
## [362]    eval-mae:0.084480   train-mae:0.055013 
## [363]    eval-mae:0.084481   train-mae:0.054947 
## [364]    eval-mae:0.084424   train-mae:0.054909 
## [365]    eval-mae:0.084447   train-mae:0.054874 
## [366]    eval-mae:0.084462   train-mae:0.054830 
## [367]    eval-mae:0.084453   train-mae:0.054784 
## [368]    eval-mae:0.084497   train-mae:0.054713 
## [369]    eval-mae:0.084524   train-mae:0.054628 
## [370]    eval-mae:0.084564   train-mae:0.054578 
## [371]    eval-mae:0.084652   train-mae:0.054503 
## [372]    eval-mae:0.084584   train-mae:0.054418 
## [373]    eval-mae:0.084584   train-mae:0.054359 
## [374]    eval-mae:0.084543   train-mae:0.054309 
## [375]    eval-mae:0.084549   train-mae:0.054245 
## [376]    eval-mae:0.084527   train-mae:0.054191 
## [377]    eval-mae:0.084571   train-mae:0.054131 
## [378]    eval-mae:0.084512   train-mae:0.054093 
## [379]    eval-mae:0.084489   train-mae:0.054033 
## [380]    eval-mae:0.084500   train-mae:0.053985 
## [381]    eval-mae:0.084476   train-mae:0.053950 
## [382]    eval-mae:0.084487   train-mae:0.053913 
## [383]    eval-mae:0.084448   train-mae:0.053867 
## [384]    eval-mae:0.084456   train-mae:0.053785 
## [385]    eval-mae:0.084410   train-mae:0.053715 
## [386]    eval-mae:0.084386   train-mae:0.053668 
## [387]    eval-mae:0.084407   train-mae:0.053649 
## [388]    eval-mae:0.084424   train-mae:0.053626 
## [389]    eval-mae:0.084410   train-mae:0.053594 
## [390]    eval-mae:0.084467   train-mae:0.053542 
## [391]    eval-mae:0.084463   train-mae:0.053521 
## [392]    eval-mae:0.084370   train-mae:0.053448 
## [393]    eval-mae:0.084482   train-mae:0.053389 
## [394]    eval-mae:0.084497   train-mae:0.053355 
## [395]    eval-mae:0.084482   train-mae:0.053313 
## [396]    eval-mae:0.084498   train-mae:0.053266 
## [397]    eval-mae:0.084478   train-mae:0.053233 
## [398]    eval-mae:0.084425   train-mae:0.053193 
## [399]    eval-mae:0.084415   train-mae:0.053149 
## [400]    eval-mae:0.084407   train-mae:0.053104 
## [401]    eval-mae:0.084395   train-mae:0.053061 
## [402]    eval-mae:0.084426   train-mae:0.053023 
## [403]    eval-mae:0.084408   train-mae:0.052971 
## [404]    eval-mae:0.084358   train-mae:0.052871 
## [405]    eval-mae:0.084301   train-mae:0.052844 
## [406]    eval-mae:0.084331   train-mae:0.052780 
## [407]    eval-mae:0.084369   train-mae:0.052714 
## [408]    eval-mae:0.084420   train-mae:0.052650 
## [409]    eval-mae:0.084449   train-mae:0.052608 
## [410]    eval-mae:0.084383   train-mae:0.052540 
## [411]    eval-mae:0.084424   train-mae:0.052486 
## [412]    eval-mae:0.084369   train-mae:0.052419 
## [413]    eval-mae:0.084357   train-mae:0.052369 
## [414]    eval-mae:0.084351   train-mae:0.052343 
## [415]    eval-mae:0.084340   train-mae:0.052329 
## [416]    eval-mae:0.084311   train-mae:0.052281 
## [417]    eval-mae:0.084291   train-mae:0.052225 
## [418]    eval-mae:0.084283   train-mae:0.052165 
## [419]    eval-mae:0.084310   train-mae:0.052132 
## [420]    eval-mae:0.084325   train-mae:0.052103 
## [421]    eval-mae:0.084254   train-mae:0.052044 
## [422]    eval-mae:0.084209   train-mae:0.051979 
## [423]    eval-mae:0.084190   train-mae:0.051961 
## [424]    eval-mae:0.084179   train-mae:0.051901 
## [425]    eval-mae:0.084110   train-mae:0.051819 
## [426]    eval-mae:0.084115   train-mae:0.051767 
## [427]    eval-mae:0.084119   train-mae:0.051744 
## [428]    eval-mae:0.084201   train-mae:0.051716 
## [429]    eval-mae:0.084185   train-mae:0.051659 
## [430]    eval-mae:0.084138   train-mae:0.051643 
## [431]    eval-mae:0.084084   train-mae:0.051593 
## [432]    eval-mae:0.084070   train-mae:0.051544 
## [433]    eval-mae:0.084097   train-mae:0.051488 
## [434]    eval-mae:0.084022   train-mae:0.051436 
## [435]    eval-mae:0.084042   train-mae:0.051411 
## [436]    eval-mae:0.084057   train-mae:0.051396 
## [437]    eval-mae:0.084068   train-mae:0.051339 
## [438]    eval-mae:0.084071   train-mae:0.051288 
## [439]    eval-mae:0.084004   train-mae:0.051223 
## [440]    eval-mae:0.083965   train-mae:0.051140 
## [441]    eval-mae:0.083886   train-mae:0.051050 
## [442]    eval-mae:0.083863   train-mae:0.051024 
## [443]    eval-mae:0.083845   train-mae:0.050989 
## [444]    eval-mae:0.083819   train-mae:0.050962 
## [445]    eval-mae:0.083856   train-mae:0.050925 
## [446]    eval-mae:0.083814   train-mae:0.050879 
## [447]    eval-mae:0.083765   train-mae:0.050836 
## [448]    eval-mae:0.083769   train-mae:0.050787 
## [449]    eval-mae:0.083775   train-mae:0.050745 
## [450]    eval-mae:0.083750   train-mae:0.050700 
## [451]    eval-mae:0.083791   train-mae:0.050674 
## [452]    eval-mae:0.083776   train-mae:0.050637 
## [453]    eval-mae:0.083723   train-mae:0.050550 
## [454]    eval-mae:0.083803   train-mae:0.050513 
## [455]    eval-mae:0.083859   train-mae:0.050460 
## [456]    eval-mae:0.083814   train-mae:0.050408 
## [457]    eval-mae:0.083767   train-mae:0.050340 
## [458]    eval-mae:0.083771   train-mae:0.050287 
## [459]    eval-mae:0.083755   train-mae:0.050240 
## [460]    eval-mae:0.083750   train-mae:0.050189 
## [461]    eval-mae:0.083720   train-mae:0.050142 
## [462]    eval-mae:0.083717   train-mae:0.050106 
## [463]    eval-mae:0.083710   train-mae:0.050061 
## [464]    eval-mae:0.083683   train-mae:0.050015 
## [465]    eval-mae:0.083689   train-mae:0.049983 
## [466]    eval-mae:0.083666   train-mae:0.049909 
## [467]    eval-mae:0.083639   train-mae:0.049852 
## [468]    eval-mae:0.083617   train-mae:0.049820 
## [469]    eval-mae:0.083591   train-mae:0.049770 
## [470]    eval-mae:0.083577   train-mae:0.049716 
## [471]    eval-mae:0.083554   train-mae:0.049673 
## [472]    eval-mae:0.083583   train-mae:0.049596 
## [473]    eval-mae:0.083558   train-mae:0.049555 
## [474]    eval-mae:0.083495   train-mae:0.049509 
## [475]    eval-mae:0.083587   train-mae:0.049473 
## [476]    eval-mae:0.083601   train-mae:0.049437 
## [477]    eval-mae:0.083587   train-mae:0.049383 
## [478]    eval-mae:0.083559   train-mae:0.049347 
## [479]    eval-mae:0.083517   train-mae:0.049307 
## [480]    eval-mae:0.083496   train-mae:0.049250 
## [481]    eval-mae:0.083533   train-mae:0.049206 
## [482]    eval-mae:0.083537   train-mae:0.049162 
## [483]    eval-mae:0.083506   train-mae:0.049146 
## [484]    eval-mae:0.083505   train-mae:0.049081 
## [485]    eval-mae:0.083484   train-mae:0.049040 
## [486]    eval-mae:0.083516   train-mae:0.048998 
## [487]    eval-mae:0.083491   train-mae:0.048962 
## [488]    eval-mae:0.083445   train-mae:0.048926 
## [489]    eval-mae:0.083446   train-mae:0.048881 
## [490]    eval-mae:0.083456   train-mae:0.048840 
## [491]    eval-mae:0.083439   train-mae:0.048826 
## [492]    eval-mae:0.083419   train-mae:0.048768 
## [493]    eval-mae:0.083413   train-mae:0.048739 
## [494]    eval-mae:0.083410   train-mae:0.048667 
## [495]    eval-mae:0.083395   train-mae:0.048628 
## [496]    eval-mae:0.083412   train-mae:0.048589 
## [497]    eval-mae:0.083414   train-mae:0.048558 
## [498]    eval-mae:0.083424   train-mae:0.048521 
## [499]    eval-mae:0.083406   train-mae:0.048495 
## [500]    eval-mae:0.083423   train-mae:0.048462 
## [501]    eval-mae:0.083402   train-mae:0.048429 
## [502]    eval-mae:0.083383   train-mae:0.048368 
## [503]    eval-mae:0.083338   train-mae:0.048326 
## [504]    eval-mae:0.083347   train-mae:0.048276 
## [505]    eval-mae:0.083347   train-mae:0.048219 
## [506]    eval-mae:0.083345   train-mae:0.048153 
## [507]    eval-mae:0.083294   train-mae:0.048102 
## [508]    eval-mae:0.083321   train-mae:0.048060 
## [509]    eval-mae:0.083434   train-mae:0.048037 
## [510]    eval-mae:0.083403   train-mae:0.047984 
## [511]    eval-mae:0.083413   train-mae:0.047944 
## [512]    eval-mae:0.083429   train-mae:0.047920 
## [513]    eval-mae:0.083422   train-mae:0.047848 
## [514]    eval-mae:0.083424   train-mae:0.047797 
## [515]    eval-mae:0.083417   train-mae:0.047785 
## [516]    eval-mae:0.083381   train-mae:0.047721 
## [517]    eval-mae:0.083367   train-mae:0.047657 
## [518]    eval-mae:0.083448   train-mae:0.047613 
## [519]    eval-mae:0.083447   train-mae:0.047596 
## [520]    eval-mae:0.083448   train-mae:0.047582 
## [521]    eval-mae:0.083486   train-mae:0.047549 
## [522]    eval-mae:0.083497   train-mae:0.047541 
## [523]    eval-mae:0.083452   train-mae:0.047492 
## [524]    eval-mae:0.083480   train-mae:0.047451 
## [525]    eval-mae:0.083466   train-mae:0.047403 
## [526]    eval-mae:0.083412   train-mae:0.047376 
## [527]    eval-mae:0.083386   train-mae:0.047350 
## [528]    eval-mae:0.083379   train-mae:0.047307 
## [529]    eval-mae:0.083400   train-mae:0.047270 
## [530]    eval-mae:0.083399   train-mae:0.047251 
## [531]    eval-mae:0.083375   train-mae:0.047216 
## [532]    eval-mae:0.083389   train-mae:0.047145 
## [533]    eval-mae:0.083398   train-mae:0.047097 
## [534]    eval-mae:0.083554   train-mae:0.047045 
## [535]    eval-mae:0.083531   train-mae:0.046996 
## [536]    eval-mae:0.083492   train-mae:0.046947 
## [537]    eval-mae:0.083512   train-mae:0.046911 
## [538]    eval-mae:0.083471   train-mae:0.046876 
## [539]    eval-mae:0.083465   train-mae:0.046823 
## [540]    eval-mae:0.083473   train-mae:0.046812 
## [541]    eval-mae:0.083713   train-mae:0.046781 
## [542]    eval-mae:0.083704   train-mae:0.046763 
## [543]    eval-mae:0.083721   train-mae:0.046713 
## [544]    eval-mae:0.083725   train-mae:0.046696 
## [545]    eval-mae:0.083722   train-mae:0.046662 
## [546]    eval-mae:0.083720   train-mae:0.046620 
## [547]    eval-mae:0.083701   train-mae:0.046592 
## [548]    eval-mae:0.083718   train-mae:0.046556 
## [549]    eval-mae:0.083690   train-mae:0.046525 
## [550]    eval-mae:0.083672   train-mae:0.046515 
## [551]    eval-mae:0.083683   train-mae:0.046481 
## [552]    eval-mae:0.083655   train-mae:0.046449 
## [553]    eval-mae:0.083640   train-mae:0.046404 
## [554]    eval-mae:0.083666   train-mae:0.046354 
## [555]    eval-mae:0.083599   train-mae:0.046326 
## [556]    eval-mae:0.083763   train-mae:0.046306 
## [557]    eval-mae:0.083761   train-mae:0.046247 
## [558]    eval-mae:0.083770   train-mae:0.046206 
## [559]    eval-mae:0.083779   train-mae:0.046170 
## [560]    eval-mae:0.083762   train-mae:0.046133 
## [561]    eval-mae:0.083762   train-mae:0.046083 
## [562]    eval-mae:0.083710   train-mae:0.046065 
## [563]    eval-mae:0.083731   train-mae:0.046022 
## [564]    eval-mae:0.083765   train-mae:0.045998 
## [565]    eval-mae:0.083778   train-mae:0.045987 
## [566]    eval-mae:0.083797   train-mae:0.045909 
## [567]    eval-mae:0.083871   train-mae:0.045876 
## [568]    eval-mae:0.083859   train-mae:0.045829 
## [569]    eval-mae:0.083843   train-mae:0.045788 
## [570]    eval-mae:0.083819   train-mae:0.045774 
## [571]    eval-mae:0.083869   train-mae:0.045727 
## [572]    eval-mae:0.083920   train-mae:0.045674 
## [573]    eval-mae:0.083919   train-mae:0.045650 
## [574]    eval-mae:0.083938   train-mae:0.045619 
## [575]    eval-mae:0.083917   train-mae:0.045581 
## [576]    eval-mae:0.083931   train-mae:0.045545 
## [577]    eval-mae:0.083879   train-mae:0.045508 
## [578]    eval-mae:0.083853   train-mae:0.045480 
## [579]    eval-mae:0.083834   train-mae:0.045453 
## [580]    eval-mae:0.083861   train-mae:0.045425 
## [581]    eval-mae:0.083882   train-mae:0.045386 
## [582]    eval-mae:0.083881   train-mae:0.045353 
## [583]    eval-mae:0.083884   train-mae:0.045307 
## [584]    eval-mae:0.083881   train-mae:0.045272 
## [585]    eval-mae:0.083879   train-mae:0.045234 
## [586]    eval-mae:0.083839   train-mae:0.045194 
## [587]    eval-mae:0.083812   train-mae:0.045156 
## [588]    eval-mae:0.083850   train-mae:0.045141 
## [589]    eval-mae:0.083856   train-mae:0.045110 
## [590]    eval-mae:0.083834   train-mae:0.045063 
## [591]    eval-mae:0.083821   train-mae:0.045037 
## [592]    eval-mae:0.083797   train-mae:0.044993 
## [593]    eval-mae:0.083789   train-mae:0.044946 
## [594]    eval-mae:0.083779   train-mae:0.044935 
## [595]    eval-mae:0.083772   train-mae:0.044895 
## [596]    eval-mae:0.083797   train-mae:0.044883 
## [597]    eval-mae:0.083818   train-mae:0.044855 
## [598]    eval-mae:0.083830   train-mae:0.044829 
## [599]    eval-mae:0.083813   train-mae:0.044787 
## [600]    eval-mae:0.083772   train-mae:0.044749 
## [601]    eval-mae:0.083758   train-mae:0.044721 
## [602]    eval-mae:0.083731   train-mae:0.044681 
## [603]    eval-mae:0.083759   train-mae:0.044630 
## [604]    eval-mae:0.083749   train-mae:0.044590 
## [605]    eval-mae:0.083722   train-mae:0.044564 
## [606]    eval-mae:0.083730   train-mae:0.044516 
## [607]    eval-mae:0.083769   train-mae:0.044474 
## [608]    eval-mae:0.083765   train-mae:0.044426 
## [609]    eval-mae:0.083755   train-mae:0.044395 
## [610]    eval-mae:0.083184   train-mae:0.044377 
## [611]    eval-mae:0.083125   train-mae:0.044355 
## [612]    eval-mae:0.083128   train-mae:0.044330 
## [613]    eval-mae:0.083133   train-mae:0.044309 
## [614]    eval-mae:0.083118   train-mae:0.044274 
## [615]    eval-mae:0.083122   train-mae:0.044259 
## [616]    eval-mae:0.083136   train-mae:0.044192 
## [617]    eval-mae:0.083122   train-mae:0.044152 
## [618]    eval-mae:0.083146   train-mae:0.044102 
## [619]    eval-mae:0.083129   train-mae:0.044062 
## [620]    eval-mae:0.083112   train-mae:0.044038 
## [621]    eval-mae:0.083128   train-mae:0.043983 
## [622]    eval-mae:0.083160   train-mae:0.043955 
## [623]    eval-mae:0.083160   train-mae:0.043931 
## [624]    eval-mae:0.083141   train-mae:0.043882 
## [625]    eval-mae:0.083063   train-mae:0.043845 
## [626]    eval-mae:0.083058   train-mae:0.043815 
## [627]    eval-mae:0.083066   train-mae:0.043773 
## [628]    eval-mae:0.082594   train-mae:0.043764 
## [629]    eval-mae:0.082608   train-mae:0.043735 
## [630]    eval-mae:0.082599   train-mae:0.043704 
## [631]    eval-mae:0.082554   train-mae:0.043646 
## [632]    eval-mae:0.082559   train-mae:0.043611 
## [633]    eval-mae:0.082571   train-mae:0.043567 
## [634]    eval-mae:0.082574   train-mae:0.043515 
## [635]    eval-mae:0.082560   train-mae:0.043513 
## [636]    eval-mae:0.082534   train-mae:0.043489 
## [637]    eval-mae:0.082546   train-mae:0.043475 
## [638]    eval-mae:0.082504   train-mae:0.043430 
## [639]    eval-mae:0.082500   train-mae:0.043392 
## [640]    eval-mae:0.082509   train-mae:0.043369 
## [641]    eval-mae:0.082480   train-mae:0.043328 
## [642]    eval-mae:0.082465   train-mae:0.043285 
## [643]    eval-mae:0.082479   train-mae:0.043252 
## [644]    eval-mae:0.082498   train-mae:0.043217 
## [645]    eval-mae:0.082482   train-mae:0.043201 
## [646]    eval-mae:0.082426   train-mae:0.043154 
## [647]    eval-mae:0.082397   train-mae:0.043114 
## [648]    eval-mae:0.082370   train-mae:0.043101 
## [649]    eval-mae:0.082360   train-mae:0.043095 
## [650]    eval-mae:0.082347   train-mae:0.043057 
## [651]    eval-mae:0.082353   train-mae:0.043026 
## [652]    eval-mae:0.082386   train-mae:0.042987 
## [653]    eval-mae:0.082540   train-mae:0.042955 
## [654]    eval-mae:0.082546   train-mae:0.042921 
## [655]    eval-mae:0.082538   train-mae:0.042910 
## [656]    eval-mae:0.082519   train-mae:0.042877 
## [657]    eval-mae:0.082486   train-mae:0.042846 
## [658]    eval-mae:0.082517   train-mae:0.042802 
## [659]    eval-mae:0.082529   train-mae:0.042793 
## [660]    eval-mae:0.082736   train-mae:0.042778 
## [661]    eval-mae:0.082703   train-mae:0.042731 
## [662]    eval-mae:0.082692   train-mae:0.042679 
## [663]    eval-mae:0.082664   train-mae:0.042631 
## [664]    eval-mae:0.082711   train-mae:0.042607 
## [665]    eval-mae:0.082700   train-mae:0.042594 
## [666]    eval-mae:0.082730   train-mae:0.042547 
## [667]    eval-mae:0.082709   train-mae:0.042515 
## [668]    eval-mae:0.082710   train-mae:0.042496 
## [669]    eval-mae:0.082713   train-mae:0.042460 
## [670]    eval-mae:0.082732   train-mae:0.042417 
## [671]    eval-mae:0.082707   train-mae:0.042398 
## [672]    eval-mae:0.082714   train-mae:0.042370 
## [673]    eval-mae:0.082732   train-mae:0.042333 
## [674]    eval-mae:0.082714   train-mae:0.042310 
## [675]    eval-mae:0.082727   train-mae:0.042288 
## [676]    eval-mae:0.082659   train-mae:0.042249 
## [677]    eval-mae:0.082657   train-mae:0.042231 
## [678]    eval-mae:0.082255   train-mae:0.042229 
## [679]    eval-mae:0.082245   train-mae:0.042204 
## [680]    eval-mae:0.082245   train-mae:0.042195 
## [681]    eval-mae:0.082193   train-mae:0.042151 
## [682]    eval-mae:0.082198   train-mae:0.042120 
## [683]    eval-mae:0.082191   train-mae:0.042080 
## [684]    eval-mae:0.082182   train-mae:0.042057 
## [685]    eval-mae:0.082129   train-mae:0.042027 
## [686]    eval-mae:0.082126   train-mae:0.042013 
## [687]    eval-mae:0.082111   train-mae:0.041975 
## [688]    eval-mae:0.082122   train-mae:0.041936 
## [689]    eval-mae:0.082117   train-mae:0.041894 
## [690]    eval-mae:0.082102   train-mae:0.041851 
## [691]    eval-mae:0.082152   train-mae:0.041832 
## [692]    eval-mae:0.082135   train-mae:0.041814 
## [693]    eval-mae:0.082112   train-mae:0.041762 
## [694]    eval-mae:0.082119   train-mae:0.041745 
## [695]    eval-mae:0.082079   train-mae:0.041697 
## [696]    eval-mae:0.082064   train-mae:0.041669 
## [697]    eval-mae:0.082041   train-mae:0.041626 
## [698]    eval-mae:0.082023   train-mae:0.041618 
## [699]    eval-mae:0.081983   train-mae:0.041580 
## [700]    eval-mae:0.081974   train-mae:0.041536 
## [701]    eval-mae:0.082011   train-mae:0.041498 
## [702]    eval-mae:0.081995   train-mae:0.041478 
## [703]    eval-mae:0.082008   train-mae:0.041446 
## [704]    eval-mae:0.082004   train-mae:0.041407 
## [705]    eval-mae:0.081988   train-mae:0.041385 
## [706]    eval-mae:0.082060   train-mae:0.041361 
## [707]    eval-mae:0.082040   train-mae:0.041329 
## [708]    eval-mae:0.082032   train-mae:0.041314 
## [709]    eval-mae:0.082054   train-mae:0.041304 
## [710]    eval-mae:0.082058   train-mae:0.041294 
## [711]    eval-mae:0.082060   train-mae:0.041238 
## [712]    eval-mae:0.082092   train-mae:0.041209 
## [713]    eval-mae:0.082079   train-mae:0.041165 
## [714]    eval-mae:0.082047   train-mae:0.041141 
## [715]    eval-mae:0.082048   train-mae:0.041120 
## [716]    eval-mae:0.082037   train-mae:0.041067 
## [717]    eval-mae:0.082029   train-mae:0.041033 
## [718]    eval-mae:0.082039   train-mae:0.041027 
## [719]    eval-mae:0.082013   train-mae:0.040986 
## [720]    eval-mae:0.082002   train-mae:0.040944 
## [721]    eval-mae:0.081925   train-mae:0.040908 
## [722]    eval-mae:0.081925   train-mae:0.040875 
## [723]    eval-mae:0.081958   train-mae:0.040859 
## [724]    eval-mae:0.081934   train-mae:0.040849 
## [725]    eval-mae:0.081981   train-mae:0.040810 
## [726]    eval-mae:0.081994   train-mae:0.040791 
## [727]    eval-mae:0.082016   train-mae:0.040765 
## [728]    eval-mae:0.082013   train-mae:0.040733 
## [729]    eval-mae:0.081985   train-mae:0.040709 
## [730]    eval-mae:0.081995   train-mae:0.040694 
## [731]    eval-mae:0.081950   train-mae:0.040663 
## [732]    eval-mae:0.081951   train-mae:0.040627 
## [733]    eval-mae:0.081946   train-mae:0.040607 
## [734]    eval-mae:0.081970   train-mae:0.040578 
## [735]    eval-mae:0.081972   train-mae:0.040551 
## [736]    eval-mae:0.081967   train-mae:0.040527 
## [737]    eval-mae:0.081953   train-mae:0.040493 
## [738]    eval-mae:0.081934   train-mae:0.040466 
## [739]    eval-mae:0.081940   train-mae:0.040443 
## [740]    eval-mae:0.081721   train-mae:0.040430 
## [741]    eval-mae:0.081709   train-mae:0.040379 
## [742]    eval-mae:0.081655   train-mae:0.040355 
## [743]    eval-mae:0.081632   train-mae:0.040331 
## [744]    eval-mae:0.081617   train-mae:0.040299 
## [745]    eval-mae:0.081620   train-mae:0.040286 
## [746]    eval-mae:0.081607   train-mae:0.040255 
## [747]    eval-mae:0.081590   train-mae:0.040207 
## [748]    eval-mae:0.081606   train-mae:0.040176 
## [749]    eval-mae:0.081599   train-mae:0.040151 
## [750]    eval-mae:0.081551   train-mae:0.040120 
## [751]    eval-mae:0.081543   train-mae:0.040106 
## [752]    eval-mae:0.081548   train-mae:0.040084 
## [753]    eval-mae:0.081553   train-mae:0.040057 
## [754]    eval-mae:0.081546   train-mae:0.040025 
## [755]    eval-mae:0.081541   train-mae:0.040000 
## [756]    eval-mae:0.081525   train-mae:0.039966 
## [757]    eval-mae:0.081550   train-mae:0.039926 
## [758]    eval-mae:0.081532   train-mae:0.039901 
## [759]    eval-mae:0.081501   train-mae:0.039869 
## [760]    eval-mae:0.081517   train-mae:0.039809 
## [761]    eval-mae:0.081492   train-mae:0.039765 
## [762]    eval-mae:0.081460   train-mae:0.039740 
## [763]    eval-mae:0.081411   train-mae:0.039709 
## [764]    eval-mae:0.081407   train-mae:0.039686 
## [765]    eval-mae:0.081346   train-mae:0.039653 
## [766]    eval-mae:0.081360   train-mae:0.039632 
## [767]    eval-mae:0.081332   train-mae:0.039603 
## [768]    eval-mae:0.081325   train-mae:0.039587 
## [769]    eval-mae:0.081310   train-mae:0.039555 
## [770]    eval-mae:0.081299   train-mae:0.039524 
## [771]    eval-mae:0.081338   train-mae:0.039502 
## [772]    eval-mae:0.081322   train-mae:0.039479 
## [773]    eval-mae:0.081322   train-mae:0.039471 
## [774]    eval-mae:0.081343   train-mae:0.039420 
## [775]    eval-mae:0.081323   train-mae:0.039392 
## [776]    eval-mae:0.081451   train-mae:0.039358 
## [777]    eval-mae:0.081454   train-mae:0.039340 
## [778]    eval-mae:0.081474   train-mae:0.039315 
## [779]    eval-mae:0.081465   train-mae:0.039286 
## [780]    eval-mae:0.081457   train-mae:0.039251 
## [781]    eval-mae:0.081455   train-mae:0.039237 
## [782]    eval-mae:0.081433   train-mae:0.039222 
## [783]    eval-mae:0.081434   train-mae:0.039198 
## [784]    eval-mae:0.081321   train-mae:0.039180 
## [785]    eval-mae:0.081348   train-mae:0.039128 
## [786]    eval-mae:0.081348   train-mae:0.039094 
## [787]    eval-mae:0.081357   train-mae:0.039076 
## [788]    eval-mae:0.081360   train-mae:0.039037 
## [789]    eval-mae:0.081355   train-mae:0.039030 
## [790]    eval-mae:0.081338   train-mae:0.038997 
## [791]    eval-mae:0.081296   train-mae:0.038952 
## [792]    eval-mae:0.081285   train-mae:0.038913 
## [793]    eval-mae:0.081278   train-mae:0.038890 
## [794]    eval-mae:0.081243   train-mae:0.038857 
## [795]    eval-mae:0.081208   train-mae:0.038838 
## [796]    eval-mae:0.081219   train-mae:0.038809 
## [797]    eval-mae:0.081215   train-mae:0.038795 
## [798]    eval-mae:0.081219   train-mae:0.038766 
## [799]    eval-mae:0.081235   train-mae:0.038741 
## [800]    eval-mae:0.081238   train-mae:0.038708 
## [801]    eval-mae:0.081194   train-mae:0.038675 
## [802]    eval-mae:0.081198   train-mae:0.038649 
## [803]    eval-mae:0.081176   train-mae:0.038638 
## [804]    eval-mae:0.081195   train-mae:0.038616 
## [805]    eval-mae:0.081199   train-mae:0.038605 
## [806]    eval-mae:0.081239   train-mae:0.038563 
## [807]    eval-mae:0.081191   train-mae:0.038522 
## [808]    eval-mae:0.081174   train-mae:0.038497 
## [809]    eval-mae:0.081168   train-mae:0.038456 
## [810]    eval-mae:0.081173   train-mae:0.038423 
## [811]    eval-mae:0.081171   train-mae:0.038400 
## [812]    eval-mae:0.081180   train-mae:0.038372 
## [813]    eval-mae:0.081186   train-mae:0.038345 
## [814]    eval-mae:0.081191   train-mae:0.038308 
## [815]    eval-mae:0.081176   train-mae:0.038267 
## [816]    eval-mae:0.081134   train-mae:0.038246 
## [817]    eval-mae:0.081134   train-mae:0.038245 
## [818]    eval-mae:0.081116   train-mae:0.038208 
## [819]    eval-mae:0.081122   train-mae:0.038193 
## [820]    eval-mae:0.081122   train-mae:0.038181 
## [821]    eval-mae:0.081108   train-mae:0.038160 
## [822]    eval-mae:0.081066   train-mae:0.038127 
## [823]    eval-mae:0.081006   train-mae:0.038104 
## [824]    eval-mae:0.080996   train-mae:0.038099 
## [825]    eval-mae:0.080992   train-mae:0.038055 
## [826]    eval-mae:0.081000   train-mae:0.038045 
## [827]    eval-mae:0.081026   train-mae:0.038005 
## [828]    eval-mae:0.081042   train-mae:0.037976 
## [829]    eval-mae:0.081048   train-mae:0.037961 
## [830]    eval-mae:0.081036   train-mae:0.037942 
## [831]    eval-mae:0.081047   train-mae:0.037905 
## [832]    eval-mae:0.081007   train-mae:0.037888 
## [833]    eval-mae:0.081037   train-mae:0.037862 
## [834]    eval-mae:0.081004   train-mae:0.037845 
## [835]    eval-mae:0.081014   train-mae:0.037795 
## [836]    eval-mae:0.080995   train-mae:0.037767 
## [837]    eval-mae:0.080989   train-mae:0.037734 
## [838]    eval-mae:0.080951   train-mae:0.037716 
## [839]    eval-mae:0.080926   train-mae:0.037657 
## [840]    eval-mae:0.080914   train-mae:0.037630 
## [841]    eval-mae:0.080921   train-mae:0.037595 
## [842]    eval-mae:0.080914   train-mae:0.037580 
## [843]    eval-mae:0.080885   train-mae:0.037561 
## [844]    eval-mae:0.080898   train-mae:0.037552 
## [845]    eval-mae:0.080906   train-mae:0.037534 
## [846]    eval-mae:0.080894   train-mae:0.037504 
## [847]    eval-mae:0.080900   train-mae:0.037472 
## [848]    eval-mae:0.080901   train-mae:0.037456 
## [849]    eval-mae:0.080900   train-mae:0.037432 
## [850]    eval-mae:0.080889   train-mae:0.037425 
## [851]    eval-mae:0.080876   train-mae:0.037384 
## [852]    eval-mae:0.080868   train-mae:0.037361 
## [853]    eval-mae:0.080866   train-mae:0.037343 
## [854]    eval-mae:0.080818   train-mae:0.037305 
## [855]    eval-mae:0.080843   train-mae:0.037285 
## [856]    eval-mae:0.080827   train-mae:0.037247 
## [857]    eval-mae:0.080838   train-mae:0.037215 
## [858]    eval-mae:0.080844   train-mae:0.037182 
## [859]    eval-mae:0.080832   train-mae:0.037158 
## [860]    eval-mae:0.080888   train-mae:0.037131 
## [861]    eval-mae:0.080927   train-mae:0.037107 
## [862]    eval-mae:0.080920   train-mae:0.037061 
## [863]    eval-mae:0.080911   train-mae:0.037031 
## [864]    eval-mae:0.080905   train-mae:0.036995 
## [865]    eval-mae:0.080919   train-mae:0.036978 
## [866]    eval-mae:0.080915   train-mae:0.036956 
## [867]    eval-mae:0.080922   train-mae:0.036923 
## [868]    eval-mae:0.080952   train-mae:0.036889 
## [869]    eval-mae:0.080953   train-mae:0.036857 
## [870]    eval-mae:0.080890   train-mae:0.036809 
## [871]    eval-mae:0.080880   train-mae:0.036796 
## [872]    eval-mae:0.080865   train-mae:0.036770 
## [873]    eval-mae:0.080851   train-mae:0.036742 
## [874]    eval-mae:0.080848   train-mae:0.036699 
## [875]    eval-mae:0.080831   train-mae:0.036693 
## [876]    eval-mae:0.080819   train-mae:0.036662 
## [877]    eval-mae:0.080816   train-mae:0.036639 
## [878]    eval-mae:0.080817   train-mae:0.036610 
## [879]    eval-mae:0.080853   train-mae:0.036578 
## [880]    eval-mae:0.080859   train-mae:0.036565 
## [881]    eval-mae:0.080858   train-mae:0.036533 
## [882]    eval-mae:0.080855   train-mae:0.036494 
## [883]    eval-mae:0.080840   train-mae:0.036474 
## [884]    eval-mae:0.080839   train-mae:0.036462 
## [885]    eval-mae:0.080828   train-mae:0.036437 
## [886]    eval-mae:0.080783   train-mae:0.036398 
## [887]    eval-mae:0.080783   train-mae:0.036363 
## [888]    eval-mae:0.080781   train-mae:0.036339 
## [889]    eval-mae:0.080789   train-mae:0.036333 
## [890]    eval-mae:0.080756   train-mae:0.036293 
## [891]    eval-mae:0.080767   train-mae:0.036256 
## [892]    eval-mae:0.080778   train-mae:0.036230 
## [893]    eval-mae:0.080791   train-mae:0.036200 
## [894]    eval-mae:0.080790   train-mae:0.036175 
## [895]    eval-mae:0.080790   train-mae:0.036152 
## [896]    eval-mae:0.080791   train-mae:0.036121 
## [897]    eval-mae:0.080788   train-mae:0.036099 
## [898]    eval-mae:0.080779   train-mae:0.036068 
## [899]    eval-mae:0.080804   train-mae:0.036035 
## [900]    eval-mae:0.080797   train-mae:0.036011 
## [901]    eval-mae:0.080786   train-mae:0.035976 
## [902]    eval-mae:0.080790   train-mae:0.035943 
## [903]    eval-mae:0.080781   train-mae:0.035902 
## [904]    eval-mae:0.080767   train-mae:0.035876 
## [905]    eval-mae:0.080753   train-mae:0.035863 
## [906]    eval-mae:0.080769   train-mae:0.035848 
## [907]    eval-mae:0.080891   train-mae:0.035827 
## [908]    eval-mae:0.080911   train-mae:0.035775 
## [909]    eval-mae:0.080912   train-mae:0.035729 
## [910]    eval-mae:0.080923   train-mae:0.035712 
## [911]    eval-mae:0.080928   train-mae:0.035680 
## [912]    eval-mae:0.080901   train-mae:0.035669 
## [913]    eval-mae:0.080920   train-mae:0.035649 
## [914]    eval-mae:0.080902   train-mae:0.035630 
## [915]    eval-mae:0.080925   train-mae:0.035594 
## [916]    eval-mae:0.080898   train-mae:0.035575 
## [917]    eval-mae:0.080903   train-mae:0.035552 
## [918]    eval-mae:0.081019   train-mae:0.035543 
## [919]    eval-mae:0.081006   train-mae:0.035508 
## [920]    eval-mae:0.081035   train-mae:0.035461 
## [921]    eval-mae:0.081015   train-mae:0.035436 
## [922]    eval-mae:0.081013   train-mae:0.035406 
## [923]    eval-mae:0.080998   train-mae:0.035371 
## [924]    eval-mae:0.080992   train-mae:0.035346 
## [925]    eval-mae:0.080968   train-mae:0.035323 
## [926]    eval-mae:0.080941   train-mae:0.035300 
## [927]    eval-mae:0.080940   train-mae:0.035269 
## [928]    eval-mae:0.080921   train-mae:0.035256 
## [929]    eval-mae:0.080920   train-mae:0.035243 
## [930]    eval-mae:0.080942   train-mae:0.035210 
## [931]    eval-mae:0.080951   train-mae:0.035189 
## [932]    eval-mae:0.080991   train-mae:0.035158 
## [933]    eval-mae:0.080865   train-mae:0.035144 
## [934]    eval-mae:0.080879   train-mae:0.035121 
## [935]    eval-mae:0.080883   train-mae:0.035085 
## [936]    eval-mae:0.080880   train-mae:0.035056 
## [937]    eval-mae:0.080885   train-mae:0.035024 
## [938]    eval-mae:0.080903   train-mae:0.034991 
## [939]    eval-mae:0.080995   train-mae:0.034968 
## [940]    eval-mae:0.081014   train-mae:0.034942 
## [941]    eval-mae:0.081009   train-mae:0.034920 
## [942]    eval-mae:0.081000   train-mae:0.034892 
## [943]    eval-mae:0.080989   train-mae:0.034861 
## [944]    eval-mae:0.081009   train-mae:0.034863 
## [945]    eval-mae:0.081009   train-mae:0.034845 
## [946]    eval-mae:0.080985   train-mae:0.034808 
## [947]    eval-mae:0.080977   train-mae:0.034772 
## [948]    eval-mae:0.081004   train-mae:0.034745 
## [949]    eval-mae:0.080999   train-mae:0.034696 
## [950]    eval-mae:0.081015   train-mae:0.034681 
## [951]    eval-mae:0.081012   train-mae:0.034649 
## [952]    eval-mae:0.081019   train-mae:0.034635 
## [953]    eval-mae:0.081019   train-mae:0.034628 
## [954]    eval-mae:0.081012   train-mae:0.034602 
## [955]    eval-mae:0.081014   train-mae:0.034561 
## [956]    eval-mae:0.081027   train-mae:0.034521 
## [957]    eval-mae:0.081019   train-mae:0.034492 
## [958]    eval-mae:0.081032   train-mae:0.034465 
## [959]    eval-mae:0.081021   train-mae:0.034441 
## [960]    eval-mae:0.081023   train-mae:0.034426 
## [961]    eval-mae:0.081008   train-mae:0.034410 
## [962]    eval-mae:0.081013   train-mae:0.034393 
## [963]    eval-mae:0.081018   train-mae:0.034386 
## [964]    eval-mae:0.081015   train-mae:0.034352 
## [965]    eval-mae:0.080996   train-mae:0.034339 
## [966]    eval-mae:0.081023   train-mae:0.034321 
## [967]    eval-mae:0.081046   train-mae:0.034300 
## [968]    eval-mae:0.081037   train-mae:0.034290 
## [969]    eval-mae:0.081045   train-mae:0.034268 
## [970]    eval-mae:0.081048   train-mae:0.034238 
## [971]    eval-mae:0.081015   train-mae:0.034224 
## [972]    eval-mae:0.081020   train-mae:0.034200 
## [973]    eval-mae:0.081022   train-mae:0.034190 
## [974]    eval-mae:0.081010   train-mae:0.034161 
## [975]    eval-mae:0.081000   train-mae:0.034127 
## [976]    eval-mae:0.080990   train-mae:0.034102 
## [977]    eval-mae:0.080977   train-mae:0.034080 
## [978]    eval-mae:0.080960   train-mae:0.034067 
## [979]    eval-mae:0.080949   train-mae:0.034053 
## [980]    eval-mae:0.080940   train-mae:0.034017 
## [981]    eval-mae:0.080915   train-mae:0.033997 
## [982]    eval-mae:0.080893   train-mae:0.033971 
## [983]    eval-mae:0.080912   train-mae:0.033962 
## [984]    eval-mae:0.080910   train-mae:0.033948 
## [985]    eval-mae:0.080915   train-mae:0.033927 
## [986]    eval-mae:0.080915   train-mae:0.033923 
## [987]    eval-mae:0.080909   train-mae:0.033910 
## [988]    eval-mae:0.080936   train-mae:0.033896 
## [989]    eval-mae:0.080937   train-mae:0.033895 
## [990]    eval-mae:0.080934   train-mae:0.033880 
## [991]    eval-mae:0.080933   train-mae:0.033856 
## [992]    eval-mae:0.080948   train-mae:0.033830 
## [993]    eval-mae:0.080964   train-mae:0.033823 
## [994]    eval-mae:0.080979   train-mae:0.033783 
## [995]    eval-mae:0.080932   train-mae:0.033746 
## [996]    eval-mae:0.080966   train-mae:0.033725 
## [997]    eval-mae:0.080970   train-mae:0.033710 
## [998]    eval-mae:0.080980   train-mae:0.033682 
## [999]    eval-mae:0.080963   train-mae:0.033658 
## [1000]   eval-mae:0.080967   train-mae:0.033639</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">eval &lt;-<span class="st"> </span>bst<span class="op">$</span>evaluation_log <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">gather</span>(tipo, rmse, <span class="op">-</span>iter)
<span class="kw">ggplot</span>(eval, <span class="kw">aes</span>(<span class="dt">x=</span>iter, <span class="dt">y=</span>rmse, <span class="dt">colour=</span>tipo, <span class="dt">group=</span> tipo)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_log10</span>()</code></pre>
<p><img src="13-arboles-2_files/figure-html/unnamed-chunk-40-1.png" width="480" /></p>

<div id="refs" class="references">
<div>
<p>Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning (Information Science and Statistics)</em>. Secaucus, NJ, USA: Springer-Verlag New York, Inc.</p>
</div>
<div>
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT Press.</p>
</div>
<div>
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. <em>The Elements of Statistical Learning</em>. Springer Series in Statistics. Springer New York Inc. <a href="http://web.stanford.edu/~hastie/ElemStatLearn/">http://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
</div>
<div>
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. <em>An Introduction to Statistical Learning: With Applications in R</em>. Springer Publishing Company, Incorporated. <a href="http://www-bcf.usc.edu/~gareth/ISL/">http://www-bcf.usc.edu/~gareth/ISL/</a>.</p>
</div>
<div>
<p>Ng, Andrew. 2017. “Machine Learning.” <a href="https://www.coursera.org/learn/machine-learning">https://www.coursera.org/learn/machine-learning</a>.</p>
</div>
<div>
<p>Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” <em>J. Mach. Learn. Res.</em> 15 (1). JMLR.org: 1929–58. <a href="http://dl.acm.org/citation.cfm?id=2627435.2670313">http://dl.acm.org/citation.cfm?id=2627435.2670313</a>.</p>
</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-ESL">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. <em>The Elements of Statistical Learning</em>. Springer Series in Statistics. Springer New York Inc. <a href="http://web.stanford.edu/~hastie/ElemStatLearn/">http://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="metodos-basados-en-arboles.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/felipegonzalez/aprendizaje-maquina-mcd/edit/master/13-arboles-2.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
